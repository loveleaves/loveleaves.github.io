<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Deploy on 安哲睿</title>
        <link>https://loveleaves.github.io/categories/deploy/</link>
        <description>Recent content in Deploy on 安哲睿</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Andrew Stark</copyright>
        <lastBuildDate>Sun, 23 Mar 2025 00:19:34 +0800</lastBuildDate><atom:link href="https://loveleaves.github.io/categories/deploy/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>模型剪枝介绍</title>
        <link>https://loveleaves.github.io/p/prune/</link>
        <pubDate>Fri, 21 Jun 2024 00:00:00 +0000</pubDate>
        
        <guid>https://loveleaves.github.io/p/prune/</guid>
        <description>&lt;h2 id=&#34;references&#34;&gt;References
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://zhuanlan.zhihu.com/p/692858636?&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;深度神经网络剪枝综述&lt;/a&gt;，&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2308.06767&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommendations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.zhihu.com/question/652126515/answer/3457652467&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;目前针对大模型剪枝的方法有哪些？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://hanlab.mit.edu/courses/2023-fall-65940&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;MIT 6.5940 TinyML and Efficient Deep Learning Computing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/datawhalechina/awesome-compression&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;模型压缩的小白入门教程&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;介绍&#34;&gt;介绍
&lt;/h2&gt;&lt;p&gt;**模型剪枝（Model Pruning）**是一种用于减少神经网络模型参数数量和计算量的技术。它通过识别和去除在训练过程中对模型性能影响较小的参数或连接，从而实现模型的精简和加速。&lt;/p&gt;
&lt;p&gt;通常，模型剪枝可以分为两种类型：&lt;strong&gt;结构化剪枝（Structured Pruning）&lt;strong&gt;和&lt;/strong&gt;非结构化剪枝（Unstructured Pruning）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;结构化剪枝和非结构化剪枝的&lt;strong&gt;主要区别在于剪枝目标和由此产生的网络结构&lt;/strong&gt;。结构化剪枝根据特定规则删除连接或层结构，同时保留整体网络结构。而非结构化剪枝会剪枝各个参数，从而产生不规则的稀疏结构。&lt;/p&gt;
&lt;p&gt;模型剪枝的一般步骤包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;训练初始模型&lt;/strong&gt;：首先，需要训练一个初始的大模型，通常是为了达到足够的性能水平。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;评估参数重要性&lt;/strong&gt;：使用某种评估方法（如：权重的绝对值、梯度信息等）来确定模型中各个参数的重要性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;剪枝&lt;/strong&gt;：根据评估结果，剪枝掉不重要的参数或连接，可以是结构化的或非结构化的。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;修正和微调&lt;/strong&gt;：进行剪枝后，需要进行一定的修正和微调，以确保模型的性能不会显著下降。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;模型剪枝可以带来多方面的好处，包括减少模型的存储需求、加速推理速度、减少模型在边缘设备上的资源消耗等。然而，剪枝可能会带来一定的性能损失，因此需要在剪枝前后进行适当的评估和调整。&lt;/p&gt;
&lt;h2 id=&#34;剪枝方法&#34;&gt;剪枝方法
&lt;/h2&gt;&lt;h3 id=&#34;network-slimming&#34;&gt;Network Slimming
&lt;/h3&gt;&lt;h4 id=&#34;基本思想&#34;&gt;基本思想
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://loveleaves.github.io/imgs/prune/1.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文核心点&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;以 BN 中的 γ 为切入点，即 γ 越小，其对应的特征图越不重要&lt;/li&gt;
&lt;li&gt;为了使得 γ 能有特征选择的作用，引入 L1 正则来控制 γ
$$
L = \sum_{(x,y)} l(f(x, W), y) + \lambda \sum_{\gamma \in \Gamma} g(\gamma)
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;batchnorm&#34;&gt;BatchNorm
&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;如何得到每个特征图的重要性呢？&amp;ndash;BN要解决的问题&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Network slimming，就是利用BN层中的缩放因子Y&lt;/li&gt;
&lt;li&gt;先来回顾下BN是做什么的：$\hat{x}^{(k)} = \frac{x^{(k)} - E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}$&lt;/li&gt;
&lt;li&gt;整体感觉就是一个归一化操作，但是BN中还额外引入了两个可训练的参数：γ和β。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;BN本质作用&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BN要做的就是把越来越偏离的分布给他拉回来！&lt;/li&gt;
&lt;li&gt;再重新规范化到均值为0方差为1的标准正态分布&lt;/li&gt;
&lt;li&gt;这样能够使得激活函数在数值层面更敏感，训练更快&lt;/li&gt;
&lt;li&gt;有一种感觉：经过BN后，把数值分布强制在了非线性函数的线性区域中&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;BN额外参数&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果都是线性的了，神经网络还有意义吗？&lt;/li&gt;
&lt;li&gt;BN另一方面还需要保证一些非线性，对规范化后的结果再进行变换&lt;/li&gt;
&lt;li&gt;这两个参数是训练得到的：$y^{(k)}=\gamma^{(k)}\hat{x}^{(k)}+\beta^{(k)}$&lt;/li&gt;
&lt;li&gt;感觉就是从正太分布进行一些改变，拉动一下，变一下形状！&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;稀疏化原理与效果&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;论文中提出：训练时使用 L1 正则化能对参数进行稀疏作用&lt;/li&gt;
&lt;li&gt;L1：稀疏与特征选择；L2：平滑特征&lt;/li&gt;
&lt;li&gt;L1 正则化：$J(\vec{\theta}) = \frac{1}{2} \sum_{i=1}^{m} \left( h_{\vec{\theta}}(x^{(i)}) - y^{(i)} \right)^2 + \lambda \sum_{j=1}^{n} |\theta_j|$，$\theta_j$是要正则化的参数&lt;/li&gt;
&lt;li&gt;L2 正则化：$J(\vec{\theta}) = \frac{1}{2} \sum_{i=1}^{m} \left( h_{\vec{\theta}}(x^{(i)}) - y^{(i)} \right)^2 + \lambda \sum_{j=1}^{n} \theta_j^2$，同上&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://loveleaves.github.io/imgs/prune/2.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;剪枝流程&#34;&gt;剪枝流程
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://loveleaves.github.io/imgs/prune/3.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;代码&#34;&gt;代码
&lt;/h4&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/loveleaves/model_prune/tree/main/01NetworkSlimming&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;github&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;paper-with-code&#34;&gt;paper with code
&lt;/h2&gt;&lt;h3 id=&#34;depgraph&#34;&gt;DepGraph
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/VainF/Torch-Pruning&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;arxiv.org/abs/2301.12900&#34; &gt;paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
