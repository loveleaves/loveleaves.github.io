<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>NLP on 安哲睿</title>
        <link>https://loveleaves.github.io/categories/nlp/</link>
        <description>Recent content in NLP on 安哲睿</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Andrew Stark</copyright>
        <lastBuildDate>Thu, 11 Dec 2025 23:12:40 +0800</lastBuildDate><atom:link href="https://loveleaves.github.io/categories/nlp/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>【NLP】 NLP 手册</title>
        <link>https://loveleaves.github.io/p/nlp_intro/</link>
        <pubDate>Wed, 09 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>https://loveleaves.github.io/p/nlp_intro/</guid>
        <description>&lt;div id=&#34;verification&#34; style=&#34; display: flex;
  flex-direction: column;
  align-items: center;
  margin-top: 20vh;&#34;&gt;
    &lt;div id=&#34;entry-box&#34;&gt;
        
        &lt;div id=&#34;secret-word&#34; style=&#34;  font-size: 1.5rem;
    margin-bottom: 1rem;&#34;&gt;Unlock to view this content.&lt;/div&gt;
        &lt;form id=&#34;password-form&#34; onsubmit=&#34;checkPassword(); return false;&#34; style=&#34; display: flex;
    flex-direction: column;
    width: auto;&#34;&gt;
            &lt;input type=&#34;password&#34; name=&#34;password&#34; id=&#34;password&#34; placeholder=&#34;Please enter password&#34; required style=&#34;margin-bottom: 1.5rem;
      padding: 10px 20px;&#34;&gt;
            &lt;input type=&#34;hidden&#34; name=&#34;encryptedPassword&#34; id=&#34;encrypted-password&#34;&gt;
            &lt;input type=&#34;submit&#34; value=&#34;Submit&#34; name=&#34;submit&#34; id=&#34;secret-submit&#34; style=&#34; padding: 10px 20px;
      cursor: pointer;&#34; onmouseover=&#34;this.style.backgroundColor=&#39;#FAB005&#39;;&#34;
                   onmouseout=&#34;this.style.backgroundColor=&#39;&#39;;&#34;&gt;
        &lt;/form&gt;
    &lt;/div&gt;
    &lt;div id=&#34;secret&#34; style=&#34;display: none&#34; password=&#34;nopainsnogains&#34;&gt;

# NLP基础
## 导论
### 定义

### 常见任务

### 技术演进历史

## 文本表示（Representation）
### 概述
文本表示是将自然语言转化为计算机能够理解的数值形式，是绝大多数自然语言处理（NLP）任务的基础步骤。

早期的文本表示方法（如词袋模型）通常将整段文本编码为一个向量。这类方法实现简单、计算高效，但存在明显的局限性——表达语序和上下文语义的能力较弱。因此，现代 NLP 技术逐渐引入更加精细和表达力更强的文本表示方法，以更有效地建模语言的结构和含义。文本表示的第一步通常是分词和词表构建。

1. 分词（Tokenization）是将原始文本切分为若干具有独立语义的最小单元（即token）的过程，是所有 NLP 任务的起点。
2. 词表（Vocabulary）是由语料库构建出的、包含模型可识别 token 的集合。词表中每个token都分配有唯一的 ID，并支持 token 与 ID 之间的双向映射。

在后续训练或预测过程中，模型会首先对输入文本进行分词，再通过词表将每个 token 映射为其对应的 ID。接着，这些 ID 会被输入嵌入层（Embedding Layer），转换为低维稠密的向量表示（即词向量）

### 分词（Tokenization）
**分词粒度**
- 词级（Word-Level）分词
- 字符级（Character-Level）分词
- 子词级（Subword‑Level）分词：目前主流方法

以下是 tokenization 过程的高度概括：

![tokenization](/imgs/nlp/tokenization.png)

在分词（根据其模型）之前，tokenizer 需要进行两个步骤： **标准化（normalization）** 和 **预分词（pre-tokenization）** 。

**标准化**步骤涉及一些常规清理，例如删除不必要的空格、小写和“/”或删除重音符号。

tokenizer 一般不会在原始文本上进行训练。因此，我们首先需要将文本拆分为更小的实体，例如单词。这就是**预分词**步骤的作用。基于单词的 tokenizer 可以简单地根据空格和标点符号将原始文本拆分为单词。这些词将是 tokenizer 在训练期间可以学习的子词的边界。

三种主要的子词 tokenization 算法：BPE（由 GPT-2 等使用）、WordPiece（由 BERT 使用）和 Unigram（由 T5 等使用）。

模型 | BPE | WordPiece | Unigram
:----:|:---:|:---------:|:------:
训练 | 从小型词汇表开始，学习合并 token 的规则 | 从小型词汇表开始，学习合并 token 的规则 | 从大型词汇表开始，学习删除 token 的规则
训练步骤 | 合并对应最常见的 token 对 | 合并对应得分最高的 token 对，优先考虑每个独立 token 出现频率较低的对 | 删除会在整个语料库上最小化损失的词汇表中的所有 token
学习 | 合并规则和词汇表 | 仅词汇表 | 含有每个 token 分数的词汇表
编码 | 将一个单词分割成字符并使用在训练过程中学到的合并 | 从开始处找到词汇表中的最长子词，然后对其余部分做同样的事 | 使用在训练中学到找到最可能的 token 分割方式

#### BPE（Byte Pair Encoding）/BBPE（Byte Level Byte Pair Encoding）
字节对编码（BPE）最初被开发为一种压缩文本的算法，然后在预训练 GPT 模型时被 OpenAI 用于 tokenization。许多 Transformer 模型都使用它，包括 GPT、GPT-2、RoBERTa、BART 和 DeBERTa。

BPE 训练首先计算语料库中使用的唯一单词集合（在完成标准化和预分词步骤之后），然后取出用来编写这些词的所有符号来构建词汇表。举一个非常简单的例子，假设我们的语料库使用了这五个词：
``` python
&#34;hug&#34;, &#34;pug&#34;, &#34;pun&#34;, &#34;bun&#34;, &#34;hugs&#34;
```
基础单词集合将是 [&#34;b&#34;, &#34;g&#34;, &#34;h&#34;, &#34;n&#34;, &#34;p&#34;, &#34;s&#34;, &#34;u&#34;] 。在实际应用中，基本词汇表将至少包含所有 ASCII 字符，可能还包含一些 Unicode 字符。如果你正在 tokenization 不在训练语料库中的字符，则该字符将转换为未知 tokens，这就是为什么许多 NLP 模型在分析带有表情符号的内容的结果非常糟糕的原因之一。

&gt; GPT-2 和 RoBERTa （这两者非常相似）的 tokenizer 有一个巧妙的方法来处理这个问题：他们不把单词看成是用 Unicode 字符编写的，而是用字节编写的。这样，基本词汇表的大小很小（256），但是能包含几乎所有你能想象的字符，而不会最终转换为未知 tokens 这个技巧被称为 字节级（byte-level） BPE 。

获得这个基础单词集合后，我们通过学习 **合并（merges）** 来添加新的 tokens 直到达到期望的词汇表大小。合并是将现有词汇表中的两个元素合并为一个新元素的规则。所以，一开始会创建出含有两个字符的 tokens 然后，随着训练的进展，会产生更长的子词。

在分词器训练期间的任何一步，BPE 算法都会搜索最常见的现有 tokens 对 （在这里，“对”是指一个词中的两个连续 tokens ）。最常见的这一对会被合并，然后我们重复这个过程。

**步骤**
1. 标准化
2. 预分词
3. 将单词拆分为单个字符
4. 根据学习的合并规则，按顺序合并拆分的字符

**代码实现**
``` python
# 字母表
alphabet = []
for word in word_freqs.keys():
    for letter in word:
        if letter not in alphabet:
            alphabet.append(letter)
alphabet.sort()
vocab = [&#34;&lt;|endoftext|&gt;&#34;] + alphabet.copy() # GPT-2 模型的特殊 tokens

# 合并规则，计算出现频率，之后可以按相邻频率最高的pair对合并
def compute_pair_freqs(splits):
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            pair_freqs[pair] += freq
    return pair_freqs

# 设定词表上限，得到训练词表 vocab 及合并规则 merges
vocab_size = 50
while len(vocab) &lt; vocab_size:
    pair_freqs = compute_pair_freqs(splits)
    best_pair = &#34;&#34;
    max_freq = None
    for pair, freq in pair_freqs.items():
        if max_freq is None or max_freq &lt; freq:
            best_pair = pair
            max_freq = freq
    if not best_pair: # reach max size
        break
    splits = merge_pair(*best_pair, splits)
    merges[best_pair] = best_pair[0] + best_pair[1]
    vocab.append(best_pair[0] + best_pair[1])

# 通过vocab、merges
def tokenize(word):
    splits = [[l for l in word]]
    for pair, merge in merges.items():
        for idx, split in enumerate(splits):
            i = 0
            while i &lt; len(split) - 1:
                if split[i] == pair[0] and split[i + 1] == pair[1]:
                    split = split[:i] + [merge] + split[i + 2 :]
                else:
                    i += 1
            splits[idx] = split

    return sum(splits, [])
```

**总结**
- 使用频率进行合并

#### WordPiece
&gt; [WordPiece tokenization 算法](https://hf-mirror.com/learn/llm-course/zh-CN/chapter6/6)
WordPiece 是 Google 开发的用于 BERT 预训练的分词算法。自此之后，很多基于 BERT 的 Transformer 模型都复用了这种方法，比如 DistilBERT，MobileBERT，Funnel Transformers 和 MPNET。它在训练方面与 BPE 非常类似，但实际的分词方法有所不同。

与BPE 一样，WordPiece 也是从包含模型使用的特殊 tokens 和初始字母表的小词汇表开始的。由于它是通过添加前缀（如 BERT 中的 ## ）来识别子词的，每个词最初都会通过在词内部所有字符前添加该前缀进行分割。因此，例如 &#34;word&#34; 将被这样分割：
``` python
w ##o ##r ##d
```
因此，初始字母表包含所有出现在单词第一个位置的字符，以及出现在单词内部并带有 WordPiece 前缀的字符。

然后，同样像 BPE 一样，WordPiece 会学习合并规则。主要的不同之处在于合并对的选择方式。WordPiece 不是选择频率最高的对，而是对每对计算一个得分，使用以下公式：
&lt;center&gt;
score=(freq_of_pair)/(freq_of_first_element×freq_of_second_element)
&lt;/center&gt;

**代码实现**
``` python
# 字母表
alphabet = []
for word in word_freqs.keys():
    if word[0] not in alphabet:
        alphabet.append(word[0])
    for letter in word[1:]:
        if f&#34;##{letter}&#34; not in alphabet:
            alphabet.append(f&#34;##{letter}&#34;)
vocab = [&#34;[PAD]&#34;, &#34;[UNK]&#34;, &#34;[CLS]&#34;, &#34;[SEP]&#34;, &#34;[MASK]&#34;] + alphabet.copy()

# 合并规则，计算pair分数
def compute_pair_scores(splits):
    letter_freqs = defaultdict(int)
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            letter_freqs[split[0]] += freq
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            letter_freqs[split[i]] += freq
            pair_freqs[pair] += freq
        letter_freqs[split[-1]] += freq

    scores = {
        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])
        for pair, freq in pair_freqs.items()
    }
    return scores

# 最大子词查找
def encode_word(word):
    tokens = []
    while len(word) &gt; 0:
        i = len(word)
        while i &gt; 0 and word[:i] not in vocab:
            i -= 1
        if i == 0:
            return [&#34;[UNK]&#34;]
        tokens.append(word[:i])
        word = word[i:]
        if len(word) &gt; 0:
            word = f&#34;##{word}&#34;
    return tokens
```

**总结**
- 通过`##`识别子词
- WordPiece 和 BPE 的分词方式有所不同，WordPiece 只保存最终词汇表，而不保存学习到的合并规则。WordPiece 从待分词的词开始，找到词汇表中最长的子词，然后在其处分割。
- 当分词过程中无法在词汇库中找到该子词时，**整个词**会被标记为 unknown（未知），BPE 只会将不在词汇库中的**单个字符**标记为 unknown。

#### Unigram
&gt; [Unigram tokenization 算法](https://hf-mirror.com/learn/llm-course/zh-CN/chapter6/7)

Unigram 算法常用于 SentencePiece 中，该切分算法被 AlBERT，T5，mBART，Big Bird 和 XLNet 等模型广泛采用。

**训练**

与BPE 和 WordPiece 相比，Unigram 的工作方式正好相反：它从一个**大词汇库**开始，然后逐步删除词汇，直到达到目标词汇库大小。构建基础词汇库有多种方法：例如，我们可以选取预切分词汇中最常见的子串，或者在具有大词汇量的初始语料库上进行 BPE 得到一个初始词库。

在训练的每一步，Unigram 算法都会在给定当前词汇的情况下计算语料库的损失。然后，对于词汇表中的每个符号，算法计算如果删除该符号，整体损失会增加多少，并寻找删除后损失增加最少的符号。这些符号对语料库的整体损失影响较小，因此从某种意义上说，它们“相对不必要”并且是移除的最佳候选者。

这个过程非常消耗计算资源，因此我们不只是删除与最低损失增长相关的单个符号，而是删除与最低损失增长相关的百分之/p （p 是一个可以控制的超参数，通常是 10 或 20）的符号。然后重复此过程，直到词汇库达到所需大小。

注意，我们永远不会删除基础的单个字符，以确保任何词都能被切分。

然而，这仍然有些模糊：算法的主要部分是在词汇库中计算语料库的损失并观察当我们从词汇库中移除一些符号时损失如何变化，但我们尚未解释如何做到这一点。这一步依赖于 Unigram 模型的切分算法。

#### 常用工具
按照实现方式大致可以分为如下两类：
- 一类是基于词典或模型的传统方法，主要以“词”为单位进行切分；
- 另一类是基于子词建模算法（如BPE）的方式，从数据中自动学习高频字组合，构建子词词表。

前者的代表工具包括 jieba、HanLP等，这些工具广泛应用于传统 NLP 任务中。 

后者的代表工具包括 Hugging Face Tokenizer、SentencePiece、tiktoken等，常用于大规模预训练语言模型中。

##### SentencePiece
&gt; [官网](https://github.com/google/sentencepiece)

**主要特性**
- 多分词粒度：支持BPE、ULM子词算法，也支持char, word分词；
- 多语言：以unicode方式编码字符，将所有的输入（英文、中文等不同语言）都转化为unicode字符，解决了多语言编码方式不同的问题；
- 编解码的可逆性：之前几种分词算法对空格的处理略显粗暴，有时是无法还原的。Sentencepiece显式地将空白作为基本标记来处理，用一个元符号 “▁”（ U+2581 ）转义空白，这样就可以实现简单且可逆的编解码；
- 无须Pre-tokenization：Sentencepiece可以直接从raw text/setences进行训练，无须Pre-tokenization
- Fast and lightweight；

##### tokenizers库
&gt; [官网](https://hugging-face.cn/docs/tokenizers/index)

tokenizers是transformers的兄弟库，实现了当前最常用的分词器。

主要特点：
- 使用当今最常用的分词器来训练新词汇表和进行分词。
- 得益于 Rust 实现，速度极快（包括训练和分词）。在服务器 CPU 上，对 1GB 的文本进行分词耗时不到 20 秒。
- 易于使用，同时也极其通用。
- 专为研究和生产而设计。
- 完整的对齐跟踪。即使进行了破坏性的规范化，也始终可以获取到与任意词元对应的原始句子部分。
- 完成所有预处理：截断、填充、添加模型所需的特殊词元。

分词流程
- 归一化
- 预分词
- 模型
- 后处理

#### LLM中的分词器
##### BERT的分词器
&gt; [代码](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/tokenization_bert.py)
&gt; 
&gt; [文档](https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/bert#transformers.BertTokenizer)

BERT的分词器由两个部分组成：
- BasicTokenizer：
  - 转成 unicode：Python3，输入为str时，可以省略这一步
  - _clean_text：去除各种奇怪字符
  - _tokenize_chinese_chars：中文按字拆开
  - whitespace_tokenize：空格分词
  - _run_strip_accents：去掉变音符号
  - _run_split_on_punc：标点分词
  - 再次空格分词：whitespace_tokenize(&#34; &#34;.join(split_tokens))，先用空格join再按空白分词，可以去掉连续空格
- WordpieceTokenizer：
  - 贪心最大匹配：用双指针实现；

### 词表示（word representation）
&gt; 详见：`尚硅谷大模型技术之NLP1.0.3`的3.3


词表示的发展经历了从稀疏的one-hot编码，到稠密的语义化词向量，再到近年来的上下文相关的词表示。不同的词表示方法在表达能力、语义建模、上下文适应性等方面存在显著差异。

#### One-hot编码
缺点：
- 随着词表规模的扩大，向量维度会迅速膨胀，导致计算效率低下
- 无法体现词与词之间的语义关系（如相关词向量正交为0）

#### 语义化词向量 Word2Vec
它通过对大规模语料的学习，为每个词生成一个具有语义意义的稠密向量表示。这些向量能够在连续空间中表达词与词之间的关系，使得“意思相近”的词在空间中距离更近。

![CBOW](/imgs/nlp/cbow.png)
&lt;center&gt;CBOW&lt;/center&gt;

![Skip-Gram](/imgs/nlp/skip_gram.png)
&lt;center&gt;Skip-Gram&lt;/center&gt;

#### 上下文相关词表示（Contextual Word Representations）
虽然像Word2Vec这样的模型已经能够为词语提供具有语义的向量表示，但是它只为每个词分配一个固定的向量表示，不论它在句中出现的语境如何。这种表示被称为静态词向量（static embeddings）。

然而，语言的表达极其灵活，一个词在不同上下文中可能有完全不同的含义。如吃的苹果和苹果手机。

上下文相关词表示（Contextual Word Representations），是指词语的向量表示会根据它所在的句子上下文动态变化，从而更好地捕捉其语义。一个具有代表性的模型是——ELMo。该模型全称为 Embeddings from Language Models，发表于2018年2月。其基于LSTM 语言模型，使用上下文动态生成每个词的表示，每个词的向量由其前文和后文共同决定，是第一个被广泛应用于下游任务的上下文词向量模型。

## 传统序列模型
### RNN（Recurrent Neural Network，循环神经网络）
在自然语言中，词语的顺序对于理解句子的含义至关重要。虽然词向量能够表示词语的语义，但它本身并不包含词语之间的顺序信息。

为了解决这一问题，研究者提出RNN（Recurrent Neural Network，循环神经网络）。

RNN 会逐个读取句子中的词语，并在每一步结合当前词和前面的上下文信息，不断更新对句子的理解。通过这种机制，RNN 能够持续建模上下文，从而更准确地把握句子的整体语义。因此RNN曾是序列建模领域的主流模型，被广泛应用于各类NLP任务。

&lt;font color=&#39;red&#39;&gt;
说明：
随着技术的发展，RNN已经逐渐被结构更灵活、计算效率更高的Transformer 模型所取代，后者已经成为当前自然语言处理的主流方法。

尽管如此，RNN 仍然具有重要的学习价值。它所体现的“循环建模上下文”的思想，不仅为 LSTM 和 GRU 等改进模型奠定了基础，也有助于我们更好地理解 Transformer 等更复杂的架构。
&lt;/font&gt;

#### 网络结构
其中隐藏层的计算公式为：
$h_t = \tanh(x_t*W_x+h_{t-1}*W_h+b)$

![RNN计算图](/imgs/nlp/RNN计算图.svg)
&lt;center&gt;
RNN计算图
&lt;/center&gt;

![RNN](/imgs/nlp/RNN.png)
&lt;center&gt;
多层+双向 RNN
&lt;/center&gt;

Pytorch API： [torch.nn.RNN 模块](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)

``` python
torch.nn.RNN(
    input_size, # 每个时间步输入特征的维度（词向量维度）
    hidden_size, # 隐藏状态的维度
    num_layers=1, # RNN 层数，默认为 1
    nonlinearity=&#34;tanh&#34;, # 激活函数，&#39;tanh&#39;（默认）或 &#39;relu&#39;
    bias=True, # 是否使用偏置项，默认 True
    batch_first=False, # 输入张量是否是 (batch, seq, feature)，默认 False 表示 (seq, batch, feature)
    dropout=0.0, # 除最后一层外，其余层之间的 dropout 概率
    bidirectional=False, # 是否为双向 RNN，默认 False
    device=None,
    dtype=None,
)

rnn = torch.nn.RNN(...)
output, h_n = rnn(input, h_0)

# 注意
序列padding时，输入RNN前需进行pack处理，详见LSTM部分
```

**输入输出形状：**
- 输入
  - input：输入序列，形状为(seq_len, batch_size, input_size)，如果 batch_first=True，则为 (batch_size, seq_len, input_size)
  - h_0：可选，初始隐藏状态，形状为 (num_layers × num_directions, batch_size, hidden_size)
- 输出：
  - output：RNN层的输出，包含最后一层每个时间步的隐藏状态，形状为 (seq_len, batch_size, num_directions × hidden_size )，如果如果 batch_first=True，则为(batch_size, seq_len, num_directions × hidden_size )
  - h_n：最后一个时间步的隐藏状态，包含每一层的每个方向，形状为 (num_layers × num_directions, batch_size, hidden_size)

![RNN](/imgs/nlp/RNN_shape.png)
&lt;center&gt;
多层+双向 RNN 输入输出形状
&lt;/center&gt;

#### 存在问题
尽管循环神经网络（RNN）在处理序列数据方面具有天然优势，但它在实际应用中面临一个非常严重的问题：长期依赖建模困难。这指的是：在训练过程中，当输入序列很长时，模型难以有效学习早期输入对最终输出的影响。

上述问题的根本原因在于训练过程中存在的梯度消失或梯度爆炸问题。

在训练RNN时，采用的是时间反向传播（Backpropagation Through Time, BPTT）方法，在反向传播过程中，梯度需要在每个时间步上不断链式传递。

### 长短期记忆网络（Long Short-Term Memory, LSTM）
为了缓解RNN梯度消失或者梯度爆炸的问题，Hochreiter 和 Schmidhuber 于 1997 年提出了长短期记忆网络（Long Short-Term Memory, LSTM）。

#### 网络结构
LSTM 通过引入特殊的记忆单元（Memory Cell，图中的），有效提升了模型对长序列依赖关系的建模能力。

其中隐藏层的计算公式为：
$$
i_t​=\sigma(W_{ii​}x_{t}​+b_{ii}​+W_{hi}​h_{t−1}​+b_{hi}​)\\\\
f_t​=\sigma(W_{if}​x_t​+b_{if}​+W_{hf}​h_{t−1}​+b_{hf}​)\\\\
g_t​=\tanh(W_{ig}​x_t​+b_{ig}​+W_{hg}​h_{t−1}​+b_{hg}​)\\\\
o_t​=\sigma(W_{io}​x_t​+b_{io}​+W_{ho}​h_{t−1}​+b_{ho}​)\\\\
c_t​=f_t​ \odot c_{t−1}​+i_t​ \odot g_t\\\\
​h_t​=o_t \odot \tanh(c_t​)​
$$
注：
- σ：sigmoid function，(0,1)
- ⊙：Hadamard product，各元素乘积

其沿时间步展开后的内部结构如下图所示，核心结构是三个“门”，分别是：
- 遗忘门（f）：决定当前时间步要忘记多少过去的记忆。
- 输入门（i）：控制要从当前时间步的输入向记忆单元存入多少新的信息。
- 输出门（o）：控制从记忆单元中读取多少信息作为当前时间步的隐藏状态进行输出。

![LSTM 结构](/imgs/nlp/lstm_structure1.png)
&lt;center&gt;LSTM 结构&lt;/center&gt;

![LSTM 展开结构](/imgs/nlp/lstm_structure.png)
&lt;center&gt;LSTM 展开结构&lt;/center&gt;

![LSTM](/imgs/nlp/lstm.png)
&lt;center&gt;多层+双向 LSTM&lt;/center&gt;

Pytorch API： [torch.nn.LSTM 模块](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)

``` python
torch.nn.LSTM(
    input_size, # 每个时间步输入特征的维度（词向量维度）
    hidden_size, # 隐藏状态的维度
    num_layers=1, # LSTM 层数，默认为 1
    nonlinearity=&#34;tanh&#34;, # 激活函数，&#39;tanh&#39;（默认）或 &#39;relu&#39;
    bias=True, # 是否使用偏置项，默认 True
    batch_first=False, # 输入张量是否是 (batch, seq, feature)，默认 False 表示 (seq, batch, feature)
    dropout=0.0, # 除最后一层外，其余层之间的 dropout 概率
    bidirectional=False, # 是否为双向 LSTM，默认 False
    proj_size=0, # 隐藏状态的投影输出维度；若为 0，则不使用 projection。（详见官方文档，用于调整）
    device=None,
    dtype=None,
)

lstm = torch.nn.LSTM()
output, (h_n, c_n) = lstm(input, (h_0, c_0))
```

**注意：**
``` python
# 对于序列模型中有padding操作时，注意把padding去掉再丢入LSTM计算
# 双向多层LSTM模型示例
class ReviewAnalyzeModel(nn.Module):
    def __init__(self, vocab_size, padding_idx=0):
        super(ReviewAnalyzeModel, self).__init__()
        self.padding_idx = padding_idx

        self.embedding = nn.Embedding(
            num_embeddings=vocab_size,
            embedding_dim=config.EMBEDDING_DIM,
            padding_idx=padding_idx
        )

        self.lstm = nn.LSTM(
            input_size=config.EMBEDDING_DIM,
            hidden_size=config.HIDDEN_SIZE,
            num_layers=config.NUM_LAYERS,
            batch_first=True,
            bidirectional=True
        )

        self.linear = nn.Linear(2 * config.HIDDEN_SIZE, 1)

    def forward(self, x):
        # x.shape [batch_size, seq_len]，尾部padding

        # ① 自动计算每条样本实际长度（忽略 padding）
        lengths = (x != self.padding_idx).sum(dim=1)

        # ② embedding
        embedding = self.embedding(x)

        # ③ pack（让 LSTM 忽略 padding）
        packed = nn.utils.rnn.pack_padded_sequence(
            embedding,
            lengths.cpu(),
            batch_first=True,
            enforce_sorted=False
        )

        # ④ LSTM
        _, (h_n, _) = self.lstm(packed)

        # h_n: [num_layers * num_directions, batch, hidden_dim]
        # 由于是双向，最后一层包括 forward 与 backward
        # forward:  index = (num_layers - 1) * 2
        # backward: index = forward + 1
        forward_idx = (config.NUM_LAYERS - 1) * 2
        backward_idx = forward_idx + 1

        last_hidden_forward = h_n[forward_idx]  # [batch, hidden]
        last_hidden_backward = h_n[backward_idx]  # [batch, hidden]

        # ⑤ 拼接双向 hidden
        last_hidden = torch.cat((last_hidden_forward, last_hidden_backward), dim=-1)

        # ⑥ 线性分类
        out = self.linear(last_hidden)

        return out.squeeze(1)
```

**输入输出形状：**

&lt;font color=&#39;red&#39;&gt;
说明：
如果proj_size设置为0，那么输入、输出除了`c_0`与`c_n`，其他与RNN完全一致。
&lt;/font&gt;

- 输入（假定proj_size设置为0）
  - input：输入序列，形状为(seq_len, batch_size, input_size)，如果 batch_first=True，则为 (batch_size, seq_len, input_size)
  - h_0：可选，初始隐藏状态，形状为 (num_layers × num_directions, batch_size, hidden_size)
  - c_0：可选，初始细胞状态，形状为 (num_layers × num_directions, batch_size, hidden_size)
- 输出：
  - output：LSTM层的输出，包含最后一层每个时间步的隐藏状态，形状为 (seq_len, batch_size, num_directions × hidden_size )，如果如果 batch_first=True，则为(batch_size, seq_len, num_directions × hidden_size )
  - h_n：最后一个时间步的隐藏状态，包含每一层的每个方向，形状为 (num_layers × num_directions, batch_size, hidden_size)
  - c_n：最后一个时间步的细胞状态，包含每一层的每个方向，形状为 (num_layers × num_directions, batch_size, hidden_size)

![LSTM](/imgs/nlp/LSTM_shape.png)
&lt;center&gt;多层+双向 LSTM 输入输出形状&lt;/center&gt;

#### LSTM为何能缓解梯度消失和梯度爆炸？
LSTM通过引入记忆单元（Memory Cell），在时间步之间提供了一条稳定的梯度传播路径。

记忆单元的更新公式为
$$c_t​=f_t​⊙c_{t−1}​+i_t​⊙g_t$$

所以$\frac{\partial C_t}{\partial C_{t-1}}$（简单起见，按照标量推导）

在反向传播时，沿记忆单元路径，梯度传播实际上是多个$f_t$连乘的结果。虽然每个$f_t$的取值小于1，但通常较接近于1。这是因为$f_t$由遗忘门生成，在一般任务中，遗忘门倾向于“记得多、忘得少”，因此$f_t$的值通常较大。

由于乘积中的每一项$f_t$较接近1，整体衰减速度远小于传统RNN中隐藏状态链式传播时的指数衰减。这使得早期时间步的输入，能够通过记忆单元路径稳定地影响到最终的总梯度，从而有效参与参数的更新，保证了模型对长序列依赖的学习能力。

#### 存在问题
尽管 LSTM 相较传统 RNN 解决了长期依赖问题，性能大幅提升，但在实际应用中，仍存在一些明显的局限性和问题，主要包括：
- 难以并行计算
  - LSTM 的时间步之间具有强依赖性（后一个时间步的输入依赖前一个时间步的输出），导致无法进行大规模并行加速，训练和推理速度受限。
- 参数量大，计算开销高：
  - 每个 LSTM 单元内部包含多个门控机制（输入门、遗忘门、输出门），每个门都需要独立计算，导致参数数量和计算量远大于普通 RNN。
  - 在资源受限的场景下（如移动端、嵌入式设备），部署 LSTM 会面临挑战。
- 长期依赖建模仍然有限
  - 虽然 LSTM 延缓了梯度消失问题，但并不能完全消除。当序列极长时，模型依然难以有效捕捉非常远距离的依赖关系。

### 门控循环单元（Gated Recurrent Unit，GRU）
Gated Recurrent Unit（GRU）是为了进一步简化 LSTM 结构、降低计算成本而提出的一种变体。GRU 保留了门控机制的核心思想，但相比 LSTM，结构更为简洁，参数更少，训练效率更高。

在许多实际任务中，GRU 能在保持类似性能的同时，显著减少训练时间。

#### 网络结构
与LSTM相比，GRU做出了以下改进：
- 取消了LSTM中独立的记忆单元，只保留隐藏状态。
- 通过两个门控结构控制信息流动：
  - 重置门（Reset Gate）：用于控制遗忘多少旧信息
  - 更新门（Update Gate）：用于控制保留多少旧信息，以及引入多少新信息

其中计算公式为：
$$
r_t = \sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr})\\\\
z_t = \sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\\\
n_t = \tanh(W_{in} x_t + b_{in} + r_t \odot (W_{hn} h_{(t-1)}+ b_{hn}))\\\\
h_t = (1 - z_t) \odot n_t + z_t \odot h_{(t-1)}
$$

![GRU结构图](/imgs/nlp/gru.png)
&lt;center&gt;
GRU结构图
&lt;/center&gt;

![GRU展开结构图](/imgs/nlp/gru_structure.png)
&lt;center&gt;
GRU展开结构图
&lt;/center&gt;

Pytorch API： [torch.nn.GRU 模块](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html)

``` python
torch.nn.GRU(
    input_size, # 每个时间步输入特征的维度（词向量维度）
    hidden_size, # 隐藏状态的维度
    num_layers=1, # GRU 层数，默认为 1
    nonlinearity=&#34;tanh&#34;, # 激活函数，&#39;tanh&#39;（默认）或 &#39;relu&#39;
    bias=True, # 是否使用偏置项，默认 True
    batch_first=False, # 输入张量是否是 (batch, seq, feature)，默认 False 表示 (seq, batch, feature)
    dropout=0.0, # 除最后一层外，其余层之间的 dropout 概率
    bidirectional=False, # 是否为双向 GRU，默认 False
    device=None,
    dtype=None,
)

gru= torch.nn.GRU()
output, h_n = gru(input, h_0)
```

**输入输出形状：**
- 同RNN

## Seq2Seq模型
传统的自然语言处理任务（如文本分类、序列标注）以​**​静态输出**​​为主，其目标是预测固定类别或标签。然而，现实中的许多应用需要模型**​​动态生成新的序列**​​，例如：
- ​​机器翻译​​：输入中文句子，输出对应的英文翻译。
- ​​文本摘要​​：输入长篇文章，生成简短的摘要。
- ​问答系统​​：输入用户问题，生成自然语言回答。
- ​对话系统​​：输入对话历史，生成连贯的下一条回复。

这些任务具有两个关键共同点：
- ​​输入和输出均为序列​​（如词、字符或子词序列）。
- ​输入与输出序列长度动态可变​​（例如翻译任务中，中英文句子长度可能不同）。

为了解决这类问题，研究者提出了**Seq2Seq（Sequence to Sequence，序列到序列）**模型。

### 模型结构
Seq2Seq 模型由一个编码器（Encoder）和一个解码器（Decoder）构成。
- 编码器（Encoder）：负责提取输入序列的语义信息，并将其压缩为一个固定长度的上下文向量（Context Vector）
- 解码器（Decoder）：基于该上下文向量，逐步生成目标序列。

![Seq2Seq结构图](/imgs/nlp/seq2seq.png)
&lt;center&gt;Seq2Seq结构图&lt;/center&gt;

#### 编码器（Encoder）
编码器主要由一个循环神经网络（RNN/LSTM/GRU）构成，其任务是将输入序列的语义信息提取并压缩为一个上下文向量。

在模型处理输入序列时，循环神经网络会依次接收每个token的输入，并在每个时间步步更新隐藏状态。每个隐藏状态都携带了截止到当前位置为止的信息。随着序列推进，信息不断累积，最终会在最后一个时间步形成一个包含整句信息的隐藏状态。

这个最后的隐藏状态就会作为上下文向量（context vector），传递给解码器，用于指导后续的序列生成。

![Seq2Seq encoder](/imgs/nlp/seq2seq_encoder.png)
&lt;center&gt;Seq2Seq RNN encoder&lt;/center&gt;

为增强编码器的理解能力，循环网络也可以采用双向结构（结合前文与后文信息）或多层结构（提取更深的语义特征）。

#### 解码器（Decoder）
解码器主要也由一个循环神经网络（RNN / LSTM / GRU）构成，其任务是基于编码器传递的上下文向量，逐步生成目标序列。

![Seq2Seq decoder](/imgs/nlp/seq2seq_decoder.png)
&lt;center&gt;Seq2Seq RNN decoder&lt;/center&gt;

在生成开始时，循环神经网络以上下文向量作为初始隐藏状态，并接收一个特殊的起始标记 `&lt;sos&gt;（start of sentence）`作为第一个时间步的输入，用于预测第一个 token。

随后，在每一个时间步，模型都会根据前一时刻的隐藏状态和上一步生成的 token，预测当前的输出。这种“将前一步的输出作为下一步输入”的方式被称为自回归生成（Autoregressive Generation），它确保了生成结果的连贯性。

生成过程会持续进行，直到模型生成了一个特殊的结束标记` &lt;eos&gt;（end of sentence）`，表示句子生成完成。

&gt; 说明：起始标记和结束标记会在训练数据中显式添加，模型会在训练中学会何时开始、如何续写，以及何时结束，从而掌握完整的生成流程。

### 模型训练和推理机制
#### 模型训练
- 编码器：通过嵌入层和循环神经网络（RNN / LSTM / GRU）的逐步处理，将整句编码为上下文向量。
- 解码器：解码器使用该上下文向量初始化其隐藏状态，然后逐步生成目标序列。
  - 注意：训练阶段与推理阶段的解码策略是不同的
  - 1、在推理阶段，解码器采用自回归生成方式：每一步的输入是模型自己上一步的预测结果。
  - 2、在训练阶段，通常使用一种称为 Teacher Forcing 的策略，即使用目标序列中真实的前一个token。

#### 模型推理
- 编码器：推理阶段的编码器处理流程与训练时完全一致。
- 解码器：生成方式采用自回归生成（Autoregressive Generation）：每一步的输出会作为下一步的输入，逐步构造完整句子。
  - 1、自回归生成流程：起始标记 `&lt;sos&gt;`，结束标记`&lt;eos&gt;`，上一步生成的词作为当前输入
  - 2、**词选择策略**：
    - 贪心解码（Greedy Decoding）：每一步都选择概率最高的词，局部最优。
    - 束搜索（Beam Search）：每一步保留多个候选词序列（如 beam size = 3），并在扩展后选择得分最高的完整句子。

### 存在问题
在上述 Seq2Seq 架构中，编码器会将整个源句压缩为一个固定长度的上下文向量，并将其作为解码器生成目标序列的唯一参考。这种“压缩再解压”的方式虽然结构简洁，但在实际任务中暴露出两个核心问题：
1. 信息压缩困难，语义表达受限
对于编码器而言，用一个定长向量去表达任意复杂的句子，是一项非常困难的任务。尤其在面对长句时，信息很容易在压缩过程中丢失，导致语义表达不完整。
这种“信息瓶颈”限制了模型在处理长文本或复杂语义结构时的表现。

2. 缺乏动态感知，解码难以精准生成
解码器始终只能基于同一个上下文向量进行生成。
但在实际生成过程中，不同位置的目标词，往往依赖源句中不同的关键信息：
生成主语时，可能更依赖源句的开头；
生成谓语或宾语时，可能需要参考句中或句末内容。
然而在固定表示下，解码器无法“有选择地关注”输入序列的不同部分，只能一视同仁地处理所有信息，从而降低了生成的准确性与灵活性。

## Attention机制
传统的 Seq2Seq 模型中，编码器在处理源句时，无论其长度如何，最终都只能将整句信息压缩为一个固定长度的上下文向量，用作解码器的唯一参考。这种设计存在两个显著问题：
- 信息压缩困难：固定向量难以完整表达长句或复杂语义，容易丢失关键信息；
- 缺乏动态感知：解码器在每一步生成中都只能依赖同一个上下文向量，难以根据不同位置的生成需要灵活提取信息。

为了解决上述问题，研究者引入了** Attention 机制**。其**核心思想**是：

解码器在生成目标序列的每一步时，不再依赖于一个静态的上下文向量，而是根据当前的解码状态，动态地从编码器各时间步的隐藏状态中选取最相关的信息，以辅助当前步的生成。

这种机制赋予模型“对齐”能力，使其能够自动判断源句中哪些位置对当前的目标词更为重要，从而有效缓解信息瓶颈问题，提升生成质量与表达能力。

### 工作原理
注意力机制的核心思想，是解码器在生成目标序列的每一步时，动态地从编码器的各个时间步的隐藏状态中提取当前所需的信息，而不再只依赖一个固定的上下文向量。

![attention](/imgs/nlp/attention.png)
&lt;center&gt;attention&lt;/center&gt;

这一机制通常通过以下 4 个关键步骤实现：

#### 相关性计算
在目标序列生成的每一步，解码器都会计算当前时间步的隐藏状态与编码器各个时间步输出之间的相关性。这些相关性衡量了源句中每个位置对当前生成内容的重要程度，从而决定模型应将多少注意力分配给不同的源位置。
相关性的计算依赖于特定的函数，通常被称为注意力评分函数（attention scoring function）。常见的评分函数实现方式将在下一节中详细介绍。

![attention](/imgs/nlp/attention_step1.png)
&lt;center&gt;attention 相关性计算&lt;/center&gt;

#### 注意力权重计算
得到所有源位置的注意力评分后，使用 Softmax 函数将其归一化为概率分布，作为注意力权重。得分越高的位置，其对应的权重越大，代表模型在当前生成中更关注该位置的信息。

![attention](/imgs/nlp/attention_step2.png)
&lt;center&gt;attention 注意力权重计算&lt;/center&gt;

#### 上下文向量计算
将所有编码器输出按照注意力权重进行加权求和，得到一个上下文向量。这个向量就表示当前时间步，模型从源句中提取出的关键信息。

![attention](/imgs/nlp/attention_step3.png)
&lt;center&gt;attention 上下文向量计算&lt;/center&gt;

#### 解码信息融合
在得到上下文向量后，解码器将其与当前时间步的隐藏状态进行拼接，以融合两者信息，最终通过线性变换和 Softmax，生成当前时间步目标词的概率分布。

![attention](/imgs/nlp/attention_step4.png)
&lt;center&gt;attention 解码信息融合&lt;/center&gt;

### 注意力评分函数
注意力评分函数有多种实现方式。本节将介绍三种常见的计算方法：点积评分（Dot）、通用点积评分（General）和拼接评分（Concat）。它们虽然在结构上各有差异，但本质上都是用于衡量解码器当前隐藏状态与编码器各时间步隐藏状态之间的相关性，并据此分配注意力权重。

#### 点积评分（Dot）
点积评分是注意力机制中最简单、最直接的一种相关性评分方法。它通过计算解码器当前时间步的隐藏状态与编码器每个时间步的隐藏状态的点积，来衡量二者之间的相关性：

![attention_score_function_dot](/imgs/nlp/attention_score_function_dot.png)

其含义可以理解为：如果两个向量方向越一致（即越接近），它们的点积就越大，表示相关性越强，模型应当给予更多注意力。

#### 通用点积评分（General）
通用点积评分在点积的基础上引入了一个可学习的权重矩阵W,用于先对编码器隐藏状态进行线性变换，再与解码器隐藏状态进行点积：

![attention_score_function_general](/imgs/nlp/attention_score_function_general.png)

该方法的设计动机主要是为了解决编码器和解码器隐藏状态维度不一致的问题。通过引入权重矩阵W，不仅实现了维度对齐，也增强了模型对编码器输出的适应能力，从而提升了注意力机制的表达能力。

#### 拼接评分（Concat）
拼接评分是一种表达能力更强的相关性评分方法。它的核心思想是：将解码器当前隐藏状态与编码器每个时间步的隐藏状态拼接为一个长向量，经过线性变换和非线性激活，最后用一个向量进行投影，得到最终打分值：

![attention_score_function_concat](/imgs/nlp/attention_score_function_concat.png)

相比前两种方法，Concat 评分方式在建模能力上更强。它不仅考虑了两个状态的数值关系，还引入非线性变换，能够捕捉更复杂的交互模式，更适合处理对齐关系复杂的任务场景。

### 存在问题
尽管注意力机制极大地增强了 Seq2Seq 模型的建模能力，但由于其核心依然依赖于 RNN 结构，仍面临两个根本性问题：
- 计算过程无法并行
  - RNN 的时间步之间存在强依赖，必须顺序执行，限制了训练效率和硬件资源的利用率。
- 长期依赖问题仍未根除
  - 模型需要跨多个时间步传递信息，对于超长序列，训练过程中容易出现梯度消失，难以有效建模长距离依赖关系。

## Transformer模型
此前的Seq2Seq模型通过注意力机制取得了一定提升，但由于整体结构仍依赖 RNN，依然存在计算效率低、难以建模长距离依赖等结构性限制。

为了解决这些问题，Google在2017 年发表一篇论文《[Attention Is All You Need](https://arxiv.org/pdf/1706.03762)》，提出了一种全新的模型架构——Transformer。该模型完全摒弃了 RNN 结构，转而使用注意力机制直接建模序列中各位置之间的关系。通过这种方式，Transformer不仅显著提升了训练效率，也增强了模型对长距离依赖的建模能力。

Transformer 的提出对自然语言处理产生了深远影响。在机器翻译任务中，它首次超越了 RNN 模型的表现，并成为后续各类预训练语言模型的基础框架，如 BERT、GPT 等。这些模型推动 NLP 进入了“预训练 + 微调”的新时代，极大地提升了模型在多种任务上的通用性与性能。如今，Transformer 架构不仅广泛应用于 NLP，还扩展至语音识别、图像处理、代码生成等多个领域，成为现代深度学习中最具代表性的通用模型之一。

### 模型结构
#### 核心思想
在seq2seq基础上，通过Attention等替代RNN进行位置关系建模，便于并行计算，更适合捕捉长距离依赖。

![transformer_demo](/imgs/nlp/transformer_demo.png)

#### 整体结构
标准的 Transformer 模型通常包含 6个编码器层和 6 个解码器层。

![transformer_demo2](/imgs/nlp/transformer.png)

### 编码器（Encoder）
![transformer_encoder](/imgs/nlp/transformer_encoder.png)

- Transformer 的编码器用于理解输入序列的语义信息，并生成每个token的上下文表示，为解码器生成目标序列提供基础。
- 编码器由多个结构相同的编码器层（Encoder Layer）堆叠而成。
- 每个 Encoder Layer的主要任务都是对其输入序列进行上下文建模，使每个位置的表示都能融合来自整个序列的全局信息。
- 每个 Encoder Layer都包含两个子层（sublayer），分别是自注意力子层（Self-Attention Sublayer）和前馈神经网络子层（Feed-Forward Sublayer）。
  - Self-Attention：用于捕捉序列中各位置之间的依赖关系。
  - Feed-Forward：用于对每个位置的表示进行非线性变换，从而提升模型的表达能力。

#### 自注意力层（Self-Attention）/ 多头自注意力（multi Self-Attention）
之所以被称为“自”注意力，是因为模型在计算每个位置的表示时，所参考的信息全部来自同一个输入序列本身，而不是来自另一个序列。

##### 自注意力计算过程
自注意力的完整计算过程如下：

$$
Attention(Q,K,V)=Softmax(\frac{Q \cdot K^{T}}{\sqrt{d_k}})V
$$

![transformer_attention_1](/imgs/nlp/transformer_attention_1.png)

![transformer_attention_2](/imgs/nlp/transformer_attention_2.png)

**（1）生成Query、Key、Value向量**

自注意力机制的第一步，是将输入序列中的每个位置表示映射为三个不同的向量，分别是 查询（Query）、键（Key） 和 值（Value）。其中$W_q、W_k、W_v$均为可学习的参数矩阵。
- Query：表示当前词的用于发起注意力匹配的向量； $Q=X W_x$
- Key：表示序列中每个位置的内容标识，用于与 Query 进行匹配； $Q=X W_k$
- Value：表示该位置携带的信息，用于加权汇总得到新的表示。 $Q=X W_v$

**（2）计算位置间相关性**

完成 Query、Key、Value 向量的生成后，模型会使用每个位置的 Query 向量与所有位置的 Key 向量进行相关性评分。

评分函数采用向量点积形式。由于在高维空间中，点积的数值可能过大，会影响 softmax 的稳定性，因此在实际计算中对结果进行了缩放。最终的评分函数为：
$$
Score(i,j) = \frac{q_i \cdot k_j}{\sqrt{d_k}} 
$$
其中$d_k$是key向量的维度，用于缩放点积的幅度（假设q、k方差为1，则$q_i \cdot k_j$的方差为$d_k$）。

**（3）计算注意力权重**

在得到每个位置与所有位置之间的相关性评分后，模型会使用 softmax 函数进行归一化，确保每个位置对所有位置的关注程度之和为 1，从而形成一个有效的加权分布。

**（4）加权汇总生成输出**

最后，模型会根据注意力权重对所有位置的 Value 向量进行加权求和，得到每个位置融合全局信息后的新表示。

##### 多头自注意力计算过程
自注意力机制通过 Query、Key 和 Value 向量计算每个位置与其他位置之间的依赖关系，使模型能够有效捕捉序列中的全局信息。

然而，自然语言本身具有高度的语义复杂性，一个句子往往同时包含多种类型的语义关系。要准确理解这类句子，模型需要同时识别并建模多种层次和类型的依赖关系。但这些信息很难通过单一视角或一套注意力机制完整捕捉。

为此，Transformer 引入了多头注意力机制（Multi-Head Attention）。其核心思想是通过多组独立的 Query、Key、Value 投影，让不同注意力头分别专注于不同的语义关系，最后将各头的输出拼接融合。

多头注意力的计算过程如下：

**（1）分别计算各头注意力**

每个 Self-Attention Head 独立计算一套注意力输出。

![Multi_Head_Attention_1](/imgs/nlp/Multi_Head_Attention_1.png)

**（2）合并多头注意力**

多个输出矩阵按维度拼接，再乘以得到最终多头注意力的输出。

![Multi_Head_Attention_2](/imgs/nlp/Multi_Head_Attention_2.png)

#### 前馈神经网络层（Feed-Forward Network，FFN）
前馈神经网络（Feed-Forward Network，简称 FFN）是 Transformer 编码器中每个子层的重要组成部分，紧接在多头注意力子层之后。它通过对每个位置的表示进行**逐位置、非线性**的特征变换，进一步提升模型对复杂语义的建模能力。

一个标准的 FFN 子层包含两个线性变换和一个非线性激活函数，中间通常使用 ReLU激活。其计算公式如下：

$$
FFN(x)=Linear_2(ReLU(Linear_1(x)))=W_2\cdot ReLU(W_1\cdot x+b_1)+b_2
$$

![transformer_ffn](/imgs/nlp/transformer_ffn.png)

#### 残差连接与层归一化（Residual Connection &amp; LayerNorm）
在 Transformer 的每个编码器层中，每个子层，包括自注意力子层和前馈神经网络子层，其输出都要经过残差连接（Residual Connection）和层归一化（Layer Normalization）处理。这两者是深层神经网络中常用的结构，用于缓解模型训练中的梯度消失、收敛困难等问题，对于Transformer能够堆叠多层至关重要。

![transformer_res_ln](/imgs/nlp/transformer_res_ln.png)

##### 残差连接

残差连接（Residual Connection，也称“跳跃连接”或“捷径连接”）最初在计算机视觉领域被提出，用于缓解深层神经网络中的梯度消失问题。其核心思想是：

将子层的输入直接与其输出相加，形成一条跨越子层的“捷径”，其数学形式为：
$$
y=x+SubLayer(x)
$$
残差连接确保反向传播时，梯度至少有一条稳定通路可回传，是深层网络可稳定训练的关键结构。

##### 层归一化

每个子层在残差连接之后都会进行层归一化（Layer Normalization，简称 LayerNorm）。它的主要作用是规范输入序列中每个token的特征分布（某个token的表示可能在不同维度上有较大数值差异），
提升模型训练的稳定性。

该操作会将每个token的向量调整为均值为 0、方差为 1 的规范分布，具体的计算公式如下：

假如某个token的特征向量为$x=[x^1, x^2,...,x^d]$，

1. 均值计算：

计算该向量在所有特征维度上的平均值
$$
\mu = \frac{1}{d} \sum_{i=1}^{d} x^i
$$
其中$d$为特征维度（向量长度）。

2. 标准差计算
计算向量各维度的标准差
$$
\sigma = \sqrt{\frac{1}{d} \sum_{i=1}^{d} (x^i-\mu)^2}
$$

3. 标准化变换
将每个特征值转换为均值为 0、方差为 1 的标准正态分布；
$$
\hat{x^i}=\frac{x^i-\mu}{\sigma+\epsilon}
$$
$\epsilon$为一个小的常数，防止出现除以0的情况。

4. 缩放和平移
让模型可以学习在归一化后的基础上进行适当的调整，保证归一化不会限制模型的表示能力。
$$
LayerNorm(x^i)=\gamma^i\cdot\hat{x^i}+\beta^{i}
$$
$\gamma^i$和$\beta^i$为可学习参数。

##### Pre_Norm
在更深层次网络使用效果可能更好，在pytorch的nn.Transformer中通过norm_first参数设置。

Transformer的被称之为`Post_Norm`：
$$
LayerNorm(x+SubLayer(x))
$$

Pre_Norm方法：
$$
x+SubLayer(LayerNorm(x))
$$

#### 位置编码（Positional Encoding）
Transformer 模型完全摒弃了 RNN 结构，可以并行处理所有位置的信息，但也无法天然地捕捉词语之间的顺序关系。

为了解决这一问题，Transformer 引入了一个关键机制——位置编码（Positional Encoding）。该机制为每个词引入一个表示其位置信息的向量，并将其与对应的词向量相加，作为模型输入的一部分。

**编码策略**

1. 最直接的方式：使用绝对位置编号来表示每个词的位置。
   - 问题：越靠后的 token 位置编码就越大，若直接与词向量相加，会造成数值倾斜，让模型更关注位置，而忽视词义。
2. 位置编号归一化为[0, 1]区间，例如用$\frac{Pos}{T}$表示位置，其中 T是句子长度。
   - 问题：相同位置的词在不同长度句子中的位置编码不再一致。
3. Transformer：使用了一种基于正弦（sin）和余弦（cos）函数的位置编码方式，具体定义如下：
$$
PE_{（pos,2i）}=\sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})\\\\
PE_{（pos,2i+1）}=\cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})
$$
其中：
- pos是当前词在序列中的位置；
- i用于表示位置编码向量的维度索引，2i表示偶数维，2i+1表示奇数维；
- $d_{model}$是词向量的维度大小。

Transformer提出的这种编码方式不依赖任何可学习参数，数值稳定，并具备以下优势：
- 所有值都在[−1,1]范围内，数值稳定
- 编码方式固定、可预计算，无需训练；
- 相同位置的编码在不同句子中保持一致；
- 编码之间具有数学规律，便于模型在注意力机制中感知词语之间的相对位置关系。

### 解码器（Decoder）
编码器也由多个结构相同的解码器层堆叠组成。每个Decoder Layer都包含三个子层，分别是Masked自注意力子层、编码器-解码器注意力子层（Encoder-Decoder Attention）和前馈神经网络子层（Feed-Forward Network）。
- Masked自注意力子层（Masked Self Attention）：
  - 用于建模当前位置与前文词之间的依赖关系。
  - 为了在训练时模拟逐词生成的过程，引入遮盖机制（Mask），限制每个位置只能关注它前面的词。
- 编码器-解码器注意力子层（Encoder-Decoder Attention）：
  - 用于建模当前解码位置与源序列各位置之间的依赖关系。
  - 通过注意力机制，模型能够根据当前状态从编码器的输出中提取相关上下文信息（相当于 Seq2Seq 模型中的 Attention 机制）。
- 前馈神经网络子层（Feed-Forward Network）：与编码器中结构完全一致，对每个位置的表示进行非线性变换，增强模型的表达能力。

每个子层后也都配有残差连接与层归一化（Layer Normalization），结构设计与编码器保持一致，确保训练的稳定性和效率。

此外，解码器在输入端同样需要加入位置编码（Positional Encoding），用于提供序列中的位置信息，其计算方式与编码器中相同。
在输出端，解码器的隐藏向量会送入一个线性变换层（Linear），映射为词表大小的向量，并通过 Softmax 生成一个概率分布，用于预测当前应输出的词。

#### Masked自注意力子层（Masked Self Attention）
- 并行计算：一次性输入完整目标序列，同时预测每个位置的词。
- 遮盖机制（Mask）：限制每个位置只能关注它前面的词。

![encoder_mask](/imgs/nlp/encoder_mask.png)

&gt; 注意：在推理阶段，我们只使用解码器最后一个位置的输出作为当前步的预测结果。

Mask 机制的实现非常简单：只需将注意力得分矩阵中当前位置对其后续位置的评分设置为`−∞`，如下图所示：

![mask_1](/imgs/nlp/mask_1.png)

这样，在经过 softmax 运算后，这些位置的权重会趋近于 0。最终在加权求和时，来自未来位置的信息几乎不会参与计算，从而实现了“**当前词只能看到它前面的词**”的约束。如下图所示：

![mask_2](/imgs/nlp/mask_2.png)

#### 编码器-解码器注意力子层（Encoder-Decoder Attention）
该子层的主要作用是：建模当前解码位置与源语言序列中各位置之间的依赖关系，帮助模型在生成目标词时有效地参考输入内容，相当于Seq2Seq模型中的注意力机制。

编码器-解码器注意力的核心机制与前面讲过的自注意力机制完全一致，区别仅在于：
- Query 来自解码器当前的输入表示，即当前生成状态；
- Key和Value 来自编码器的输出表示，即整个源序列的上下文。

### 训练与推理机制
Transformer 的训练与推理都基于自回归生成机制（Autoregressive Generation）：模型逐步生成目标序列中的每一个词。然而，在实现方式上，训练与推理存在明显区别。

#### 模型训练
并行计算+mask机制

#### 模型推理
推理阶段，模型每一步都要重新输入当前已生成的全部词，通过自注意力机制建模上下文关系，预测下一个词。

模型会基于**完整前文重新计算注意力分布**，生成当前步的输出。由于每一步的输入依赖前一步结果，整个过程必须**顺序执行，无法并行**。

每步输出的是一个词的概率分布，最终生成结果也可使用不同的**解码策略**
- 贪心搜索（Greedy）
- 束搜索（Beam Search）

### Pytorch API使用
``` python
from torch import nn

# 经典 Transformer 结构
transformer = nn.Transformer(
    d_model=512, 
    nhead=8, 
    num_encoder_layers=6, 
    num_decoder_layers=6, 
    batch_first=True
)
```

PyTorch 中的 Transformer 模块由以下几个核心类构成：

#### nn.Transformer
&gt; [nn.Transformer](https://docs.pytorch.org/docs/stable/generated/torch.nn.Transformer.html)

封装了完整的 Transformer架构，由编码器和解码器组成。作为顶层接口，适用于需要同时使用编码器和解码器的任务，如机器翻译。支持用户通过参数自定义层数、注意力头数、隐藏维度等模型结构。

#### nn.TransformerEncoder
&gt; [nn.TransformerEncoder](https://docs.pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html)

实现了Transformer编码器结构，由多个编码器层的堆叠而成，用于将输入序列编码为上下文相关的表示。

#### nn.TransformerDecoder
&gt; [nn.TransformerDecoder](https://docs.pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html)

实现了Transformer解码器结构，由多个解码器层堆叠而成，用于基于编码结果逐步生成目标序列。

#### nn.TransformerEncoderLayer
&gt; [nn.TransformerEncoderLayer](https://docs.pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html)

实现了单个编码器层结构，包含一个多头自注意力子层和一个前馈神经网络子层，两者均带有残差连接和 LayerNorm。

#### nn.TransformerDecoderLayer
&gt; [nn.TransformerDecoderLayer](https://docs.pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html)
实现了单个解码器层结构，包含自注意力、编码器-解码器注意力、前馈子层，同样配有残差连接和 LayerNorm。

## 常用工具
### 深度学习框架
#### Pytorch
常用的深度学习框架

#### TensorFlow

### 机器学习库
#### sklearn
它提供了丰富的工具和算法，帮助用户轻松构建、训练和评估机器学习模型。

数据处理：
- 数据集切分：train_test_split

### 数据处理
- numpy
- pandas
- scipy

### 图形化交互
- TensorBoard：可以监听log数据，图形化展示训练曲线
- tqdm：进度条
- netron：模型结构可视化

# References
- [尚硅谷大模型技术之NLP1.0.3.pdf](/pdf/nlp/尚硅谷大模型技术之NLP1.0.3.pdf)
- [尚硅谷大模型技术之NLP1.0.3.docx](/pdf/nlp/尚硅谷大模型技术之NLP1.0.3.docx)
&lt;/div&gt;
&lt;/div&gt;


</description>
        </item>
        
    </channel>
</rss>
