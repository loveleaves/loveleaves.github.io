<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>LLM on 安哲睿</title>
        <link>https://loveleaves.github.io/categories/llm/</link>
        <description>Recent content in LLM on 安哲睿</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Andrew Stark</copyright>
        <lastBuildDate>Thu, 18 Dec 2025 23:01:03 +0800</lastBuildDate><atom:link href="https://loveleaves.github.io/categories/llm/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>【NLP】 NLP 手册</title>
        <link>https://loveleaves.github.io/p/nlp_intro/</link>
        <pubDate>Wed, 09 Oct 2024 00:00:00 +0000</pubDate>
        
        <guid>https://loveleaves.github.io/p/nlp_intro/</guid>
        <description>&lt;div id=&#34;verification&#34; style=&#34; display: flex;
  flex-direction: column;
  align-items: center;
  margin-top: 20vh;&#34;&gt;
    &lt;div id=&#34;entry-box&#34;&gt;
        
        &lt;div id=&#34;secret-word&#34; style=&#34;  font-size: 1.5rem;
    margin-bottom: 1rem;&#34;&gt;Unlock to view this content.&lt;/div&gt;
        &lt;form id=&#34;password-form&#34; onsubmit=&#34;checkPassword(); return false;&#34; style=&#34; display: flex;
    flex-direction: column;
    width: auto;&#34;&gt;
            &lt;input type=&#34;password&#34; name=&#34;password&#34; id=&#34;password&#34; placeholder=&#34;Please enter password&#34; required style=&#34;margin-bottom: 1.5rem;
      padding: 10px 20px;&#34;&gt;
            &lt;input type=&#34;hidden&#34; name=&#34;encryptedPassword&#34; id=&#34;encrypted-password&#34;&gt;
            &lt;input type=&#34;submit&#34; value=&#34;Submit&#34; name=&#34;submit&#34; id=&#34;secret-submit&#34; style=&#34; padding: 10px 20px;
      cursor: pointer;&#34; onmouseover=&#34;this.style.backgroundColor=&#39;#FAB005&#39;;&#34;
                   onmouseout=&#34;this.style.backgroundColor=&#39;&#39;;&#34;&gt;
        &lt;/form&gt;
    &lt;/div&gt;
    &lt;div id=&#34;secret&#34; style=&#34;display: none&#34; password=&#34;nopainsnogains&#34;&gt;

# NLP基础
## 导论
### 定义
自然语言处理（Natural Language Processing, NLP），是人工智能领域的一个重要分支。自然语言，指人类日常使用的语言（如中文、英文），NLP 的目标是让计算机“理解”或“使用”这些语言。

### 常见任务

常见任务=》平安银行算法经历=》简历
自然语言处理包含多个典型任务，主要可分为以下几类：

#### 文本分类
对整段文本进行判断或归类。

常见应用：情感分析（判断评价是正面还是负面）、垃圾邮件识别、新闻主题分类等。

#### 序列标注
对一段文本中的每个词或字打上标签。

常见应用：命名实体识别（找出人名、地名、手机号码等）

#### 文本生成
根据已有内容生成新的自然语言文本。

常见应用：自动写作、摘要生成、智能回复、对话系统等。

#### 信息抽取
从文本中提取出结构化的信息。

常见应用：给出一段文本和一个问题，从中抽取答案。

#### 文本转换
将一种文本转换为另一种形式。

常见应用：机器翻译，摘要生成等。

### 技术演进历史

#### 规则系统阶段
在20世纪50年代至80年代初，自然语言处理主要依赖人工编写的语言规则，这些规则由语言学家和程序员手动制定。这一阶段的代表系统有早期的机器翻译系统（如Georgetown-IBM实验）和ELIZA聊天机器人。这类系统在特定领域表现良好，但缺乏通用性，扩展性差，对语言的复杂性处理有限。

#### 统计方法阶段
90年代，随着计算能力的提升和语料资源的积累，统计方法逐渐成为主流。通过对大量文本数据进行概率建模，系统能够“学习”语言中的模式和规律。典型方法包括n-gram模型、隐马尔可夫模型（HMM）和最大熵模型。这一阶段标志着从“专家经验”向“数据驱动”方法的转变。

#### 机器学习阶段
进入21世纪，NLP技术逐步引入传统机器学习方法，如逻辑回归、支持向量机（SVM）、决策树、条件随机场（CRF）等。这些方法在命名实体识别、文本分类等任务上表现出色。在此阶段，特征工程成为关键环节，研究者需要设计大量手工特征来提升模型性能。该阶段的特点是学习算法更为复杂，模型泛化能力增强。

#### 深度学习阶段
自2010年代中期开始，深度学习在NLP中迅速崛起。基于神经网络的模型RNN、LSTM、GRU等，取代了传统手工特征工程，能够从海量数据中自动提取语义表示。随后，Transformer架构的提出极大提升了语言理解与生成的能力，深度学习不仅在精度上实现突破，也推动了预训练语言模型（如GPT、BERT等）和迁移学习的发展，使NLP技术更通用、更强大。

## 文本表示（Representation）
### 概述
文本表示是将自然语言转化为计算机能够理解的数值形式，是绝大多数自然语言处理（NLP）任务的基础步骤。

早期的文本表示方法（如词袋模型）通常将整段文本编码为一个向量。这类方法实现简单、计算高效，但存在明显的局限性——表达语序和上下文语义的能力较弱。因此，现代 NLP 技术逐渐引入更加精细和表达力更强的文本表示方法，以更有效地建模语言的结构和含义。文本表示的第一步通常是分词和词表构建。

1. 分词（Tokenization）是将原始文本切分为若干具有独立语义的最小单元（即token）的过程，是所有 NLP 任务的起点。
2. 词表（Vocabulary）是由语料库构建出的、包含模型可识别 token 的集合。词表中每个token都分配有唯一的 ID，并支持 token 与 ID 之间的双向映射。

在后续训练或预测过程中，模型会首先对输入文本进行分词，再通过词表将每个 token 映射为其对应的 ID。接着，这些 ID 会被输入嵌入层（Embedding Layer），转换为低维稠密的向量表示（即词向量）

### 分词（Tokenization）
**分词粒度**
- 词级（Word-Level）分词
- 字符级（Character-Level）分词
- 子词级（Subword‑Level）分词：目前主流方法

以下是 tokenization 过程的高度概括：

![tokenization](/imgs/nlp/tokenization.png)

在分词（根据其模型）之前，tokenizer 需要进行两个步骤： **标准化（normalization）** 和 **预分词（pre-tokenization）** 。

**标准化**步骤涉及一些常规清理，例如删除不必要的空格、小写和“/”或删除重音符号。

tokenizer 一般不会在原始文本上进行训练。因此，我们首先需要将文本拆分为更小的实体，例如单词。这就是**预分词**步骤的作用。基于单词的 tokenizer 可以简单地根据空格和标点符号将原始文本拆分为单词。这些词将是 tokenizer 在训练期间可以学习的子词的边界。

三种主要的子词 tokenization 算法：BPE（由 GPT-2 等使用）、WordPiece（由 BERT 使用）和 Unigram（由 T5 等使用）。

模型 | BPE | WordPiece | Unigram
:----:|:---:|:---------:|:------:
训练 | 从小型词汇表开始，学习合并 token 的规则 | 从小型词汇表开始，学习合并 token 的规则 | 从大型词汇表开始，学习删除 token 的规则
训练步骤 | 合并对应最常见的 token 对 | 合并对应得分最高的 token 对，优先考虑每个独立 token 出现频率较低的对 | 删除会在整个语料库上最小化损失的词汇表中的所有 token
学习 | 合并规则和词汇表 | 仅词汇表 | 含有每个 token 分数的词汇表
编码 | 将一个单词分割成字符并使用在训练过程中学到的合并 | 从开始处找到词汇表中的最长子词，然后对其余部分做同样的事 | 使用在训练中学到找到最可能的 token 分割方式

#### BPE（Byte Pair Encoding）/BBPE（Byte Level Byte Pair Encoding）
字节对编码（BPE）最初被开发为一种压缩文本的算法，然后在预训练 GPT 模型时被 OpenAI 用于 tokenization。许多 Transformer 模型都使用它，包括 GPT、GPT-2、RoBERTa、BART 和 DeBERTa。

BPE 训练首先计算语料库中使用的唯一单词集合（在完成标准化和预分词步骤之后），然后取出用来编写这些词的所有符号来构建词汇表。举一个非常简单的例子，假设我们的语料库使用了这五个词：
``` python
&#34;hug&#34;, &#34;pug&#34;, &#34;pun&#34;, &#34;bun&#34;, &#34;hugs&#34;
```
基础单词集合将是 [&#34;b&#34;, &#34;g&#34;, &#34;h&#34;, &#34;n&#34;, &#34;p&#34;, &#34;s&#34;, &#34;u&#34;] 。在实际应用中，基本词汇表将至少包含所有 ASCII 字符，可能还包含一些 Unicode 字符。如果你正在 tokenization 不在训练语料库中的字符，则该字符将转换为未知 tokens，这就是为什么许多 NLP 模型在分析带有表情符号的内容的结果非常糟糕的原因之一。

&gt; GPT-2 和 RoBERTa （这两者非常相似）的 tokenizer 有一个巧妙的方法来处理这个问题：他们不把单词看成是用 Unicode 字符编写的，而是用字节编写的。这样，基本词汇表的大小很小（256），但是能包含几乎所有你能想象的字符，而不会最终转换为未知 tokens 这个技巧被称为 字节级（byte-level） BPE 。

获得这个基础单词集合后，我们通过学习 **合并（merges）** 来添加新的 tokens 直到达到期望的词汇表大小。合并是将现有词汇表中的两个元素合并为一个新元素的规则。所以，一开始会创建出含有两个字符的 tokens 然后，随着训练的进展，会产生更长的子词。

在分词器训练期间的任何一步，BPE 算法都会搜索最常见的现有 tokens 对 （在这里，“对”是指一个词中的两个连续 tokens ）。最常见的这一对会被合并，然后我们重复这个过程。

**步骤**
1. 标准化
2. 预分词
3. 将单词拆分为单个字符
4. 根据学习的合并规则，按顺序合并拆分的字符

**代码实现**
``` python
# 字母表
alphabet = []
for word in word_freqs.keys():
    for letter in word:
        if letter not in alphabet:
            alphabet.append(letter)
alphabet.sort()
vocab = [&#34;&lt;|endoftext|&gt;&#34;] + alphabet.copy() # GPT-2 模型的特殊 tokens

# 合并规则，计算出现频率，之后可以按相邻频率最高的pair对合并
def compute_pair_freqs(splits):
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            pair_freqs[pair] += freq
    return pair_freqs

# 设定词表上限，得到训练词表 vocab 及合并规则 merges
vocab_size = 50
while len(vocab) &lt; vocab_size:
    pair_freqs = compute_pair_freqs(splits)
    best_pair = &#34;&#34;
    max_freq = None
    for pair, freq in pair_freqs.items():
        if max_freq is None or max_freq &lt; freq:
            best_pair = pair
            max_freq = freq
    if not best_pair: # reach max size
        break
    splits = merge_pair(*best_pair, splits)
    merges[best_pair] = best_pair[0] + best_pair[1]
    vocab.append(best_pair[0] + best_pair[1])

# 通过vocab、merges
def tokenize(word):
    splits = [[l for l in word]]
    for pair, merge in merges.items():
        for idx, split in enumerate(splits):
            i = 0
            while i &lt; len(split) - 1:
                if split[i] == pair[0] and split[i + 1] == pair[1]:
                    split = split[:i] + [merge] + split[i + 2 :]
                else:
                    i += 1
            splits[idx] = split

    return sum(splits, [])
```

**总结**
- 使用频率进行合并

#### WordPiece
&gt; [WordPiece tokenization 算法](https://hf-mirror.com/learn/llm-course/zh-CN/chapter6/6)
WordPiece 是 Google 开发的用于 BERT 预训练的分词算法。自此之后，很多基于 BERT 的 Transformer 模型都复用了这种方法，比如 DistilBERT，MobileBERT，Funnel Transformers 和 MPNET。它在训练方面与 BPE 非常类似，但实际的分词方法有所不同。

与BPE 一样，WordPiece 也是从包含模型使用的特殊 tokens 和初始字母表的小词汇表开始的。由于它是通过添加前缀（如 BERT 中的 ## ）来识别子词的，每个词最初都会通过在词内部所有字符前添加该前缀进行分割。因此，例如 &#34;word&#34; 将被这样分割：
``` python
w ##o ##r ##d
```
因此，初始字母表包含所有出现在单词第一个位置的字符，以及出现在单词内部并带有 WordPiece 前缀的字符。

然后，同样像 BPE 一样，WordPiece 会学习合并规则。主要的不同之处在于合并对的选择方式。WordPiece 不是选择频率最高的对，而是对每对计算一个得分，使用以下公式：
&lt;center&gt;
score=(freq_of_pair)/(freq_of_first_element×freq_of_second_element)
&lt;/center&gt;

**代码实现**
``` python
# 字母表
alphabet = []
for word in word_freqs.keys():
    if word[0] not in alphabet:
        alphabet.append(word[0])
    for letter in word[1:]:
        if f&#34;##{letter}&#34; not in alphabet:
            alphabet.append(f&#34;##{letter}&#34;)
vocab = [&#34;[PAD]&#34;, &#34;[UNK]&#34;, &#34;[CLS]&#34;, &#34;[SEP]&#34;, &#34;[MASK]&#34;] + alphabet.copy()

# 合并规则，计算pair分数
def compute_pair_scores(splits):
    letter_freqs = defaultdict(int)
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            letter_freqs[split[0]] += freq
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            letter_freqs[split[i]] += freq
            pair_freqs[pair] += freq
        letter_freqs[split[-1]] += freq

    scores = {
        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])
        for pair, freq in pair_freqs.items()
    }
    return scores

# 最大子词查找
def encode_word(word):
    tokens = []
    while len(word) &gt; 0:
        i = len(word)
        while i &gt; 0 and word[:i] not in vocab:
            i -= 1
        if i == 0:
            return [&#34;[UNK]&#34;]
        tokens.append(word[:i])
        word = word[i:]
        if len(word) &gt; 0:
            word = f&#34;##{word}&#34;
    return tokens
```

**总结**
- 通过`##`识别子词
- WordPiece 和 BPE 的分词方式有所不同，WordPiece 只保存最终词汇表，而不保存学习到的合并规则。WordPiece 从待分词的词开始，找到词汇表中最长的子词，然后在其处分割。
- 当分词过程中无法在词汇库中找到该子词时，**整个词**会被标记为 unknown（未知），BPE 只会将不在词汇库中的**单个字符**标记为 unknown。

#### Unigram
&gt; [Unigram tokenization 算法](https://hf-mirror.com/learn/llm-course/zh-CN/chapter6/7)

Unigram 算法常用于 SentencePiece 中，该切分算法被 AlBERT，T5，mBART，Big Bird 和 XLNet 等模型广泛采用。

**训练**

与BPE 和 WordPiece 相比，Unigram 的工作方式正好相反：它从一个**大词汇库**开始，然后逐步删除词汇，直到达到目标词汇库大小。构建基础词汇库有多种方法：例如，我们可以选取预切分词汇中最常见的子串，或者在具有大词汇量的初始语料库上进行 BPE 得到一个初始词库。

在训练的每一步，Unigram 算法都会在给定当前词汇的情况下计算语料库的损失。然后，对于词汇表中的每个符号，算法计算如果删除该符号，整体损失会增加多少，并寻找删除后损失增加最少的符号。这些符号对语料库的整体损失影响较小，因此从某种意义上说，它们“相对不必要”并且是移除的最佳候选者。

这个过程非常消耗计算资源，因此我们不只是删除与最低损失增长相关的单个符号，而是删除与最低损失增长相关的百分之/p （p 是一个可以控制的超参数，通常是 10 或 20）的符号。然后重复此过程，直到词汇库达到所需大小。

注意，我们永远不会删除基础的单个字符，以确保任何词都能被切分。

然而，这仍然有些模糊：算法的主要部分是在词汇库中计算语料库的损失并观察当我们从词汇库中移除一些符号时损失如何变化，但我们尚未解释如何做到这一点。这一步依赖于 Unigram 模型的切分算法。

#### 常用工具
按照实现方式大致可以分为如下两类：
- 一类是基于词典或模型的传统方法，主要以“词”为单位进行切分；
- 另一类是基于子词建模算法（如BPE）的方式，从数据中自动学习高频字组合，构建子词词表。

前者的代表工具包括 jieba、HanLP等，这些工具广泛应用于传统 NLP 任务中。 

后者的代表工具包括 Hugging Face Tokenizer、SentencePiece、tiktoken等，常用于大规模预训练语言模型中。

##### SentencePiece
&gt; [官网](https://github.com/google/sentencepiece)

**主要特性**
- 多分词粒度：支持BPE、ULM子词算法，也支持char, word分词；
- 多语言：以unicode方式编码字符，将所有的输入（英文、中文等不同语言）都转化为unicode字符，解决了多语言编码方式不同的问题；
- 编解码的可逆性：之前几种分词算法对空格的处理略显粗暴，有时是无法还原的。Sentencepiece显式地将空白作为基本标记来处理，用一个元符号 “▁”（ U+2581 ）转义空白，这样就可以实现简单且可逆的编解码；
- 无须Pre-tokenization：Sentencepiece可以直接从raw text/setences进行训练，无须Pre-tokenization
- Fast and lightweight；

##### tokenizers库
&gt; [官网](https://hugging-face.cn/docs/tokenizers/index)

tokenizers是transformers的兄弟库，实现了当前最常用的分词器。

主要特点：
- 使用当今最常用的分词器来训练新词汇表和进行分词。
- 得益于 Rust 实现，速度极快（包括训练和分词）。在服务器 CPU 上，对 1GB 的文本进行分词耗时不到 20 秒。
- 易于使用，同时也极其通用。
- 专为研究和生产而设计。
- 完整的对齐跟踪。即使进行了破坏性的规范化，也始终可以获取到与任意词元对应的原始句子部分。
- 完成所有预处理：截断、填充、添加模型所需的特殊词元。

分词流程
- 归一化
- 预分词
- 模型
- 后处理

#### LLM中的分词器
##### BERT的分词器
&gt; [代码](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/tokenization_bert.py)
&gt; 
&gt; [文档](https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/bert#transformers.BertTokenizer)

BERT的分词器由两个部分组成：
- BasicTokenizer：
  - 转成 unicode：Python3，输入为str时，可以省略这一步
  - _clean_text：去除各种奇怪字符
  - _tokenize_chinese_chars：中文按字拆开
  - whitespace_tokenize：空格分词
  - _run_strip_accents：去掉变音符号
  - _run_split_on_punc：标点分词
  - 再次空格分词：whitespace_tokenize(&#34; &#34;.join(split_tokens))，先用空格join再按空白分词，可以去掉连续空格
- WordpieceTokenizer：
  - 贪心最大匹配：用双指针实现；

### 词表示（word representation）
&gt; 详见：`尚硅谷大模型技术之NLP1.0.3`的3.3


词表示的发展经历了从稀疏的one-hot编码，到稠密的语义化词向量，再到近年来的上下文相关的词表示。不同的词表示方法在表达能力、语义建模、上下文适应性等方面存在显著差异。

#### One-hot编码
缺点：
- 随着词表规模的扩大，向量维度会迅速膨胀，导致计算效率低下
- 无法体现词与词之间的语义关系（如相关词向量正交为0）

#### 语义化词向量 Word2Vec
它通过对大规模语料的学习，为每个词生成一个具有语义意义的稠密向量表示。这些向量能够在连续空间中表达词与词之间的关系，使得“意思相近”的词在空间中距离更近。

![CBOW](/imgs/nlp/cbow.png)
&lt;center&gt;CBOW&lt;/center&gt;

![Skip-Gram](/imgs/nlp/skip_gram.png)
&lt;center&gt;Skip-Gram&lt;/center&gt;

#### 上下文相关词表示（Contextual Word Representations）
虽然像Word2Vec这样的模型已经能够为词语提供具有语义的向量表示，但是它只为每个词分配一个固定的向量表示，不论它在句中出现的语境如何。这种表示被称为静态词向量（static embeddings）。

然而，语言的表达极其灵活，一个词在不同上下文中可能有完全不同的含义。如吃的苹果和苹果手机。

上下文相关词表示（Contextual Word Representations），是指词语的向量表示会根据它所在的句子上下文动态变化，从而更好地捕捉其语义。一个具有代表性的模型是——ELMo。该模型全称为 Embeddings from Language Models，发表于2018年2月。其基于LSTM 语言模型，使用上下文动态生成每个词的表示，每个词的向量由其前文和后文共同决定，是第一个被广泛应用于下游任务的上下文词向量模型。

## 传统序列模型
### RNN（Recurrent Neural Network，循环神经网络）
在自然语言中，词语的顺序对于理解句子的含义至关重要。虽然词向量能够表示词语的语义，但它本身并不包含词语之间的顺序信息。

为了解决这一问题，研究者提出RNN（Recurrent Neural Network，循环神经网络）。

RNN 会逐个读取句子中的词语，并在每一步结合当前词和前面的上下文信息，不断更新对句子的理解。通过这种机制，RNN 能够持续建模上下文，从而更准确地把握句子的整体语义。因此RNN曾是序列建模领域的主流模型，被广泛应用于各类NLP任务。

&lt;font color=&#39;red&#39;&gt;
说明：
随着技术的发展，RNN已经逐渐被结构更灵活、计算效率更高的Transformer 模型所取代，后者已经成为当前自然语言处理的主流方法。

尽管如此，RNN 仍然具有重要的学习价值。它所体现的“循环建模上下文”的思想，不仅为 LSTM 和 GRU 等改进模型奠定了基础，也有助于我们更好地理解 Transformer 等更复杂的架构。
&lt;/font&gt;

#### 网络结构
其中隐藏层的计算公式为：
$h_t = \tanh(x_t*W_x+h_{t-1}*W_h+b)$

![RNN计算图](/imgs/nlp/RNN计算图.svg)
&lt;center&gt;
RNN计算图
&lt;/center&gt;

![RNN](/imgs/nlp/RNN.png)
&lt;center&gt;
多层+双向 RNN
&lt;/center&gt;

Pytorch API： [torch.nn.RNN 模块](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)

``` python
torch.nn.RNN(
    input_size, # 每个时间步输入特征的维度（词向量维度）
    hidden_size, # 隐藏状态的维度
    num_layers=1, # RNN 层数，默认为 1
    nonlinearity=&#34;tanh&#34;, # 激活函数，&#39;tanh&#39;（默认）或 &#39;relu&#39;
    bias=True, # 是否使用偏置项，默认 True
    batch_first=False, # 输入张量是否是 (batch, seq, feature)，默认 False 表示 (seq, batch, feature)
    dropout=0.0, # 除最后一层外，其余层之间的 dropout 概率
    bidirectional=False, # 是否为双向 RNN，默认 False
    device=None,
    dtype=None,
)

rnn = torch.nn.RNN(...)
output, h_n = rnn(input, h_0)

# 注意
序列padding时，输入RNN前需进行pack处理，详见LSTM部分
```

**输入输出形状：**
- 输入
  - input：输入序列，形状为(seq_len, batch_size, input_size)，如果 batch_first=True，则为 (batch_size, seq_len, input_size)
  - h_0：可选，初始隐藏状态，形状为 (num_layers × num_directions, batch_size, hidden_size)
- 输出：
  - output：RNN层的输出，包含最后一层每个时间步的隐藏状态，形状为 (seq_len, batch_size, num_directions × hidden_size )，如果如果 batch_first=True，则为(batch_size, seq_len, num_directions × hidden_size )
  - h_n：最后一个时间步的隐藏状态，包含每一层的每个方向，形状为 (num_layers × num_directions, batch_size, hidden_size)

![RNN](/imgs/nlp/RNN_shape.png)
&lt;center&gt;
多层+双向 RNN 输入输出形状
&lt;/center&gt;

#### 存在问题
尽管循环神经网络（RNN）在处理序列数据方面具有天然优势，但它在实际应用中面临一个非常严重的问题：长期依赖建模困难。这指的是：在训练过程中，当输入序列很长时，模型难以有效学习早期输入对最终输出的影响。

上述问题的根本原因在于训练过程中存在的梯度消失或梯度爆炸问题。

在训练RNN时，采用的是时间反向传播（Backpropagation Through Time, BPTT）方法，在反向传播过程中，梯度需要在每个时间步上不断链式传递。

### 长短期记忆网络（Long Short-Term Memory, LSTM）
为了缓解RNN梯度消失或者梯度爆炸的问题，Hochreiter 和 Schmidhuber 于 1997 年提出了长短期记忆网络（Long Short-Term Memory, LSTM）。

#### 网络结构
LSTM 通过引入特殊的记忆单元（Memory Cell，图中的），有效提升了模型对长序列依赖关系的建模能力。

其中隐藏层的计算公式为：
$$
i_t​=\sigma(W_{ii​}x_{t}​+b_{ii}​+W_{hi}​h_{t−1}​+b_{hi}​)\\\\
f_t​=\sigma(W_{if}​x_t​+b_{if}​+W_{hf}​h_{t−1}​+b_{hf}​)\\\\
g_t​=\tanh(W_{ig}​x_t​+b_{ig}​+W_{hg}​h_{t−1}​+b_{hg}​)\\\\
o_t​=\sigma(W_{io}​x_t​+b_{io}​+W_{ho}​h_{t−1}​+b_{ho}​)\\\\
c_t​=f_t​ \odot c_{t−1}​+i_t​ \odot g_t\\\\
​h_t​=o_t \odot \tanh(c_t​)​
$$
注：
- σ：sigmoid function，(0,1)
- ⊙：Hadamard product，各元素乘积

其沿时间步展开后的内部结构如下图所示，核心结构是三个“门”，分别是：
- 遗忘门（f）：决定当前时间步要忘记多少过去的记忆。
- 输入门（i）：控制要从当前时间步的输入向记忆单元存入多少新的信息。
- 输出门（o）：控制从记忆单元中读取多少信息作为当前时间步的隐藏状态进行输出。

![LSTM 结构](/imgs/nlp/lstm_structure1.png)
&lt;center&gt;LSTM 结构&lt;/center&gt;

![LSTM 展开结构](/imgs/nlp/lstm_structure.png)
&lt;center&gt;LSTM 展开结构&lt;/center&gt;

![LSTM](/imgs/nlp/lstm.png)
&lt;center&gt;多层+双向 LSTM&lt;/center&gt;

Pytorch API： [torch.nn.LSTM 模块](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)

``` python
torch.nn.LSTM(
    input_size, # 每个时间步输入特征的维度（词向量维度）
    hidden_size, # 隐藏状态的维度
    num_layers=1, # LSTM 层数，默认为 1
    nonlinearity=&#34;tanh&#34;, # 激活函数，&#39;tanh&#39;（默认）或 &#39;relu&#39;
    bias=True, # 是否使用偏置项，默认 True
    batch_first=False, # 输入张量是否是 (batch, seq, feature)，默认 False 表示 (seq, batch, feature)
    dropout=0.0, # 除最后一层外，其余层之间的 dropout 概率
    bidirectional=False, # 是否为双向 LSTM，默认 False
    proj_size=0, # 隐藏状态的投影输出维度；若为 0，则不使用 projection。（详见官方文档，用于调整）
    device=None,
    dtype=None,
)

lstm = torch.nn.LSTM()
output, (h_n, c_n) = lstm(input, (h_0, c_0))
```

**注意：**
``` python
# 对于序列模型中有padding操作时，注意把padding去掉再丢入LSTM计算
# 双向多层LSTM模型示例
# 可利用以下接口：
# from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence
class ReviewAnalyzeModel(nn.Module):
    def __init__(self, vocab_size, padding_idx=0):
        super(ReviewAnalyzeModel, self).__init__()
        self.padding_idx = padding_idx

        self.embedding = nn.Embedding(
            num_embeddings=vocab_size,
            embedding_dim=config.EMBEDDING_DIM,
            padding_idx=padding_idx
        )

        self.lstm = nn.LSTM(
            input_size=config.EMBEDDING_DIM,
            hidden_size=config.HIDDEN_SIZE,
            num_layers=config.NUM_LAYERS,
            batch_first=True,
            bidirectional=True
        )

        self.linear = nn.Linear(2 * config.HIDDEN_SIZE, 1)

    def forward(self, x):
        # x.shape [batch_size, seq_len]，尾部padding

        # ① 自动计算每条样本实际长度（忽略 padding）
        lengths = (x != self.padding_idx).sum(dim=1)

        # ② embedding
        embedding = self.embedding(x)

        # ③ pack（让 LSTM 忽略 padding）
        packed = nn.utils.rnn.pack_padded_sequence(
            embedding,
            lengths.cpu(),
            batch_first=True,
            enforce_sorted=False
        )

        # ④ LSTM
        _, (h_n, _) = self.lstm(packed)

        # h_n: [num_layers * num_directions, batch, hidden_dim]
        # 由于是双向，最后一层包括 forward 与 backward
        # forward:  index = (num_layers - 1) * 2
        # backward: index = forward + 1
        forward_idx = (config.NUM_LAYERS - 1) * 2
        backward_idx = forward_idx + 1

        last_hidden_forward = h_n[forward_idx]  # [batch, hidden]
        last_hidden_backward = h_n[backward_idx]  # [batch, hidden]

        # ⑤ 拼接双向 hidden
        last_hidden = torch.cat((last_hidden_forward, last_hidden_backward), dim=-1)

        # ⑥ 线性分类
        out = self.linear(last_hidden)

        return out.squeeze(1)
```

**输入输出形状：**

&lt;font color=&#39;red&#39;&gt;
说明：
如果proj_size设置为0，那么输入、输出除了`c_0`与`c_n`，其他与RNN完全一致。
&lt;/font&gt;

- 输入（假定proj_size设置为0）
  - input：输入序列，形状为(seq_len, batch_size, input_size)，如果 batch_first=True，则为 (batch_size, seq_len, input_size)
  - h_0：可选，初始隐藏状态，形状为 (num_layers × num_directions, batch_size, hidden_size)
  - c_0：可选，初始细胞状态，形状为 (num_layers × num_directions, batch_size, hidden_size)
- 输出：
  - output：LSTM层的输出，包含最后一层每个时间步的隐藏状态，形状为 (seq_len, batch_size, num_directions × hidden_size )，如果如果 batch_first=True，则为(batch_size, seq_len, num_directions × hidden_size )
  - h_n：最后一个时间步的隐藏状态，包含每一层的每个方向，形状为 (num_layers × num_directions, batch_size, hidden_size)
  - c_n：最后一个时间步的细胞状态，包含每一层的每个方向，形状为 (num_layers × num_directions, batch_size, hidden_size)

![LSTM](/imgs/nlp/LSTM_shape.png)
&lt;center&gt;多层+双向 LSTM 输入输出形状&lt;/center&gt;

#### LSTM为何能缓解梯度消失和梯度爆炸？
LSTM通过引入记忆单元（Memory Cell），在时间步之间提供了一条稳定的梯度传播路径。

记忆单元的更新公式为
$$c_t​=f_t​⊙c_{t−1}​+i_t​⊙g_t$$

所以$\frac{\partial C_t}{\partial C_{t-1}}$（简单起见，按照标量推导）

在反向传播时，沿记忆单元路径，梯度传播实际上是多个$f_t$连乘的结果。虽然每个$f_t$的取值小于1，但通常较接近于1。这是因为$f_t$由遗忘门生成，在一般任务中，遗忘门倾向于“记得多、忘得少”，因此$f_t$的值通常较大。

由于乘积中的每一项$f_t$较接近1，整体衰减速度远小于传统RNN中隐藏状态链式传播时的指数衰减。这使得早期时间步的输入，能够通过记忆单元路径稳定地影响到最终的总梯度，从而有效参与参数的更新，保证了模型对长序列依赖的学习能力。

#### 存在问题
尽管 LSTM 相较传统 RNN 解决了长期依赖问题，性能大幅提升，但在实际应用中，仍存在一些明显的局限性和问题，主要包括：
- 难以并行计算
  - LSTM 的时间步之间具有强依赖性（后一个时间步的输入依赖前一个时间步的输出），导致无法进行大规模并行加速，训练和推理速度受限。
- 参数量大，计算开销高：
  - 每个 LSTM 单元内部包含多个门控机制（输入门、遗忘门、输出门），每个门都需要独立计算，导致参数数量和计算量远大于普通 RNN。
  - 在资源受限的场景下（如移动端、嵌入式设备），部署 LSTM 会面临挑战。
- 长期依赖建模仍然有限
  - 虽然 LSTM 延缓了梯度消失问题，但并不能完全消除。当序列极长时，模型依然难以有效捕捉非常远距离的依赖关系。

### 门控循环单元（Gated Recurrent Unit，GRU）
Gated Recurrent Unit（GRU）是为了进一步简化 LSTM 结构、降低计算成本而提出的一种变体。GRU 保留了门控机制的核心思想，但相比 LSTM，结构更为简洁，参数更少，训练效率更高。

在许多实际任务中，GRU 能在保持类似性能的同时，显著减少训练时间。

#### 网络结构
与LSTM相比，GRU做出了以下改进：
- 取消了LSTM中独立的记忆单元，只保留隐藏状态。
- 通过两个门控结构控制信息流动：
  - 重置门（Reset Gate）：用于控制遗忘多少旧信息
  - 更新门（Update Gate）：用于控制保留多少旧信息，以及引入多少新信息

其中计算公式为：
$$
r_t = \sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr})\\\\
z_t = \sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\\\
n_t = \tanh(W_{in} x_t + b_{in} + r_t \odot (W_{hn} h_{(t-1)}+ b_{hn}))\\\\
h_t = (1 - z_t) \odot n_t + z_t \odot h_{(t-1)}
$$

![GRU结构图](/imgs/nlp/gru.png)
&lt;center&gt;
GRU结构图
&lt;/center&gt;

![GRU展开结构图](/imgs/nlp/gru_structure.png)
&lt;center&gt;
GRU展开结构图
&lt;/center&gt;

Pytorch API： [torch.nn.GRU 模块](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html)

``` python
torch.nn.GRU(
    input_size, # 每个时间步输入特征的维度（词向量维度）
    hidden_size, # 隐藏状态的维度
    num_layers=1, # GRU 层数，默认为 1
    nonlinearity=&#34;tanh&#34;, # 激活函数，&#39;tanh&#39;（默认）或 &#39;relu&#39;
    bias=True, # 是否使用偏置项，默认 True
    batch_first=False, # 输入张量是否是 (batch, seq, feature)，默认 False 表示 (seq, batch, feature)
    dropout=0.0, # 除最后一层外，其余层之间的 dropout 概率
    bidirectional=False, # 是否为双向 GRU，默认 False
    device=None,
    dtype=None,
)

gru= torch.nn.GRU()
output, h_n = gru(input, h_0)
```

**输入输出形状：**
- 同RNN

## Seq2Seq模型
传统的自然语言处理任务（如文本分类、序列标注）以​**​静态输出**​​为主，其目标是预测固定类别或标签。然而，现实中的许多应用需要模型**​​动态生成新的序列**​​，例如：
- ​​机器翻译​​：输入中文句子，输出对应的英文翻译。
- ​​文本摘要​​：输入长篇文章，生成简短的摘要。
- ​问答系统​​：输入用户问题，生成自然语言回答。
- ​对话系统​​：输入对话历史，生成连贯的下一条回复。

这些任务具有两个关键共同点：
- ​​输入和输出均为序列​​（如词、字符或子词序列）。
- ​输入与输出序列长度动态可变​​（例如翻译任务中，中英文句子长度可能不同）。

为了解决这类问题，研究者提出了**Seq2Seq（Sequence to Sequence，序列到序列）**模型。

### 模型结构
Seq2Seq 模型由一个编码器（Encoder）和一个解码器（Decoder）构成。
- 编码器（Encoder）：负责提取输入序列的语义信息，并将其压缩为一个固定长度的上下文向量（Context Vector）
- 解码器（Decoder）：基于该上下文向量，逐步生成目标序列。

![Seq2Seq结构图](/imgs/nlp/seq2seq.png)
&lt;center&gt;Seq2Seq结构图&lt;/center&gt;

#### 编码器（Encoder）
编码器主要由一个循环神经网络（RNN/LSTM/GRU）构成，其任务是将输入序列的语义信息提取并压缩为一个上下文向量。

在模型处理输入序列时，循环神经网络会依次接收每个token的输入，并在每个时间步步更新隐藏状态。每个隐藏状态都携带了截止到当前位置为止的信息。随着序列推进，信息不断累积，最终会在最后一个时间步形成一个包含整句信息的隐藏状态。

这个最后的隐藏状态就会作为上下文向量（context vector），传递给解码器，用于指导后续的序列生成。

![Seq2Seq encoder](/imgs/nlp/seq2seq_encoder.png)
&lt;center&gt;Seq2Seq RNN encoder&lt;/center&gt;

为增强编码器的理解能力，循环网络也可以采用双向结构（结合前文与后文信息）或多层结构（提取更深的语义特征）。

#### 解码器（Decoder）
解码器主要也由一个循环神经网络（RNN / LSTM / GRU）构成，其任务是基于编码器传递的上下文向量，逐步生成目标序列。

![Seq2Seq decoder](/imgs/nlp/seq2seq_decoder.png)
&lt;center&gt;Seq2Seq RNN decoder&lt;/center&gt;

在生成开始时，循环神经网络以上下文向量作为初始隐藏状态，并接收一个特殊的起始标记 `&lt;sos&gt;（start of sentence）`作为第一个时间步的输入，用于预测第一个 token。

随后，在每一个时间步，模型都会根据前一时刻的隐藏状态和上一步生成的 token，预测当前的输出。这种“将前一步的输出作为下一步输入”的方式被称为自回归生成（Autoregressive Generation），它确保了生成结果的连贯性。

生成过程会持续进行，直到模型生成了一个特殊的结束标记` &lt;eos&gt;（end of sentence）`，表示句子生成完成。

&gt; 说明：起始标记和结束标记会在训练数据中显式添加，模型会在训练中学会何时开始、如何续写，以及何时结束，从而掌握完整的生成流程。

### 模型训练和推理机制
#### 模型训练
- 编码器：通过嵌入层和循环神经网络（RNN / LSTM / GRU）的逐步处理，将整句编码为上下文向量。
- 解码器：解码器使用该上下文向量初始化其隐藏状态，然后逐步生成目标序列。
  - 注意：训练阶段与推理阶段的解码策略是不同的
  - 1、在推理阶段，解码器采用自回归生成方式：每一步的输入是模型自己上一步的预测结果。
  - 2、在训练阶段，通常使用一种称为 Teacher Forcing 的策略，即使用目标序列中真实的前一个token。

#### 模型推理
- 编码器：推理阶段的编码器处理流程与训练时完全一致。
- 解码器：生成方式采用自回归生成（Autoregressive Generation）：每一步的输出会作为下一步的输入，逐步构造完整句子。
  - 1、自回归生成流程：起始标记 `&lt;sos&gt;`，结束标记`&lt;eos&gt;`，上一步生成的词作为当前输入
  - 2、**词选择策略**：
    - 贪心解码（Greedy Decoding）：每一步都选择概率最高的词，局部最优。
    - 束搜索（Beam Search）：每一步保留多个候选词序列（如 beam size = 3），并在扩展后选择得分最高的完整句子。

### 存在问题
在上述 Seq2Seq 架构中，编码器会将整个源句压缩为一个固定长度的上下文向量，并将其作为解码器生成目标序列的唯一参考。这种“压缩再解压”的方式虽然结构简洁，但在实际任务中暴露出两个核心问题：
1. 信息压缩困难，语义表达受限

对于编码器而言，用一个定长向量去表达任意复杂的句子，是一项非常困难的任务。尤其在面对长句时，信息很容易在压缩过程中丢失，导致语义表达不完整。

这种“信息瓶颈”限制了模型在处理长文本或复杂语义结构时的表现。

2. 缺乏动态感知，解码难以精准生成

解码器始终只能基于同一个上下文向量进行生成。
但在实际生成过程中，不同位置的目标词，往往依赖源句中不同的关键信息：
- 生成主语时，可能更依赖源句的开头；
- 生成谓语或宾语时，可能需要参考句中或句末内容。

然而在固定表示下，解码器无法“有选择地关注”输入序列的不同部分，只能一视同仁地处理所有信息，从而降低了生成的准确性与灵活性。

## Attention机制
传统的 Seq2Seq 模型中，编码器在处理源句时，无论其长度如何，最终都只能将整句信息压缩为一个固定长度的上下文向量，用作解码器的唯一参考。这种设计存在两个显著问题：
- 信息压缩困难：固定向量难以完整表达长句或复杂语义，容易丢失关键信息；
- 缺乏动态感知：解码器在每一步生成中都只能依赖同一个上下文向量，难以根据不同位置的生成需要灵活提取信息。

为了解决上述问题，研究者引入了** Attention 机制**。其**核心思想**是：

解码器在生成目标序列的每一步时，不再依赖于一个静态的上下文向量，而是根据当前的解码状态，动态地从编码器各时间步的隐藏状态中选取最相关的信息，以辅助当前步的生成。

这种机制赋予模型“对齐”能力，使其能够自动判断源句中哪些位置对当前的目标词更为重要，从而有效缓解信息瓶颈问题，提升生成质量与表达能力。

### 工作原理
注意力机制的核心思想，是解码器在生成目标序列的每一步时，动态地从编码器的各个时间步的隐藏状态中提取当前所需的信息，而不再只依赖一个固定的上下文向量。

![attention](/imgs/nlp/attention.png)
&lt;center&gt;attention&lt;/center&gt;

这一机制通常通过以下 4 个关键步骤实现：

#### 相关性计算
在目标序列生成的每一步，解码器都会计算当前时间步的隐藏状态与编码器各个时间步输出之间的相关性。这些相关性衡量了源句中每个位置对当前生成内容的重要程度，从而决定模型应将多少注意力分配给不同的源位置。

相关性的计算依赖于特定的函数，通常被称为注意力评分函数（attention scoring function）。常见的评分函数实现方式将在下一节中详细介绍。

![attention](/imgs/nlp/attention_step1.png)
&lt;center&gt;attention 相关性计算&lt;/center&gt;

#### 注意力权重计算
得到所有源位置的注意力评分后，使用 Softmax 函数将其归一化为概率分布，作为注意力权重。得分越高的位置，其对应的权重越大，代表模型在当前生成中更关注该位置的信息。

![attention](/imgs/nlp/attention_step2.png)
&lt;center&gt;attention 注意力权重计算&lt;/center&gt;

#### 上下文向量计算
将所有编码器输出按照注意力权重进行加权求和，得到一个上下文向量。这个向量就表示当前时间步，模型从源句中提取出的关键信息。

![attention](/imgs/nlp/attention_step3.png)
&lt;center&gt;attention 上下文向量计算&lt;/center&gt;

#### 解码信息融合
在得到上下文向量后，解码器将其与当前时间步的隐藏状态进行拼接，以融合两者信息，最终通过线性变换和 Softmax，生成当前时间步目标词的概率分布。

![attention](/imgs/nlp/attention_step4.png)
&lt;center&gt;attention 解码信息融合&lt;/center&gt;

### 注意力评分函数
注意力评分函数有多种实现方式。本节将介绍三种常见的计算方法：点积评分（Dot）、通用点积评分（General）和拼接评分（Concat）。它们虽然在结构上各有差异，但本质上都是用于衡量解码器当前隐藏状态与编码器各时间步隐藏状态之间的相关性，并据此分配注意力权重。

#### 点积评分（Dot）
点积评分是注意力机制中最简单、最直接的一种相关性评分方法。它通过计算解码器当前时间步的隐藏状态与编码器每个时间步的隐藏状态的点积，来衡量二者之间的相关性：

![attention_score_function_dot](/imgs/nlp/attention_score_function_dot.png)

其含义可以理解为：如果两个向量方向越一致（即越接近），它们的点积就越大，表示相关性越强，模型应当给予更多注意力。

#### 通用点积评分（General）
通用点积评分在点积的基础上引入了一个可学习的权重矩阵W,用于先对编码器隐藏状态进行线性变换，再与解码器隐藏状态进行点积：

![attention_score_function_general](/imgs/nlp/attention_score_function_general.png)

该方法的设计动机主要是为了解决编码器和解码器隐藏状态维度不一致的问题。通过引入权重矩阵W，不仅实现了维度对齐，也增强了模型对编码器输出的适应能力，从而提升了注意力机制的表达能力。

#### 拼接评分（Concat）
拼接评分是一种表达能力更强的相关性评分方法。它的核心思想是：将解码器当前隐藏状态与编码器每个时间步的隐藏状态拼接为一个长向量，经过线性变换和非线性激活，最后用一个向量进行投影，得到最终打分值：

![attention_score_function_concat](/imgs/nlp/attention_score_function_concat.png)

相比前两种方法，Concat 评分方式在建模能力上更强。它不仅考虑了两个状态的数值关系，还引入非线性变换，能够捕捉更复杂的交互模式，更适合处理对齐关系复杂的任务场景。

### 存在问题
尽管注意力机制极大地增强了 Seq2Seq 模型的建模能力，但由于其核心依然依赖于 RNN 结构，仍面临两个根本性问题：
- 计算过程无法并行
  - RNN 的时间步之间存在强依赖，必须顺序执行，限制了训练效率和硬件资源的利用率。
- 长期依赖问题仍未根除
  - 模型需要跨多个时间步传递信息，对于超长序列，训练过程中容易出现梯度消失，难以有效建模长距离依赖关系。

## Transformer模型
此前的Seq2Seq模型通过注意力机制取得了一定提升，但由于整体结构仍依赖 RNN，依然存在计算效率低、难以建模长距离依赖等结构性限制。

为了解决这些问题，Google在2017 年发表一篇论文《[Attention Is All You Need](https://arxiv.org/pdf/1706.03762)》，提出了一种全新的模型架构——Transformer。该模型完全摒弃了 RNN 结构，转而使用注意力机制直接建模序列中各位置之间的关系。通过这种方式，Transformer不仅显著提升了训练效率，也增强了模型对长距离依赖的建模能力。

Transformer 的提出对自然语言处理产生了深远影响。在机器翻译任务中，它首次超越了 RNN 模型的表现，并成为后续各类预训练语言模型的基础框架，如 BERT、GPT 等。这些模型推动 NLP 进入了“预训练 + 微调”的新时代，极大地提升了模型在多种任务上的通用性与性能。如今，Transformer 架构不仅广泛应用于 NLP，还扩展至语音识别、图像处理、代码生成等多个领域，成为现代深度学习中最具代表性的通用模型之一。

### 模型结构
#### 核心思想
在seq2seq基础上，通过Attention等替代RNN进行位置关系建模，便于并行计算，更适合捕捉长距离依赖。

![transformer_demo](/imgs/nlp/transformer_demo.png)

#### 整体结构
标准的 Transformer 模型通常包含 6个编码器层和 6 个解码器层。

![transformer_demo2](/imgs/nlp/transformer.png)

### 编码器（Encoder）
![transformer_encoder](/imgs/nlp/transformer_encoder.png)

- Transformer 的编码器用于理解输入序列的语义信息，并生成每个token的上下文表示，为解码器生成目标序列提供基础。
- 编码器由多个结构相同的编码器层（Encoder Layer）堆叠而成。
- 每个 Encoder Layer的主要任务都是对其输入序列进行上下文建模，使每个位置的表示都能融合来自整个序列的全局信息。
- 每个 Encoder Layer都包含两个子层（sublayer），分别是自注意力子层（Self-Attention Sublayer）和前馈神经网络子层（Feed-Forward Sublayer）。
  - Self-Attention：用于捕捉序列中各位置之间的依赖关系。
  - Feed-Forward：用于对每个位置的表示进行非线性变换，从而提升模型的表达能力。

#### 自注意力层（Self-Attention）/ 多头自注意力（multi-Head Self-Attention，MHA）
之所以被称为“自”注意力，是因为模型在计算每个位置的表示时，所参考的信息全部来自同一个输入序列本身，而不是来自另一个序列。

##### 自注意力计算过程
自注意力的完整计算过程如下：

$$
Attention(Q,K,V)=Softmax(\frac{Q \cdot K^{T}}{\sqrt{d_k}})V
$$

![transformer_attention_1](/imgs/nlp/transformer_attention_1.png)

![transformer_attention_2](/imgs/nlp/transformer_attention_2.png)

**（1）生成Query、Key、Value向量**

自注意力机制的第一步，是将输入序列中的每个位置表示映射为三个不同的向量，分别是 查询（Query）、键（Key） 和 值（Value）。其中$W_q、W_k、W_v$均为可学习的参数矩阵。
- Query：表示当前词的用于发起注意力匹配的向量； $Q=X W_x$
- Key：表示序列中每个位置的内容标识，用于与 Query 进行匹配； $Q=X W_k$
- Value：表示该位置携带的信息，用于加权汇总得到新的表示。 $Q=X W_v$

**（2）计算位置间相关性**

完成 Query、Key、Value 向量的生成后，模型会使用每个位置的 Query 向量与所有位置的 Key 向量进行相关性评分。

评分函数采用向量点积形式。由于在高维空间中，点积的数值可能过大，会影响 softmax 的稳定性，因此在实际计算中对结果进行了缩放。最终的评分函数为：
$$
Score(i,j) = \frac{q_i \cdot k_j}{\sqrt{d_k}} 
$$
其中$d_k$是key向量的维度，用于缩放点积的幅度（假设q、k方差为1，则$q_i \cdot k_j$的方差为$d_k$）。

**（3）计算注意力权重**

在得到每个位置与所有位置之间的相关性评分后，模型会使用 softmax 函数进行归一化，确保每个位置对所有位置的关注程度之和为 1，从而形成一个有效的加权分布。

**（4）加权汇总生成输出**

最后，模型会根据注意力权重对所有位置的 Value 向量进行加权求和，得到每个位置融合全局信息后的新表示。

##### 多头自注意力计算过程
自注意力机制通过 Query、Key 和 Value 向量计算每个位置与其他位置之间的依赖关系，使模型能够有效捕捉序列中的全局信息。

然而，自然语言本身具有高度的语义复杂性，一个句子往往同时包含多种类型的语义关系。要准确理解这类句子，模型需要同时识别并建模多种层次和类型的依赖关系。但这些信息很难通过单一视角或一套注意力机制完整捕捉。

为此，Transformer 引入了多头注意力机制（Multi-Head Attention）。其核心思想是通过多组独立的 Query、Key、Value 投影，让不同注意力头分别专注于不同的语义关系，最后将各头的输出拼接融合。

多头注意力的计算过程如下：

$$
MHA(Q,K,V)=Concat(head_1,head_2,..,head_n)W^o
$$

**（1）分别计算各头注意力**

每个 Self-Attention Head 独立计算一套注意力输出。

![Multi_Head_Attention_1](/imgs/nlp/Multi_Head_Attention_1.png)

**（2）合并多头注意力**

多个输出矩阵按维度拼接，再乘以得到最终多头注意力的输出。

![Multi_Head_Attention_2](/imgs/nlp/Multi_Head_Attention_2.png)

#### 前馈神经网络层（Feed-Forward Network，FFN）
前馈神经网络（Feed-Forward Network，简称 FFN）是 Transformer 编码器中每个子层的重要组成部分，紧接在多头注意力子层之后。它通过对每个位置的表示进行**逐位置、非线性**的特征变换，进一步提升模型对复杂语义的建模能力。

一个标准的 FFN 子层包含两个线性变换和一个非线性激活函数，中间通常使用 ReLU激活。其计算公式如下：

$$
FFN(x)=Linear_2(ReLU(Linear_1(x)))=W_2\cdot ReLU(W_1\cdot x+b_1)+b_2
$$

![transformer_ffn](/imgs/nlp/transformer_ffn.png)

#### 残差连接与层归一化（Residual Connection &amp; LayerNorm）
在 Transformer 的每个编码器层中，每个子层，包括自注意力子层和前馈神经网络子层，其输出都要经过残差连接（Residual Connection）和层归一化（Layer Normalization）处理。这两者是深层神经网络中常用的结构，用于缓解模型训练中的梯度消失、收敛困难等问题，对于Transformer能够堆叠多层至关重要。

![transformer_res_ln](/imgs/nlp/transformer_res_ln.png)

##### 残差连接

残差连接（Residual Connection，也称“跳跃连接”或“捷径连接”）最初在计算机视觉领域被提出，用于缓解深层神经网络中的梯度消失问题。其核心思想是：

将子层的输入直接与其输出相加，形成一条跨越子层的“捷径”，其数学形式为：
$$
y=x+SubLayer(x)
$$
残差连接确保反向传播时，梯度至少有一条稳定通路可回传，是深层网络可稳定训练的关键结构。

##### 层归一化
每个子层在残差连接之后都会进行层归一化（Layer Normalization，简称 LayerNorm）。它的主要作用是规范输入序列中每个token的特征分布（某个token的表示可能在不同维度上有较大数值差异），提升模型训练的稳定性。

该操作会将每个token的向量调整为均值为 0、方差为 1 的规范分布（LayerNorm 并不会把任意分布变成真正的标准正态分布，它做的是：仿射标准化（affine normalization），即均值 → 0、方差 → 1，而不保证高阶矩（偏度、峰度）符合正态分布），具体的计算公式如下：
$$
总体公式：LayerNorm(x)=\gamma \odot \frac{x-\mu}{\sigma+\epsilon}+\beta
$$
假如某个token的特征向量为$x=[x^1, x^2,...,x^d]$，

1. 均值计算：

计算该向量在所有特征维度上的平均值
$$
\mu = \frac{1}{d} \sum_{i=1}^{d} x^i
$$
其中$d$为特征维度（向量长度）。

2. 标准差计算

计算向量各维度的标准差
$$
\sigma = \sqrt{\frac{1}{d} \sum_{i=1}^{d} (x^i-\mu)^2}
$$

3. 标准化变换

将每个特征值转换为均值为 0、方差为 1 的标准正态分布；
$$
\hat{x^i}=\frac{x^i-\mu}{\sigma+\epsilon}
$$
$\epsilon$为一个小的常数，防止出现除以0的情况。

4. 缩放和平移（可学习的仿射变换）

让模型可以学习在归一化后的基础上进行适当的调整，保证归一化不会限制模型的表示能力。
$$
LayerNorm(x^i)=\gamma^i\cdot\hat{x^i}+\beta^{i}
$$
$\gamma^i$和$\beta^i$为可学习参数。

思考：
1. **为什么 Transformer 用 LayerNorm 而不是 BatchNorm？**

| 对比项                   | BatchNorm | LayerNorm |
| --------------------- | --------- | --------- |
| 归一化维度                 | batch 维   | feature 维 |
| 是否依赖 batch size       | 是         | 否         |
| NLP / Transformer 适配性 | 差         | 极好        |
| 推理阶段一致性               | 有偏差       | 完全一致      |
| 变长序列                  | 不适合       | 天然支持      |


##### Pre_Norm
在更深层次网络使用效果可能更好，在pytorch的nn.Transformer中通过norm_first参数设置。

Transformer的被称之为`Post_Norm`：
$$
LayerNorm(x+SubLayer(x))
$$

Pre_Norm方法：
$$
x+SubLayer(LayerNorm(x))
$$

#### 位置编码（Positional Encoding）
Transformer 模型完全摒弃了 RNN 结构，可以并行处理所有位置的信息，但也无法天然地捕捉词语之间的顺序关系。

为了解决这一问题，Transformer 引入了一个关键机制——位置编码（Positional Encoding）。该机制为每个词引入一个表示其位置信息的向量，并将其与对应的词向量相加，作为模型输入的一部分。

**编码策略**

1. 最直接的方式：使用绝对位置编号来表示每个词的位置。

问题：越靠后的 token 位置编码就越大，若直接与词向量相加，会造成数值倾斜，让模型更关注位置，而忽视词义。

2. 位置编号归一化为[0, 1]区间，例如用$\frac{Pos}{T}$表示位置，其中 T是句子长度。

问题：使用$\frac{Pos}{T}$作为位置编码在形式上是合法的，但：
- 它将相对位置信息与序列长度强耦合，导致相同相对距离在不同长度下映射到不同编码值，使 Attention 无法学习稳定的相对位置关系；
- 同时该编码在内积空间中退化为低秩结构，表达能力不足，且在长序列外推时表现不稳定。
- 相比之下，正弦位置编码具备可线性解析的位移结构，能够在 Attention 中自然建模相对位置并支持长度外推。

3. Transformer：使用了一种基于正弦（sin）和余弦（cos）函数的位置编码方式，具体定义如下：
$$
PE_{（pos,2i）}=\sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})\\\\
PE_{（pos,2i+1）}=\cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})
$$
其中：
- pos是当前词在序列中的位置；
- i用于表示位置编码向量的维度索引，2i表示偶数维，2i+1表示奇数维；
- $d_{model}$是词向量的维度大小。

Transformer提出的这种编码方式不依赖任何可学习参数，数值稳定，并具备以下优势：
- 所有值都在[−1,1]范围内，数值稳定
- 编码方式固定、可预计算，无需训练；
- 相同位置的编码在不同句子中保持一致；
- 编码之间具有数学规律，便于模型在注意力机制中感知词语之间的相对位置关系。

##### 问题本质：位置编码要解决什么？
Transformer 的自注意力机制：
$$
\mathrm{Attn}(Q,K,V)=\mathrm{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
$$
本身 **对序列是置换不变的（permutation invariant）**。

位置编码的目标是：
&gt; 向 token 表示中注入一个 **与绝对位置和相对位置有关、可泛化、可比较的信号**

因此一个“合格”的位置编码至少需要满足：

1. **同一位置的编码在不同样本中一致**
2. **不同位置可区分**
3. **相对位置信息可被线性或低阶运算提取**
4. **对未见过的序列长度具备外推能力**

&gt; 如果方案二的$\frac{pos}{T}$改为$\frac{pos}{C}, c=10000$会有问题吗？

1. 定义
$$
\mathrm{PE}(pos)=\frac{pos}{C},\quad C=10000
$$
2. 绝对位置一致性：无论句长多少，数值一致。
3. 但问题依然存在（关键）：
   1. 位置编码是一维标量：所有位置落在一条直线上，表达能力极弱：Attention 只能感知：$Q(K+PE)^\top$，线性偏移，难以刻画复杂相对关系
   2. 尺度问题（推导）：最大位置$pos_{max}$：$\frac{pos_{max}}{10000} \ll 1$，与 embedding 数值（通常 $\sim \mathcal{N}(0,1)$）相比：$\mathrm{Var}(\mathrm{PE}) \ll \mathrm{Var}(\mathrm{Token})$，位置几乎被“淹没”
   3. 无法表达相对位移（核心）：设两个位置：$\Delta = \frac{pos_2 - pos_1}{10000}$，Attention 中的内积项：$(x + p_1)^\top(x + p_2)$，只能学到**线性距离差**，无法区分：“近但在前”、“近但在后”、“周期性结构”

&gt; 为什么 Transformer 使用 sin / cos（核心推导）
1. 定义（原始公式）
$$
\begin{aligned}
\mathrm{PE}_{(pos,2i)} &amp;= \sin\left(\frac{pos}{10000^{2i/d}}\right) \\\\
\mathrm{PE}_{(pos,2i+1)} &amp;= \cos\left(\frac{pos}{10000^{2i/d}}\right)
\end{aligned}
$$

2. 核心性质一：相对位置可线性表示（关键推导）

利用三角恒等式：
$$
\sin(a+b) = \sin a \cos b + \cos a \sin b \\\\
\cos(a+b) = \cos a \cos b - \sin a \sin b
$$
设：
$$
PE(pos+k) = A(k)\cdot PE(pos)
$$
其中 (A(k)) 是仅依赖于 (k) 的线性变换。
Attention 可通过线性层感知相对位移

3. 核心性质二：多尺度位置分解
$$
\omega_i = 10000^{-2i/d}
$$

* 小 (i)：低频 → 长距离
* 大 (i)：高频 → 局部顺序

即：
$$
pos \mapsto (\sin(\omega_1 pos), \dots, \sin(\omega_d pos))
$$
等价于：
&gt; 将位置映射到一个 **多尺度 Fourier 特征空间**

4. 核心性质三：对任意长度具备外推性

sin / cos：

* 定义在整个实数域
* 不依赖最大长度
* 不需要重新训练 embedding

&gt; `pos / seq_len` 完全不具备该性质

---

### 解码器（Decoder）
编码器也由多个结构相同的解码器层堆叠组成。每个Decoder Layer都包含三个子层，分别是Masked自注意力子层、编码器-解码器注意力子层（Encoder-Decoder Attention）和前馈神经网络子层（Feed-Forward Network）。
- Masked自注意力子层（Masked Self Attention）：
  - 用于建模当前位置与前文词之间的依赖关系。
  - 为了在训练时模拟逐词生成的过程，引入遮盖机制（Mask），限制每个位置只能关注它前面的词。
- 编码器-解码器注意力子层（Encoder-Decoder Attention）：
  - 用于建模当前解码位置与源序列各位置之间的依赖关系。
  - 通过注意力机制，模型能够根据当前状态从编码器的输出中提取相关上下文信息（相当于 Seq2Seq 模型中的 Attention 机制）。
- 前馈神经网络子层（Feed-Forward Network）：与编码器中结构完全一致，对每个位置的表示进行非线性变换，增强模型的表达能力。

每个子层后也都配有残差连接与层归一化（Layer Normalization），结构设计与编码器保持一致，确保训练的稳定性和效率。

此外，解码器在输入端同样需要加入位置编码（Positional Encoding），用于提供序列中的位置信息，其计算方式与编码器中相同。
在输出端，解码器的隐藏向量会送入一个线性变换层（Linear），映射为词表大小的向量，并通过 Softmax 生成一个概率分布，用于预测当前应输出的词。

#### Masked自注意力子层（Masked Self Attention）
- 并行计算：一次性输入完整目标序列，同时预测每个位置的词。
- 遮盖机制（Mask）：限制每个位置只能关注它前面的词。

![encoder_mask](/imgs/nlp/encoder_mask.png)

&gt; 注意：在推理阶段，我们只使用解码器最后一个位置的输出作为当前步的预测结果。

Mask 机制的实现非常简单：只需将注意力得分矩阵中当前位置对其后续位置的评分设置为`−∞`，如下图所示：

![mask_1](/imgs/nlp/mask_1.png)

这样，在经过 softmax 运算后，这些位置的权重会趋近于 0。最终在加权求和时，来自未来位置的信息几乎不会参与计算，从而实现了“**当前词只能看到它前面的词**”的约束。如下图所示：

![mask_2](/imgs/nlp/mask_2.png)

#### 编码器-解码器注意力子层（Encoder-Decoder Attention）
该子层的主要作用是：建模当前解码位置与源语言序列中各位置之间的依赖关系，帮助模型在生成目标词时有效地参考输入内容，相当于Seq2Seq模型中的注意力机制。

编码器-解码器注意力的核心机制与前面讲过的自注意力机制完全一致，区别仅在于：
- Query 来自解码器当前的输入表示，即当前生成状态；
- Key和Value 来自编码器的输出表示，即整个源序列的上下文。

### 训练与推理机制
Transformer 的训练与推理都基于自回归生成机制（Autoregressive Generation）：模型逐步生成目标序列中的每一个词。然而，在实现方式上，训练与推理存在明显区别。

#### 模型训练
并行计算+mask机制

#### 模型推理
推理阶段，模型每一步都要重新输入当前已生成的全部词，通过自注意力机制建模上下文关系，预测下一个词。

模型会基于**完整前文重新计算注意力分布**，生成当前步的输出。由于每一步的输入依赖前一步结果，整个过程必须**顺序执行，无法并行**。

每步输出的是一个词的概率分布，最终生成结果也可使用不同的**解码策略**
- 贪心搜索（Greedy）
- 束搜索（Beam Search）

### Pytorch API使用
``` python
from torch import nn

# 经典 Transformer 结构
transformer = nn.Transformer(
    d_model=512, 
    nhead=8, 
    num_encoder_layers=6, 
    num_decoder_layers=6, 
    batch_first=True
)
```

PyTorch 中的 Transformer 模块由以下几个核心类构成：

#### nn.Transformer
&gt; [nn.Transformer](https://docs.pytorch.org/docs/stable/generated/torch.nn.Transformer.html)

封装了完整的 Transformer架构，由编码器和解码器组成。作为顶层接口，适用于需要同时使用编码器和解码器的任务，如机器翻译。支持用户通过参数自定义层数、注意力头数、隐藏维度等模型结构。

#### nn.TransformerEncoder
&gt; [nn.TransformerEncoder](https://docs.pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html)

实现了Transformer编码器结构，由多个编码器层的堆叠而成，用于将输入序列编码为上下文相关的表示。

#### nn.TransformerDecoder
&gt; [nn.TransformerDecoder](https://docs.pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html)

实现了Transformer解码器结构，由多个解码器层堆叠而成，用于基于编码结果逐步生成目标序列。

#### nn.TransformerEncoderLayer
&gt; [nn.TransformerEncoderLayer](https://docs.pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html)

实现了单个编码器层结构，包含一个多头自注意力子层和一个前馈神经网络子层，两者均带有残差连接和 LayerNorm。

#### nn.TransformerDecoderLayer
&gt; [nn.TransformerDecoderLayer](https://docs.pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html)
实现了单个解码器层结构，包含自注意力、编码器-解码器注意力、前馈子层，同样配有残差连接和 LayerNorm。

### The Annotated Transformer 解读
&gt; [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)
&gt; 
&gt; [Transformer从入门到精通](https://www.cnblogs.com/Icys/p/18119309/annotated-transformer-chinese)

《The Annotated Transformer》是对Transformer模型的详细注释和实现，基于论文《Attention Is All You Need》。该文档逐行解释了Transformer的架构、编码器-解码器堆栈、自注意力机制、位置编码等关键组件，并提供了PyTorch实现。

---

## 预训练模型
早期的自然语言处理方法通常针对每个具体任务单独训练模型，且严重依赖大量人工标注数据。虽然在部分场景下效果可观，但也暴露出显著局限：
- **语言知识难以复用**：每个模型都需从零开始训练，导致训练成本高、效率低；
- **强依赖高质量标注**：在医疗、法律等专业领域，标注数据获取困难且代价高昂。

为解决这些问题，研究者提出了新的建模范式——**“预训练 + 微调”**：
- **预训练阶段**：在大规模未标注语料上训练语言模型，学习词汇、句法和上下文等通用语言规律；
- **微调阶段**：将预训练模型迁移至具体任务，仅需少量标注数据即可完成任务适配。

这一方法显著提升了模型的通用性和开发效率，已成为当前 NLP 的主流技术路线，并广泛应用于文本分类、问答系统、翻译、对话等任务中。

---

### 预训练模型分类
预训练语言模型几乎都构建在 Transformer 架构之上。相较于传统的循环神经网络，Transformer具有以下优势：
- 并行计算效率高，适合大规模训练；
- 上下文建模能力强，可捕捉长距离依赖；
- 结构通用灵活，可适配多种任务类型；
- 易于扩展与迁移，支持参数堆叠与多任务学习。

因此，Transformer 成为预训练模型的主流基础架构。根据 Transformer 的使用方式不同，预训练模型大致可分为以下三类：

#### 解码器（Decoder-only）模型
仅使用Transformer解码器，代表模型为GPT（Generative Pre-trained Transformer），其由 OpenAI于2018年6月提出，论文题为[《Improving Language Understanding by Generative Pre-Training》](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)。

#### 编码器（Encoder-only）模型
仅使用Transformer 编码器，代表模型为BERT（Bidirectional Encoder Representations from Transformers），由Google于2018年10月提出，论文题为[《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》](https://arxiv.org/pdf/1810.04805)。

#### 编码器-解码器（Encoder-Decoder）模型
同时使用Transformer编码器和解码器，代表模型为T5（Text-to-Text Transfer Transformer），由Google于2019年10月提出，论文题为[《Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer》](https://arxiv.org/pdf/1910.10683)。

自 GPT、BERT 和 T5 等模型发布以来，基于 Transformer 的预训练模型不断涌现，模型架构和能力持续演进。下图总结了 2018 年至 2023 年间具有代表性的模型及其发展脉络。

![pretrained_model](/imgs/nlp/pretrained_model.png)

---

### 主流模型

#### GPT
GPT（Generative Pre-trained Transformer）是第一个系统性提出“预训练 + 微调”范式的语言模型。

其核心思想是通过大规模无监督语料进行生成式语言建模预训练，即训练模型根据左侧上下文预测下一个词，从而让模型学习自然语言的通用语法、语义和上下文依赖能力。完成预训练后，再通过微调适应具体的下游任务。

GPT首次展示了生成式语言模型在自然语言理解任务中的广泛迁移能力，为后续 GPT 系列及整个预训练语言模型的发展奠定了基础。

##### 模型结构
GPT基于Transformer的解码器结构，但与标准的Transformer解码器并不完全相同，GPT具体结构如下图所示：

![gpt](/imgs/nlp/gpt.png)

1. 输入嵌入层（Text &amp; Position Embedding）

GPT不同于原始Transformer的一点在于：位置编码采用的是可学习的位置嵌入（learnable positional embedding）。这意味着每个位置对应一个可训练的向量，模型可以在训练过程中自动优化这些向量，而非使用不可训练的三角函数编码（如正弦/余弦函数）。

2. 解码器

GPT不同于原始Transformer：掩码多头自注意力（12头）

3. 输出层

根据任务不同，GPT模型的输出可以接入不同的任务头：
- Text Prediction（文本预测）：用于下一个词的生成，输出是词表大小的概率分布，经过Softmax获得，预训练阶段使用的便是该任务头。
- Task Classifier（任务分类器）：该任务头多用于模型微调阶段，以适配具体的下游任务。通过提取特定位置的表示（如最后一个token）对整个输入文本进行分类（如情感分析、话题识别等）。

##### 预训练
GPT 的预训练阶段采用生成式语言建模（Generative Language Modeling）作为训练目标，在大规模无监督文本上进行自监督学习。

具体而言，模型的任务是基于已观察到的前文上下文，预测当前词的位置应出现的词，从而学习自然语言的统计规律与上下文依赖关系。

这种自回归语言建模方式不依赖人工标注，训练样本可以直接从原始文本中自动构建，极大地降低了构建数据的成本。

在实践中，GPT-1 使用了一个名为 BooksCorpus 的英文语料库，包含来自 7000 多本小说的完整书籍文本，总规模约 8 亿词。该语料语言自然、上下文完整，非常适合训练具备长距离依赖建模能力的语言模型。

##### 微调
GPT的微调阶段是在完成无监督语言建模预训练之后，使用有监督的任务数据对模型进行进一步训练，使其适应具体的下游任务。

**微调的核心思路**是：在保留预训练语言建模能力的基础上，利用标注数据对整个模型进行端到端优化，从而实现知识迁移。

具体实践中，GPT采用了如下两个关键措施：
- 添加任务输出层：在预训练模型顶部引入一个线性输出层（Linear Head），用于将 GPT 的隐藏状态映射为下游任务所需的标签或输出。
- 统一输入格式设计：GPT 作为自回归语言模型，其输入需为连续的文本序列。因此，在微调过程中需将各种下游任务转化为统一的文本输入格式。

下图展示了不同任务的微调逻辑：

![gpt_finetune](/imgs/nlp/gpt_finetune.png)

通过这种方式，GPT 在保留预训练模型结构和参数的基础上，仅添加极少量新参数（如线性层），便可高效完成从语言建模到多种下游任务的迁移。

此外，统一的输入格式设计进一步简化了多任务处理流程，使 GPT 能以一致的方式应对多种 NLP 任务，从而展现出强大的通用性与扩展性。

#### BERT
BERT（Bidirectional Encoder Representations from Transformers）是由 Google 于 2018 年提出的一种语言预训练模型。其核心创新在于采用 Transformer 的编码器（Encoder）结构，通过双向自注意力机制，在建模每个 token 表示时同时整合左右两个方向的上下文信息，从而获得更准确、更丰富的语义表示。

BERT 的设计更侧重于自然语言理解类任务，广泛应用于文本分类、序列标注、句子匹配等场景。模型发布后，在多个语言理解基准测试中取得了前所未有的领先成绩，推动 NLP 研究全面转向“预训练 + 微调”的通用建模范式。

##### 模型结构
BERT 基于标准的 Transformer 编码器构建，其提供了两种模型规模，分别是BERT-base和BERT-large。

具体参数规格如下：

|模型版本	|层数（Layers）	|模型维度（d_model）	|注意力头数（Heads）|参数量|
| --------------|------|------ | --------- | ---- |
|BERT-base|	12|	768|	12|	1.1 亿|
|BERT-large|	24	|1024	|16	|3.4 亿|

1. 输入表示层

BERT 的每个输入 token 表示由三部分嵌入相加组成：
- Token Embedding：词本身的语义表示；
- Position Embedding：表示 token 在序列中的位置，为可学习向量；
- Segment Embedding：用于区分句子对任务中的两个句子，分别用一个可学习的向量表示。

此外，BERT 输入中通常包含两个特殊符号：
- [CLS]：句首标志，其输出向量常用于下游的文本分类任务；
- [SEP]：句间分隔符，出现在每个句子末尾。

2. 编码器

编码器结构同原始Transformer相同

3. 输出层
根据下游任务的类型，BERT 可以接入不同的任务输出头：
Token-Level 任务（如命名实体识别）：使用每个位置的输出表示；

Sequence-Level 任务（如文本分类、句子对分类）：使用特殊 token [CLS] 的输出表示，输入时被加在序列开头，专门用于汇总整个序列的语义信息。

##### 预训练
BERT 的预训练阶段包含两个核心任务：掩码语言模型（Masked Language Modeling, MLM） 和 下一句预测（Next Sentence Prediction, NSP），分别用于学习词级语义和句间逻辑关系。

1. 掩码语言模型（MLM）

为实现双向语言建模，BERT 不采用传统的从左到右或从右到左预测方式，而是引入了掩码语言模型。在训练中，BERT 会随机遮盖输入序列中约 15% 的 token，并训练模型根据上下文预测被遮盖的词。

遮盖策略如下：
- 80% 的被遮盖 token 替换为 [MASK]；
- 10% 替换为随机词；
- 10% 保持原词不变。

这种机制让模型在预训练时既能看到左侧上下文，也能看到右侧上下文，真正实现深度双向建模。

2. 下一句预测（NSP）

为了提升模型理解句间关系的能力，BERT 引入了“下一句预测”任务。训练时模型接收两个句子，判断第二句是否是第一句的真实后续句，其中：
- 50% 的训练样本是上下文中真实相邻的句子（正例）；
- 50% 是从语料中随机采样的非相邻句子（反例）。

在预训练时，BERT 同时优化 MLM 和 NSP 两个目标，具体操作如下图所示：

![bert](/imgs/nlp/bert.png)

##### 微调
在预训练完成后，BERT 可通过少量微调适配多种下游任务，如文本分类、句子匹配、问答系统、序列标注等。微调时，模型主体结构保持不变，仅在顶部添加一个任务特定的输出层，并使用下游任务数据对整个模型进行训练。

BERT 的输入格式在微调阶段基本保持与预训练一致，仍以 token 序列为输入，使用 [CLS] 和 [SEP] 等特殊符号。不同任务的差异主要体现在输出层设计，以及从模型输出中提取哪些表示进行预测。

#### T5
T5（Text-to-Text Transfer Transformer）是 Google Research 于 2020 年提出的一种统一预训练框架，它首次在完整的 Transformer 编码器-解码器结构（Encoder-Decoder）上实现了预训练语言模型。

T5的核心思想是将所有自然语言处理任务统一表示为“文本到文本”的转换问题（Text-to-Text Framework），即无论输入是文本分类、问答还是翻译，模型的输入输出均是自然语言形式的字符串，如下图所示：

![T5](/imgs/nlp/T5.png)

这一设计使得 T5 可以通过同一个模型架构、同一套预训练机制完成多种任务，具备极强的统一性与迁移能力。

##### 模型结构
T5模型大体遵循原始的Transformer架构。

##### 预训练
T5模型的预训练目标被称为（Corrupted span prediction，CSP），具体过程如下：
1. 随机遮盖输入文本中的若干连续片段（span）；
2. 将每个被遮盖的连续片段替换为一个个特殊token；
3. 令模型学习生成这些遮盖片段的内容，作为输出序列。

![T5_example](/imgs/nlp/T5_example.png)

这种方式既保留了模型的双向建模能力，又为训练提供了明确的“生成式”学习信号，使模型可以更自然的适配下游任务。

##### 微调
T5微调阶段需要将所有任务转换为文本到文本的形式，例如：

|任务类型|	输入形式|	目标输出|
|---|---|---|
|翻译|	translate English to German: That is good.|	Das ist gut.|
|情感分类|	sentiment: This movie was great.	|positive |
|问答|	question: What is the capital of France? context: France is a country...|	Paris|

### 主流技术
#### Tokenizer
| 技术                            | 关键词     | 代表模型         | 关键机制        | 论文出处                                        |
| ----------------------------- | ------- | ------------ | ----------- | ------------------------------------------- |
| Byte Pair Encoding (BPE)      | 基于频率合并  | GPT 系列初期     | 贪心合并最频对     | *Gage 1994*；*Sennrich et al. 2016*          |
| WordPiece                     | 最大化似然   | BERT 系列      | 类似 BPE + LM | *Schuster &amp; Nakajima 2012*；*Wu et al. 2016* |
| SentencePiece (Unigram LM)    | LM 模型   | T5, ALBERT   | 子词 LM + EM  | *Kudo 2018*                                 |
| Byte-Level BPE                | 基于字节    | GPT-2/3/Neox | 无需预清洗       | *Radford et al. 2019*                       |
| Unigram LM                    | 概率模型    | XLM-R        | 子词概率 + 剪枝   | *Kudo &amp; Richardson 2018*                    |
| Morphological / Feature-aware | 形态/语言规则 | 少数语言专用       | 融语法/形态学     | 多篇任务论文                                      |

#### 位置嵌入（Positional Encoding）
当前预训练模型中的位置建模方法，已经从**“绝对位置编码”**演化为**“相对位置与几何约束”**，主流可分为五大类：

1. **绝对位置编码（Absolute Position Embedding, APE）**

   * 可学习（Learned APE）
   * 固定函数（Sinusoidal PE）
2. **相对位置编码（Relative Position Encoding, RPE）**

   * 相对位置偏置（Relative Bias）
   * 相对位置向量
3. **旋转位置编码（RoPE）**
4. **线性偏置位置编码（ALiBi）**
5. **混合 / 改进型方法（如 YaRN、xPos、NTK-aware RoPE）**

**各技术概览：**
| 方法              | 是否主流       | 代表模型         |
| --------------- | ---------- | ------------ |
| Sin/Cos APE     | 否          | 早期 Transformer，教学 / baseline，**现代大模型基本不用**         |
| Learned APE     | 否          | BERT，**在长上下文预训练模型中基本被淘汰**        |
| Relative Bias   | 是（Encoder） | T5 / PaLM，**Encoder-heavy 模型中的事实标准**   |
| RoPE            | 是（Decoder） | LLaMA / Qwen/DeepSeek，**Decoder-only LLM 的事实标准** |
| ALiBi           | 次主流        | BLOOM，部分长上下文变体模型        |
| YaRN / NTK-RoPE | 是（长上下文）    | LLaMA Long，**主流 LLM 长上下文扩展的工程标准手段**   |

---

##### 绝对位置编码（已逐渐退出主流）
1. 固定正弦位置编码（Sinusoidal PE）
$$
\begin{aligned}
PE_{(pos,2i)} &amp;= \sin\left(\frac{pos}{10000^{2i/d}}\right) \
PE_{(pos,2i+1)} &amp;= \cos\left(\frac{pos}{10000^{2i/d}}\right)
\end{aligned}
$$

**优点**

* 不依赖最大长度
* 理论上具备外推性
* 相对位移可线性表示（Fourier 特性）

**致命问题**

* **与注意力是“加性耦合”**，不是结构性约束
* 在超长上下文中性能显著下降
* 实际大模型中已很少单独使用

**论文**

* *Attention Is All You Need*, Vaswani et al., 2017

2. 可学习绝对位置嵌入（Learned APE）
$$
x_i = e_i + p_i,\quad p_i \in \mathbb{R}^d
$$
**优点**
* 表达能力强
* 短序列任务效果好

**核心问题（工程上不可接受）**
1. **无法外推到未见长度**
2. embedding 表大小与 max length 线性增长
3. 在长上下文任务中不稳定

**论文 / 模型**
* BERT（Devlin et al., 2018）
* GPT-1 / GPT-2（Radford et al.）

##### 相对位置编码（第一代主流替代方案）
1. Transformer-XL 相对位置编码（向量形式）

Attention 分数中显式引入相对距离：
$$
\mathrm{Attn}*{ij} = q_i^\top k_j + q_i^\top r*{i-j}
$$

**特点**
* 相对位置显式建模
* 对序列长度泛化更好

**问题**
* 实现复杂
* 计算与存储成本高
* 不利于大规模并行

**论文**
* *Transformer-XL*, Dai et al., 2019

2. 相对位置偏置（Relative Position Bias）【工程主流】

**机制（以 T5 为代表）**
$$
\mathrm{Attn}*{ij} = q_i^\top k_j + b*{i-j}
$$

其中：
* (b_{i-j}) 是可学习标量
* 通常采用 **bucket 化距离**

**优点**
* 参数量极小
* 与 Attention 结构天然融合
* 稳定、易实现、可扩展

**论文与模型**
* *T5*, Raffel et al., 2020
* PaLM
* FLAN
* ViT v2（部分变体）

##### 旋转位置编码（RoPE）
1. RoPE（Rotary Position Embedding）

不是“加位置”，而是**旋转 Query / Key 空间**：
$$
\mathrm{Attn}*{ij}
= (R*{pos_i} q_i)^\top (R_{pos_j} k_j)
$$
等价于：
$$
\mathrm{Attn}_{ij} \propto \cos(\theta(pos_i - pos_j))
$$

**数学本质**
* 在复平面进行相位旋转
* Attention 内积只依赖相对位移

**优点**
* 无额外参数
* 与注意力内积深度耦合
* 对长序列泛化显著优于 APE

**论文**
* *RoFormer*, Su et al., 2021

##### 线性偏置位置编码（ALiBi）
1. ALiBi（Attention with Linear Biases）
$$
\mathrm{Attn}_{ij} = q_i^\top k_j - \alpha_h |i - j|
$$

* 每个 attention head 一个 slope
* 无 embedding、无旋转

**优点**
* 极强长度外推能力
* 实现极简
* 推理成本最低

**缺点**
* 表达能力弱于 RoPE
* 在复杂语言建模中略逊

**论文**
* *Train Short, Test Long*, Press et al., 2021

##### RoPE 改进与现代长上下文方案（2023–）
NTK-aware RoPE / xPos / YaRN（工程增强）

原始 RoPE 在超长上下文中：
* 角频率过密
* 远距离 token 混叠

**代表工作**

1. NTK-aware RoPE
* 调整频率缩放
* LLaMA 2 长上下文版本

2. YaRN
* 分段频率插值
* 保留低频稳定性
* 论文：*YaRN*, Peng et al., 2023

#### Attention（MHA/GQA/MQA/MLA）
标准多头注意力（MHA）在单层对长度为 (n) 的序列、batch size (B)、头数 (H)、每头维度 (d_k) 的情况下，关键成本（单层）为：

* 计算量（近似）：$(O(B \cdot H \cdot n \cdot d_k + B \cdot H \cdot n^2))$（QK 内积产生 ($n\times n$) 矩阵）
* 内存（KV 保存供自回归解码使用时）：若保存 keys/values 到缓存，KV tensor 大小为 $(B \times n \times H \times d_k)$。

这说明：
- （a）自回归**推理**时 KV 缓存的读写成为瓶颈
- （b）若 H 很大，KV 的存储/带宽按比例放大。

原始 MHA 的设计在训练/解码规模放大时会碰到显著的 **内存带宽与显存** 问题。

##### Multi-Query Attention (MQA)
**核心思想**：所有 `query heads` 共用同一组` Key/Value` 投影（即只为 `Keys/Values` **保留 1 个头而不是 H 个**），而每个 head 仍有自己独立的 Query。

**数学 / 复杂度变化**

* 原 KV 大小：$(B \times n \times H \times d_k)$
* MQA KV 大小：$(B \times n \times 1 \times d_k = B \times n \times d_k)$

**KV 大小减少因子 ≈ (H)**（近似），因此 KV **读写带宽**与**缓存显存**在自回归解码时降低近 H 倍（最重要的工程收益）。

计算上：注意力分数计算仍需要为每个 `query head` 计算 $(Q_h K_{shared}^\top)$，但对 KV 的加载/传输开销大幅减少。

**工程/经验影响**

* **极大提升解码吞吐/降低延迟**（尤其在长上下文与大 batch 自回归生成时）。
* 可能引入**质量/表达能力下降**，因为不同 head 不能再拥有各自的 K/V 表征（有些语义/模式分工被弱化）。
* 在工程上常用于 decoder-only LLM 的推理优化；通常需要在训练/微调阶段适配或 uptraining 才能最小化性能下降。

**代表/出处**
* Shazeer, *Fast Transformer Decoding: One Write-Head is All You Need* (2019).

##### Grouped-Query Attention (GQA)
**核心思想**：在 MHA 与 MQA 之间做折衷。把 (H) 个 `query head` 按组分成 G 组$(1 \le G \le H)$，每组共享一组 `K/V`（组内的多个 `query heads` 共享该组的 `K/V`）。当 `G=H` 时退化为 MHA；当 `G=1` 时退化为 MQA。

**数学 / 复杂度**
* KV 大小：$(B \times n \times G \times d_k)$。相对于 MHA 的压缩因子为 (H/G)。
* 当 $(G \ll H)$ 时，可显著减少 KV 缓存大小与带宽，但保留比 MQA 更好的多样性。

**工程与训练要点**
* GQA 在实务中常采用 **先把已有 MHA checkpoint 转换为 GQA，再做少量 uptraining（低成本微调）**，即可恢复大部分质量（Ainslie et al. 的方法论）。这使 GQA 成为实用迁移/加速策略，而不需要从头训练新的架构。
* 选择 (G) 是工程折中：较小的 (G) 带来更大推理加速但更可能损失建模能力；文献证明用少量 uptraining（如 5% 原训练 compute）可以把损失压缩到可接受范围。

**代表/出处**
* Ainslie et al., *GQA: Training Generalized Multi-Query Transformer Models* (EMNLP 2023).

##### 为什么 MQA/GQA 在「推理工程」中有效（量化说明）
考虑自回归生成逐步解码（生成第 (t) 步）时必须读取并使用前 (t-1) 个 token 的 KV。假设每个 KV 元素为 2 字节（BF16），batch B=1：

* MHA KV bytes ≈ $((t-1)\cdot H \cdot d_k \cdot 2)$
* MQA KV bytes ≈ $((t-1)\cdot 1 \cdot d_k \cdot 2)$

因此在长上下文（t 很大）或多卡 KV 传输场景下，带宽/显存压力下降近 (H) 倍，能显著降低推理延迟与跨卡通信开销（实测在多篇工程文章/博客里呈现为数倍到十倍的吞吐改进）。GQA 在 (H/G) 维持同类效果但更平衡质量。

##### MLA
&gt; [缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)

##### FlashAttention：微架构/算法级的加速（与 MQA/GQA 不冲突）
**FlashAttention** 不是新的注意力「结构」而是 **IO-aware 的 exact attention 实现**：它通过 tile 分块、在片上 SRAM 中完成 softmax 计算，避免构造完整 (n\times n) 中间矩阵，减少 HBM↔SRAM 读写，从而实现**内存与时间双降**（并能配合 MHA/MQA/GQA 使用）。

* 对比：标准实现需要存储 (QK^\top) 或若干中间缓冲；FlashAttention 用流式（tiling）做数值稳定的 softmax，memory footprint 大幅降低，wall-clock 加速显著。
* 工程价值：训练与推理中都能直接减少显存、加速训练步长、延长可支持的上下文长度。[&#34;FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness&#34;](https://arxiv.org/abs/2205.14135?utm_source=chatgpt.com )

##### 稀疏/图式注意力（窗口 + 全局 / 随机）
* **Longformer**：局部滑动窗口 + 少数 global tokens，复杂度 (O(n))。适合许多长文档任务。
  * 参考：&#34;Longformer: The Long-Document Transformer&#34;
* **BigBird**：窗口 + 随机 + 全局三分法，理论上能近似完整 attention（Turing complete），并实现线性复杂度。
  * &#34;Big Bird: Transformers for Longer Sequences&#34;

##### 低秩 / 投影近似
* **Linformer**：对 K/V 在序列维投影到低维空间，假设 attention 矩阵近似低秩，从 (O(n^2)) 降到 (O(n))。
* &#34;Linformer: Self-Attention with Linear Complexity&#34;

##### 随机特征 / 核线性化（线性注意）
* **Performer (FAVOR(+))**：用随机特征逼近 softmax kernel，使 attention 复杂度变为线性（理论上有可控误差界）。适合超长序列但在某些任务上与 exact attention 有差距。
* &#34;[2009.14794] Rethinking Attention with Performers&#34;

##### LSH / 分组注意力
* **Reformer**：用 Locality-Sensitive Hashing 将相似 queries 聚类，仅与同 bucket keys 交互，复杂度 (O(n \log n))，并配合可逆网络节省激活内存。
* &#34;Reformer: The Efficient Transformer&#34;

#### LayerNorm
##### PostNorm（原始transformer使用）
原始设计（Vaswani et al., 2017）Transformer 使用的是：
* **LayerNorm**
* **Post-LN 结构**

即：
$$
y = \mathrm{LN}(x + \mathrm{Sublayer}(x)) \\\\
\mathrm{LN}(x) = \gamma \cdot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
$$
在小模型（6–12 层）中是可行的，但在大模型中暴露出严重问题。
- （1）深层梯度不稳定（致命）

对输入梯度近似为：
$$
\frac{\partial y}{\partial x} \approx \frac{\partial \mathrm{LN}}{\partial x} \cdot (I + \partial \mathrm{Sublayer})
$$

LayerNorm 的梯度是**缩放 + 去均值的非对角矩阵**，导致：
* 梯度被反复缩放
* 残差通道不再是“近似恒等映射”
* 层数 ↑ → 梯度消失 / 爆炸

**结论**：

&gt; 原始 LN + Post-LN 结构无法稳定扩展到 40+、80+ 层

##### Pre-LayerNorm（结构级革命）
**结构变化**

从：$y = \mathrm{LN}(x + \mathrm{Sublayer}(x))$

变为：$y = x + \mathrm{Sublayer}(\mathrm{LN}(x))$

**核心数学差异**

残差分支的梯度：
$$
\frac{\partial y}{\partial x} \approx I + \frac{\partial \mathrm{Sublayer}}{\partial x}
$$

即：
* 梯度可以**绕过 LayerNorm**
* 残差路径近似恒等映射

**工程结果**

* 训练深度可从 12 层 → 80+ 层
* 几乎所有现代 LLM 的**必要条件**

**论文支撑**

* *On Layer Normalization in the Transformer Architecture*
  Xiong et al., 2020
* *Understanding the Difficulty of Training Transformers*
  Liu et al., 2020

**实际采用**
* GPT-2 之后所有 GPT 系列
* LLaMA / Qwen / DeepSeek / Mistral

##### RMSNorm（LayerNorm 的结构简化）
&gt; RMSNorm：去均值是否真的必要？

**RMSNorm 定义**
$$
\mathrm{RMSNorm}(x) = \gamma \cdot \frac{x}{\sqrt{\frac{1}{d}\sum_i x_i^2 + \epsilon}}
$$

**与 LayerNorm 的本质差异**

| 项目      | LayerNorm | RMSNorm |
| ------- | --------- | ------- |
| 去均值     | 是         | 否       |
| 方差归一    | 是         | 是       |
| 可学习偏置 β | 有         | 无       |
| 参数量     | 2d        | d       |

**理论与经验动机**

在 **Pre-LN 架构**下：
* 均值偏移不会随深度累积（残差稳定）
* 方差才是主要不稳定来源
* 去均值的收益极小，但计算与通信成本真实存在

**工程收益**
* 计算更快
* 数值更稳定（尤其在 BF16）
* Kernel 更容易融合

**论文**
* *Root Mean Square Layer Normalization*
  Zhang &amp; Sennrich, 2019

**实际采用**
* LLaMA 系列
* Qwen 系列
* DeepSeek
* Mistral

&gt; **RMSNorm 是当前 Decoder-only LLM 的事实标准**

##### Norm 位置与残差缩放的协同设计
**问题**

即使 Pre-LN + RMSNorm，在 **100+ 层**时仍会出现：
* 残差累积导致激活范数缓慢上升

**解决策略（多种等价形式）**

- （1）Residual Scaling
$$
y = x + \alpha \cdot \mathrm{Sublayer}(\mathrm{Norm}(x))
$$
其中：$\alpha \sim \frac{1}{\sqrt{L}}$

- （2）小初始化（等价效果）
  * 输出投影权重使用更小 std
  * FFN / Attention 输出更“保守”

**理论直觉**

让残差路径在深度上保持：
$$
\mathbb{E}|x^{(l)}|^2 \approx \text{常数}
$$

**论文 / 技术来源**
* *DeepNet: Scaling Transformers to 1,000 Layers*
  Wang et al., 2022
* *Stabilizing Transformer Training by Preventing Internal Covariate Shift*（相关分析）

**实际情况**
* 多数大厂模型使用该思想
* 但**不会在论文或代码中显式标注**（工程细节）

##### Norm 相关的其他重要变体（较少但真实存在）

1. ScaleNorm（轻量替代）
$$
\mathrm{ScaleNorm}(x) = \frac{g}{|x|} x
$$

* 仅控制向量模长
* 参数极少
* 表达能力弱于 RMSNorm

**论文：**
* *Transformers without Tears*
  Nguyen &amp; Salazar, 2019

&gt; 实际 LLM 中不如 RMSNorm 常见

2. NoNorm / μParam（研究方向）

**核心思想**
* 利用初始化与参数化控制尺度
* 显式去除归一化层

**问题**
* 对初始化极度敏感
* 工程鲁棒性差

**代表论文**
* *µParam: A Unified Framework for Scaling Transformers*
  Yang et al., 2022

&gt; **尚未成为主流工程方案**

##### 主流模型真实采用情况对照表

| 模型             | Norm 类型   | LN 位置   | 备注    |
| -------------- | --------- | ------- | ----- |
| 原始 Transformer | LayerNorm | Post-LN | 仅适合浅层 |
| BERT           | LayerNorm | Post-LN | 深度有限  |
| GPT-2          | LayerNorm | Pre-LN  | 过渡阶段  |
| LLaMA          | RMSNorm   | Pre-LN  | 主流范式  |
| Qwen           | RMSNorm   | Pre-LN  | 主流范式  |
| DeepSeek       | RMSNorm   | Pre-LN  | 主流范式  |
| Mistral        | RMSNorm   | Pre-LN  | 主流范式  |

#### FFN
现代预训练模型对 Transformer 中 FFN 的优化，本质是:
- 用门控非线性（SwiGLU）和更合理的参数分配，显著提升单位参数的表达能力，
- 并在高端模型中通过 MoE 引入条件计算以突破容量瓶颈；
- 在工程上，FFN 已成为算力与优化的主战场，而不再只是 Attention 的“配角”。

**主流 FFN 技术路线对照表**

| 维度     | 原始 Transformer | 现代主流 LLM       |
| ------ | -------------- | -------------- |
| 激活     | ReLU           | SiLU           |
| 结构     | 单支路            | SwiGLU         |
| 宽度     | 4d             | ≥4d（更优）        |
| 参数占比   | ~50%           | 60–70%         |
| 稀疏性    | 无              | MoE（可选）        |
| Kernel | 标准 GEMM        | Fused / Triton |


##### 原始 Transformer 中 FFN
1. 原始 FFN 定义（Vaswani et al., 2017）
$$
\mathrm{FFN}(x)=Linear_2(ReLU(Linear_1(x)))\\\\
\iff\mathrm{FFN}(x)=\max(0, xW_1 + b_1) W_2 + b_2
$$

其中：
* $W_1 \in \mathbb{R}^{d \times d_{ff}}$
* $W_2 \in \mathbb{R}^{d_{ff} \times d}$
* 通常 $d_{ff} = 4d$

**原始定位**
* Attention：**token 间交互**
* FFN：**token 内的非线性特征变换**

2. 原始 FFN 在大模型中的问题

（1）非线性表达能力有限
* ReLU 在负半轴完全为 0
* 激活分布稀疏、梯度不连续
* 在深层堆叠中表现不稳定

（2）参数与算力利用效率不高
* 同样的参数预算下，ReLU-FFN 的表达能力不如门控结构
* 在 Scaling Law 视角下不是最优结构

（3）数值稳定性与混合精度不友好
* ReLU 的 hard cutoff
* 对 BF16 / FP16 训练不友好

##### 激活函数升级（ReLU → GELU / SiLU）
1. GELU（过渡方案）
$$
\mathrm{GELU}(x)=x \cdot \Phi(x)
$$
* 平滑近似 ReLU
* 在 BERT、早期 GPT 中使用

论文：
* *Gaussian Error Linear Units (GELUs)*
  Hendrycks &amp; Gimpel, 2016

**局限**
* 非门控结构
* 在大规模生成模型中逐渐被淘汰

2. SiLU / Swish（现代门控的基础）
$$
\mathrm{SiLU}(x) = x \cdot \sigma(x)
$$
* 平滑
* 梯度连续
* 数值稳定性好

**论文：**
* *Swish: A Self-Gated Activation Function*
  Ramachandran et al., 2017

&gt; **SiLU 成为后续 GLU 系列的核心激活函数**

##### 门控 FFN（GLU 系列）【事实标准】
1. GLU 家族的统一视角

一般形式：
$$
\mathrm{FFN}_{GLU}(x)=(xW_1) \odot g(xW_2)
$$
其中：
* $g(\cdot)$ 为激活函数
* $\odot$ 为Hadamard积，即逐元素相乘（门控）

2. SwiGLU：当前 Decoder-only LLM 的事实标准

**定义**
$$
\mathrm{SwiGLU}(x)=(xW_1) \odot \mathrm{SiLU}(xW_2)\\\\
\mathrm{FFN}(x)=\mathrm{SwiGLU}(x)W_3
$$

**与原始 FFN 的对比**

| 维度   | ReLU-FFN | SwiGLU-FFN |
| ---- | -------- | ---------- |
| 非线性  | 单支路      | 门控双支路      |
| 激活   | ReLU     | SiLU       |
| 梯度   | 不连续      | 连续         |
| 表达能力 | 较弱       | 显著更强       |

**理论与经验动机**

- （1）条件计算（Conditional Computation）
  * 门控决定哪些通道被激活
  * 等价于软稀疏专家选择
- （2）更高的函数逼近效率
  - 在相同参数预算下：GLU &gt; ReLU（经验与理论一致）
- （3）更好的梯度传播
  * 乘性门控避免 ReLU 的“死亡神经元”

**论文支撑**
* *GLU Variants Improve Transformer*
  Shazeer, 2020
* *PaLM*（Chowdhery et al., 2022，附录实验）

**实际采用**
* LLaMA / LLaMA 2 / LLaMA 3
* Qwen
* DeepSeek
* Mistral

&gt; **这是目前 FFN 结构层面的最大共识**

##### 参数重分配：为什么 FFN 仍是 4d 甚至更大？
**观察事实**
* FFN 通常占 **60%–70% 的参数量**
* Attention 占比反而下降

**理论依据：Scaling Laws**

**论文：**
* *Chinchilla: Training Compute-Optimal Large Language Models*
  Hoffmann et al., 2022

**结论：**
* 在给定计算预算下，**更大的 FFN 宽度比更多 Attention heads 更划算**
* FFN 是主要的“记忆与模式存储单元”

##### MoE（Mixture-of-Experts）FFN
1. MoE-FFN：用稀疏性换取容量

**基本结构**
$$
\mathrm{FFN}_{MoE}(x)=\sum_{e \in \mathcal{E}} p_e(x) \cdot \mathrm{FFN}_e(x)
$$
* 每个 token 只激活 Top-k 个 expert
* 总参数量 ↑↑，计算量近似不变

**理论动机**
* FFN 是主要容量瓶颈
* MoE 实现 **条件计算 + 参数规模解耦**

**关键工程挑战**
* 路由不均衡
* 通信开销
* 推理复杂度

**代表论文**
* *Switch Transformers*
  Fedus et al., 2021
* *GShard*
  Lepikhin et al., 2020

**实际使用**
* PaLM-2（部分）
* Mixtral（Mistral MoE）
* DeepSeek-MoE

&gt; **MoE 是“高端路线”，但不是所有模型的默认选择**

##### 数值与工程层面的 FFN 优化
1. 初始化与输出缩放

**问题**
* FFN 输出方差随深度累积

**对策**
* 输出投影使用更小 std
* 或显式 residual scaling：
  $$
  x_{l+1} = x_l + \alpha \cdot \mathrm{FFN}(\cdot),\quad \alpha \sim \frac{1}{\sqrt{L}}
  $$

**论文：**
* *DeepNet*
  Wang et al., 2022

2. Kernel 融合与算子优化

**工程实践**
* fused Linear + Activation + Linear
* FlashFFN / Triton kernel
* 减少内存访问与 kernel launch

**影响**
* FFN 在推理中常占 &gt;50% 的 wall-time
* kernel 优化比结构微调更重要

3. 低精度与量化适配

* BF16 / FP16 训练
* FFN 权重对量化更敏感 → 需分组量化 / SmoothQuant
* 但 FFN 仍是量化收益最大的模块之一

#### 训练与数值稳定性优化
一个工业可行的预训练 pipeline（高层）通常包含：
- 架构：Pre-LN + RMSNorm + residual scaling（若需要极深则 DeepNorm）；
- 初始化：Xavier/He 的基础 + 若不使用 Norm 可用 T-Fixup/DeepInit；
- Optimizer：AdamW（大多数）或 Adafactor（内存受限），可在需要时尝试 Lion / LAMB（大 batch）；
- 精度：BF16（或混合 FP16 + master FP32）+ 动态 loss scaling；
- LR：短 warm-up（若 Pre-LN 可弱化）+ linear/cosine decay；
- 数值保护：global gradient clipping、overflow 检测、健壮 softmax（FlashAttention）；
- 系统：ZeRO/FSDP + FlashAttention + fused kernels + activation checkpointing；
- 生产化监控与逐步放大测试。

#### 工程优化
现代大模型预训练的工程优化，本质是围绕显存、通信和算子效率构建一套多层次并行与内存管理体系：
- 以 ZeRO/FSDP 为核心解决参数规模问题
- 以 FlashAttention 和 kernel 融合解决算子与带宽瓶颈
- 并通过混合精度与重计算在可控 FLOPs 增长下换取可扩展性

**主流工程优化技术全景表**

| 类别     | 技术                    | 是否主流 |
| ------ | --------------------- | ---- |
| 并行     | ZeRO / FSDP           | 是    |
| 并行     | Tensor Parallel       | 是    |
| 并行     | Pipeline Parallel     | 条件   |
| 显存     | Activation Checkpoint | 是    |
| 精度     | BF16                  | 是    |
| Kernel | FlashAttention        | 是    |
| Kernel | Fused FFN             | 是    |
| IO     | 高效数据管道                | 是    |
| 推理     | GQA/MQA               | 是    |

##### 工程优化的总体目标与约束
在大模型预训练阶段，工程优化的核心目标并非“提高理论表达能力”，而是：

1. **把硬件算力转化为有效 token throughput**
2. **在数值稳定前提下最大化并行度**
3. **降低显存、通信、IO、kernel launch 等非算力瓶颈**
4. **保证系统可扩展到数百/上千 GPU 的稳定运行**

这决定了工程优化几乎全部围绕以下四类瓶颈展开：

* 显存（memory）
* 通信（communication）
* IO / 带宽（bandwidth）
* kernel / 算子效率（compute utilization）

##### 并行化体系（最核心的工程优化）
1. 数据并行（Data Parallel, DP）及其局限

**基本思想**
* 每张卡保存一份完整模型
* 不同 batch 分配到不同卡
* 反向传播后 All-Reduce 梯度

**问题**
* 参数 + optimizer state 全复制 → **显存线性爆炸**
* 在百亿/千亿参数下不可行

2. ZeRO：工程上的分水岭（事实标准）

**核心思想（Rajbhandari et al.）**

把冗余状态拆分（partition）到不同设备：

| Stage  | 拆分内容             |
| ------ | ---------------- |
| ZeRO-1 | Optimizer states |
| ZeRO-2 | + Gradients      |
| ZeRO-3 | + Parameters     |

**数学/工程本质**
* 把原本 (O(N)) 的显存需求降为 $O(N / {GPUs})$
* 用通信换显存

**工程影响**
* **使百亿到千亿模型成为“常规工程问题”**
* 是 DeepSpeed、FSDP 的理论基础

**论文：**
* *ZeRO: Memory Optimizations Toward Training Trillion Parameter Models*
  Rajbhandari et al., SC 2020

3. FSDP（Fully Sharded Data Parallel）

**本质**
* PyTorch 官方对 ZeRO-3 的实现与工程化增强

**优化点**
* 参数、梯度、optimizer state 全分片
* fine-grained shard（按 module）
* 与 PyTorch autograd 深度集成

**工程实践**
* 当前 PyTorch 生态的**事实标准**
* Qwen / LLaMA 生态训练广泛使用

**论文：**
* *FSDP: Fully Sharded Data Parallel*
  PyTorch team, 2022（技术报告）

4. 张量并行（Tensor Parallel, TP）

**思想**
* 把**单个矩阵乘法**拆分到多张卡
* 常用于 Attention / FFN 的线性层

**数学形式（示例）**
$$
W \in \mathbb{R}^{d \times d} \Rightarrow
[W_1, W_2, \dots, W_k]
$$

**工程代价**
* 每层都需要通信（All-Reduce / All-Gather）
* 通信频率高

**适用场景**
* 单卡放不下模型
* 与 ZeRO/FSDP 结合使用

**论文：**
* *Megatron-LM*
  Shoeybi et al., 2019

5. Pipeline Parallel（PP）

**思想**
* 按层切分模型，形成流水线
* micro-batch 填充 pipeline

**问题**
* bubble（空泡）导致利用率下降
* 调度复杂

**现状**
* 在极大模型中仍有价值
* 但工程复杂度高，通常是“最后一招”

**论文：**
* *GPipe*
  Huang et al., 2019

##### 显存优化（Memory is King）
1. Activation Checkpointing（重计算）

**思想**
* 前向不保存全部激活
* 反向时重算

**数学权衡**
* 显存：↓
* FLOPs：↑（通常 &lt;2×）

**实践**
* Transformer block 级 checkpoint
* 工业界默认开启

**论文：**
* *Training Deep Nets with Sublinear Memory Cost*
  Chen et al., 2016

2. 混合精度（BF16 / FP16）

**工程事实**
* 不使用混合精度几乎不可能训练大模型

**BF16 优势**
* 更大指数范围
* 数值更稳定
* 减少 overflow/underflow

**工程要点**
* master weights FP32
* loss scaling（FP16）

**论文：**
* *Mixed Precision Training*
  Micikevicius et al., 2018

3. KV Cache 工程优化（训练 &amp; 推理）

**核心**
* KV Cache 是显存大户（O(seq_len × layers × d)）

**技术**
* 分页 KV Cache
* chunked attention
* offload（CPU/NVMe）

**论文：**
* *Efficient Memory Management for Large Language Model Serving*
  Kwon et al., 2023（vLLM）

##### 算子与 Kernel 级优化（吞吐提升的关键）
1. FlashAttention（革命性优化）

**原理**
* IO-aware attention
* 不显式 materialize (QK^T)

**数学本质**
* softmax 的分块数值稳定计算

**工程收益**
* 显存：O(n²) → O(n)
* 速度：显著提升
* 数值稳定性更好

**论文：**
* *FlashAttention*
  Dao et al., NeurIPS 2022

2. Fused Kernels（线性 + 激活 + 线性）

**思想**
* 减少 kernel launch
* 减少中间内存读写

**典型融合**
* QKV fused
* Linear + SiLU + Linear（FFN）

**实际影响**
* 在 FFN 占主导的模型中，**收益非常大**

**工业实现：**
* Triton
* NVIDIA Transformer Engine

3. 通信-计算重叠（Overlap）

**技术**
* backward 中 overlap All-Reduce 与 GEMM
* pipeline overlap

**目标**
* 隐藏通信延迟
* 提高 GPU 利用率

**论文：**
* *Efficient Large-Scale Language Model Training on GPU Clusters*
  Narayanan et al., 2021

##### IO 与数据工程优化（常被低估）

1. 高效数据管道

**问题**
* GPU 常因 data starvation 空转

**技术**
* 多进程预取
* mmap / streaming
* shard + shuffle

**论文：**
* *Efficient Training of Language Models to Fill in the Gaps*
  Fedus et al., 2022

2. Checkpoint 工程

**挑战**
* 千亿参数 checkpoint 可达 TB 级

**技术**
* sharded checkpoint
* async save
* 增量 checkpoint

**实践：**
* DeepSpeed / FSDP checkpoint

##### 推理友好工程（训练阶段即考虑）

1. 结构选择的工程动机

| 设计           | 工程收益        |
| ------------ | ----------- |
| Decoder-only | KV Cache 友好 |
| GQA / MQA    | KV Cache 减少 |
| RMSNorm      | kernel 融合   |

**论文：**
* *GQA: Training Generalized Multi-Query Transformer Models*
  Ainslie et al., 2023

2. 量化友好训练（QAT-aware）
* SmoothQuant
* 分组量化
* FFN-aware scaling

**论文：**
* *SmoothQuant*
  Xiao et al., 2022

---

## 常用工具
### 深度学习框架
#### Pytorch
常用的深度学习框架

#### TensorFlow

### 机器学习库
#### sklearn
它提供了丰富的工具和算法，帮助用户轻松构建、训练和评估机器学习模型。

数据处理：
- 数据集切分：train_test_split

### 数据处理
- numpy
- pandas
- scipy

### 图形化交互
- TensorBoard：可以监听log数据，图形化展示训练曲线
- tqdm：进度条
- netron：模型结构可视化

### LLM网站及工具
#### HuggingFace
Hugging Face 是一家提供开源 AI 工具和平台的公司，致力于简化预训练模型的使用，加速机器学习项目的开发与落地。

最初以 Transformers 库闻名，该库极大地降低了使用 BERT、GPT、T5 等模型的门槛。如今，Hugging Face 已发展成为一个完整的 AI 开发生态系统，支持自然语言处理、计算机视觉、语音处理、多模态任务等多个领域。

Hugging Face 的生态系统主要由两个核心部分组成：

##### 1）Hugging Face Hub
Hugging Face提供了一个集中式的开源平台，用于托管和分享模型、数据集和应用。
- 官网地址为：https://huggingface.co/
- 国内镜像地址为：https://hf-mirror.com/

##### 2）工具链（Libraries）
Hugging Face 提供了一套围绕预训练模型构建的工具库。这些组件彼此独立，又可以协同工作，覆盖了从数据处理到模型训练与推理的完整流程。

各组件具体功能如下：
- Datasets：Datasets 是用于加载和处理数据集的工具库。支持从在线仓库或本地文件（如 CSV、JSON）加载文本数据，并支持清洗、编码、切分等预处理操作。处理后的数据可直接用于模型训练，是连接原始数据与模型输入的重要桥梁。
- Tokenizers：Tokenizers 是用于将文本转换为模型输入的工具。它支持文本分词、编码为 token ID，同时自动处理特殊符号、填充（padding）、attention mask 和句子对标记（token type ID）。分词器通常与模型配套使用，可通过统一接口加载。
- Transformers：Transformers 是 Hugging Face 最核心的库，用于加载、使用和微调各种预训练模型。该库统一了模型接口，支持数百种模型结构，如 BERT、GPT 等，用户可以通过一行代码 from_pretrained()直接加载公开模型，快速用于推理或训练。

#### ModelScope
类似HuggingFace，由Alibaba维护。

# NLP进阶

# Materials
## lessons/presentation/blogs
- [cs224n](https://web.stanford.edu/class/cs224n/)
- [illustrated-transformer](https://jalammar.github.io/illustrated-transformer/)
- [huggingface LLM course](https://huggingface.co/learn/llm-course/en/chapter1/6)
- [cs336](https://stanford-cs336.github.io/spring2025/)
- [苏剑林-苏神博客](https://kexue.fm/)

## Papers/
- [ACL Anthology](https://aclanthology.org/)（ACL、EMNLP、NAACL 等会议论文集合）——查 NLP 最新论文的权威库。 
- arXiv / Papers with Code / Semantic Scholar —— 快速跟踪最新 preprints 与复现代码。
- 关注主要会议：ACL, EMNLP, NeurIPS, ICML, ICLR。

## Books
- [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3)
- [Practical Deep Learning](https://course.fast.ai/)
- [Natural Language Processing with Transformers](https://github.com/nlp-with-transformers/notebooks)

# References
- [尚硅谷大模型技术之NLP1.0.3.pdf](/pdf/nlp/尚硅谷大模型技术之NLP1.0.3.pdf)
- [尚硅谷大模型技术之NLP1.0.3.docx](/pdf/nlp/尚硅谷大模型技术之NLP1.0.3.docx)
- [自然语言处理 基于预训练模型的方法.pdf](/pdf/nlp/自然语言处理_基于预训练模型的方法.pdf)
&lt;/div&gt;
&lt;/div&gt;


</description>
        </item>
        
    </channel>
</rss>
