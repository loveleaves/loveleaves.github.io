[{"content":"\rUnlock to view this content.\r## 系统工程基础 ### 系统工程概述 \u003e [系统工程全景综述：发展简史、核心方法与实践](https://www.cnblogs.com/haohai9309/p/18931861) 在复杂世界中，单一学科或单一技术往往无法独立解决现实问题。系统工程（Systems Engineering, SE）应运而生，成为跨越工程、管理、信息科学、经济学等多学科交融的重要方法体系。它关注如何定义、设计、实现和管理复杂系统，在航空航天、军事、制造、信息技术、交通、电力、金融等众多领域广泛应用。 系统工程的核心特点包括： - **整体性（Holism）**：强调整体最优而非局部最优； - **生命周期管理**：覆盖系统全生命周期各阶段； - **跨学科整合**：整合多学科知识协同工作； - **复杂性管理**：面对多目标、多约束、动态变化的复杂系统问题； - **可验证性与可追溯性**：关注需求、设计与验证之间的逻辑一致性。 这里以系统工程的发展简史为起点，系统梳理其核心方法论，重点介绍Hall经典七步法，并结合多个典型应用案例，展现系统工程在实际复杂工程中的逻辑思维、操作流程与应用价值。 ### Hall的硬系统工程方法论 \u003e [系统工程全景综述：发展简史、核心方法与实践](https://www.cnblogs.com/haohai9309/p/18931861) 在系统工程学科的发展历程中，A.D. Hall（阿瑟·霍尔）无疑是最具奠基意义的学者之一。其在1962年出版的名著 A Methodology for Systems Engineering，第一次系统性提出了完整严谨的系统工程操作流程，被称为硬系统工程方法论（Hard Systems Engineering Methodology, HSEM），也常被称作Hall七步法。 Hall方法论成为此后几十年系统工程、国防工程、复杂项目管理、软件系统开发、智能制造规划等领域的标准范式，其核心贡献有三： - **流程规范化**：提出了逻辑清晰、环节严密的七步工作流程； - **问题求解导向**：强调系统工程作为复杂问题求解的科学方法； - **跨学科集成**：将工程、管理、经济、社会、决策多学科系统性整合。 ### IPD 系统工程概念体系 - 特点：面向产品全生命周期，以满足客户需求为目的，全量设计。 - 演进路线： 1. 信息化 2. 结构化 3. 模型化 4. 数字化 5. 智能化 - 范围： - 对象：系统工程的工程对象，目标系统 - 资产：工程过程/活动中产生的有价值的数据对象，如系统特性、功能、需求、系统元素、系统用力等 - 过程/活动：需求分析、特性定义、系统功能定义、架构设计、系统设计、软件开发、硬件开发、集成与验证等 - 方法：建模、仿真、功能分配矩阵、DFX专业设计等 - 需求（requirement）与需要（need） #### 系统分析与设计流程 1、领域分析与方案设计： - 领域分析：市场/竞争分析 - 方案实现评估： - 方案设计（RR 原始需求/Raw Requirement，IR 利益干系人需求/Interest Requirement）：利益相关方需求分析=》系统需求分析、测试策略分析 2、系统需求分析： - 系统需求分析 - 构建全量功能 - 刷新全量功能 3、架构与系统设计： - 架构设计： - 系统设计（SR 系统需求/System Requirement）： - 功能设计（AR 分配需求/Assigned Requirement）： 4、架构与设计实现支撑： - 架构与设计实现 - 需求与设计实现评估 ### MBSE 基于模型的系统工程 \u003e [什么是MBSE？](https://docs.pingcode.com/requirements-management-guide/systems-engineering/what-is-mbse-model-based-systems-engineering-explained) \u003e [MBSE系列: 方法论之RFLP](https://zhuanlan.zhihu.com/p/600073245) 在传统的系统工程应用过程中，不管是需求，还是架构都是以文档为中心，每个阶段工作输出结果的载体都是文档，不同团队和阶段的交流也都是基于文档。 但随着系统复杂性的上升，以文档为中心的系统工程（Text Based Systems Engineering，TBSE）方法存在很多问题，例如: - 系统复杂化，文档数量增多，不便于书写，管理 - 文字描述存在歧义，对团队沟通协作不友好，不同部门及开发人员难以形成统一理解 - 需求变更，追溯困难 - 文档无法仿真验证 为了解决TBSE所存在的问题，国际系统工程学会(INCOSE)提出了基于模型的系统工程(Model Based Systems Engineering，MBSE)方法。 MBSE强调以图形化模型为核心，将SE方法论过程中设计内容通过工具模型化，电子化，构建不同阶段开发模型，例如需求模型，架构模型，详细设计模型等，并彼此相互关联，以此增加输出内容的可复用性、形成系统一体化设计，可以有效解决随着系统复杂度提高对传统基于文档的系统工程带来的挑战，这个也是目前大部分规范，例如ISO 26262，ASPICE等，对工作输出产物的基本要求。 模型既然是MBSE的核心，那么如何建立模型是MBSE的关键任务，所以为建立系统化模型，MBSE在SE原有方法论的基础上，新增了: - 建模语言 - 工具 这两项内容，这也就构成了MBSE落地三大基本要素PMT，即:流程(Process), 方法(Method)和工具Tools，所以简单地说: MBSE = SE方法论 + 建模语言 + 工具 建模语言定义了建模过程中可以使用的可视化元素及其代表的意义。为消除歧义，MBSE要求使用统一化的建模语言SysML。它以图像化的方式定义了不同类型的视图表达方式，例如需求视图，结构视图，流程视图，状态视图等等，这些视图相互补充，可以用于表达MBSE设计过程中关注对象的不同侧面(View)，进而形成完整的系统架构模型。 但应该如何通过这些视图将SE方法论中的工作内容尽可能有层次地，可追溯地，完整地统一在一个系统化模型中，实现一体化的模型设计呢？ 答案就是**RFLP方法**。 所谓的RFLP 方法就是： - R: 需求(Requirement) - F: 功能(Function) - L: 逻辑(Logical) - P: 物理(Physical) RFLP认为我们应该从以上四个角度(Viewpoint)去对待系统，通过建立相应的架构模型，将复杂技术产品的实现过程进行完整映射。关于RFLP进一步内容我会在架构专题中进一步探讨。 工具是指在实施MBSE过程中建立，建立及管理模型所使用的软件工具，它们可能是多个工具，包括了需求定义及管理工具，架构建模工具，测试验证管理工具等，它们之间通过接口可以进行交互，建立可追溯性，也可以是自主开发的综合性设计工具，将MBSE过程建模统一在一起。 个人认为，MBSE方法论大家都比较熟悉，MBSE落地实施的关键除了思维转换，摈弃原有基于零部件的开发，以及开发流程匹配外，模型的建立是关键，而有效的工具支持是建立及管理模型的必要条件，它能帮助将工程师从繁杂的文档工作中解放出来，专注于可视化模型的建立，整个产品研发以模型作为驱动，提高产品开发效率。 聊了这么多关于SE或MBSE基本内容，我们现在回过头来再看下标题中的问题: Q: 为什么MBSE是系统复杂性的应对之道？ 这个问题有点大，我尝试回答如下: 所谓复杂性，主要是源于研究对象结构复杂，组成元素相互作用，存在错综复杂的关系，使得我们不能由局部来认识整体。所以解决复杂性的根本在于从整体上认识系统，将其进行系统化拆分，弄清其组成元素的相互作用关系，并通过有效的手段对其进行抽象化，直观化，使得我们能够从整体上认识和理解系统。 而SE或MBSE方法论的核心正是从系统角度认识整体，一方面通过系统的方法论，重视需求和架构的作用，及时对系统进行验证和确认。另一方面，开发过程以模型为驱动，通过系统模型以图像化方式直观地揭示系统的组成及它们之间的联系，让工程师能够以一种更加容易的方式掌控系统复杂性。 ## 技术规划与创新 ### 技术洞察 #### 什么是技术洞察？ \u003e [技术洞察概述](https://zhuanlan.zhihu.com/p/711406819) \u003e [技术洞察的关键步骤和方法](https://zhuanlan.zhihu.com/p/720741325) \u003e [架构师面试必问：技术选型方法论全解析——如何为团队引入新技术](https://cloud.tencent.com.cn/developer/article/2595553) 技术洞察（TI， Technology Insight），是指对技术发展趋势、创新机会以及潜在影响的深入理解和预见。这种能力不仅要求对现有技术有透彻的了解，还需要对市场动态、社会需求以及科技发展的方向有敏锐的洞察力。 **技术洞察的关键能力要素：** 1. 信息收集能力 2. 观察能力 3. 批判性思维能力 4. 抽象/归纳能力 5. 专业能力 **技术洞察的误区：** 1. 深度：就事论事，没有深入理解事情的本质原因 2. 思考：仅仅是信息罗列，缺乏自己的思考和判断 3. 价值：没从商业视角、系统性思维去看，只看到具体的点，没看到潜在价值点 4. 全面：只看到局部，没有考察全局，推导得出的结论有偏差 **洞察工作流程** 1. 确定洞察方向/目标 2. 技术分解，分析可能的技术方案 3. 输出技术机会 4. 结果应用 #### 技术洞察的工作方法 五看： - 看趋势：技术趋势分析（演进、变更、融合） - 看市场：产业、合作伙伴、市场空间变化 - 看客户：客户痛点、客户需求分析 - 看竞争：竞争对手分析 - 看自己：差距分析、SDOT ## 需求分析 ## 系统设计 ## 全流程DFX设计 ## AI基础与工程化 ","date":"2025-11-09T00:00:00Z","permalink":"https://loveleaves.github.io/p/systems_fundamentals_intro/","title":"【系统工程基础】 系统工程基础"},{"content":"电子DIY一般流程 需求分析 \u0026amp; 方案设计 目标：明确产品要做什么、为什么做、怎么做。\n任务：\n明确功能需求（例如：温湿度监测器 + 显示 + WiFi 上传） 市场／项目定位（原型、爱好、小批量） 系统级方案选型（MCU类型、通信方式、显示、传感器） 画系统框图（block diagram：电源 → 主控 → 传感器/驱动 → 界面/通信） 常用工具／技术：\n手绘草图或白板画框图 UML 或流程图工具（比如 draw.io、Visio） 功能分解表格（Excel／Google Sheets） 技术调研（查 MCU／传感器规格、模块成本、已有参考模块） 电路原理图设计 目标：将方案中的功能模块具体化为原理图电路。\n任务：\n制定各模块（电源、MCU、通信、传感器、人机界面）电路。 引脚连接、去耦电容、保护电路、接口预留。 画出电路图、进行电气规则检查（ERC）。 常用工具：\nKiCad：开源、跨平台。 EasyEDA：在线工具，方便快速原理图设计。 Altium Designer／立创EDA：商业级工具。 仿真工具（如 SPICE 模块）用于验证关键电路。 注意事项：\nMCU 通信接口（UART、I2C、SPI）建议预留调试口。 电源模块建议选择带过流、反接保护。 模块化思维：如果使用模块（如 ESP32 模组／WiFi 模组）可减少设计量。 器件选型 \u0026amp; BOM（物料清单） 目标：确定每一个元件型号、封装、供应商、价格。\n任务：\n根据性能（如功耗、尺寸、封装、通信速率）选型。 考虑供应链／可得性（避免难找的器件造成后期延迟）。 制作 BOM 表格：元件名、型号、封装、数量、供应商、单价、备注。 常用工具／技术：\nExcel／Google Sheets：用于BOM整理。 器件选型网站：如 LCSC、Digi-Key、Mouser。 交叉比对：备选型号、价格、工厂库存。 注意事项：\n封装越小焊接越困难（如 0201 电阻）。 若是量产还要考虑成本优化、替代元件。 BOM 建议标注 “必需／可选／备用”分类。 PCB 设计 目标：将原理图转化为实物电路板布局（PCB 板）。\n任务：\n导入原理图，生成网络表。 放置元件（合理分区：电源区、模拟区、数字区、接口区）。 布线（Routing）：电源线粗、信号线短、模拟与数字分区。 添加测试点、接地平面、螺丝孔、边框。 检查 DRC（Design Rule Check）、输出 Gerber 文件／钻孔文件。 常用工具：\nKiCad（含原理图→PCB流程）。 Autodesk Fusion 360 PCB 模块：机械一体化设计。 立创EDA。 注意事项：\n对高速信号/差分信号要考虑长度匹配、阻抗控制。 避免信号环路、模拟与数字地分离。 制板文件（Gerber）务必按厂商要求检查。 制板 \u0026amp; 焊接 目标：将 PCB 提交制造、然后焊接元件，形成实物电路板。\n任务：\n提交 Gerber 文件到制造厂（如 JLCPCB、嘉立创等）进行打样。 到板后进行手工或回流焊接。 首次上电前用万用表测量电源是否短路。 分模块焊接与测试（电源模块先，上电检查，再主控再外围模块）。 常用工具／技术：\nPCB 打样服务网站（如 JLCPCB、Elecrow…） 手工焊工具：烙铁、吸锡带、助焊剂。 回流焊／热风枪（对于 SMD 封装）。 万用表、示波器用于基本测试。 注意事项：\n焊接前确认元件极性、方向。 首次上电先用限流电源或串入电阻以防短路烧板。 焊完后再做功能测试，分模块调试（电源→主控→传感器→通信）。 密度高场景，可考虑开钢网+回流焊。 固件／软件编写 目标：让产品“活起来”——编写运行在 MCU 或通信模块上的代码。\n任务：\n初始化各外设（GPIO, I2C, SPI, UART, ADC／PWM 等） 驱动传感器、显示、通信接口（如 WiFi、BLE、MQTT） 实现人机界面（按键、显示、LED） 功耗优化（如睡眠模式、外部中断） 常用工具／技术：\nArduino IDE／PlatformIO（适用于 ESP32/Arduino 生态） STM32 CubeIDE／Keil（适用于 STM32 系列） VSCode＋SDK／Python（例如 Raspberry Pi Pico） lvgl：方便设计人机交互界面 注意事项：\n代码模块化、注释清晰，便于后期升级。 串口输出日志 （Serial 打印）是调试的好朋友。 版本控制（如 Git）建议用于管理固件。 调试 \u0026amp; 测试 目标：验证硬件与软件共同工作情况、发现并修复问题。\n任务：\n电源测试（电压、纹波、散热） 主控启动验证（复位、LED指示、串口输出） 模块验证（传感器数据是否正确、显示是否正常、通信是否成功） 系统测试（整体功能、异常情况处理） 可靠性／EMC／功耗测试（如果条件允许） 常用工具／技术： 万用表、示波器、逻辑分析仪 串口调试助手（PC 软件） 电源分析仪／功耗计（用于低功耗项目） 注意事项：\n分模块先测试，再整体测试。 建议做详细测试用例：正常、异常、边界。 记录测试结果、bug 清单与修复记录。 外壳设计 \u0026amp; 文档化 目标：设计产品外壳（3D打印／激光切割／注塑），完善项目文档。\n任务：\n外壳设计：测量 PCB 尺寸＋元件高度，设计合适的封装结构。 确定制造方式：3D 打印、激光切割亚克力、金属壳、注塑。 外壳设计细节：壁厚、螺丝柱、导轨/滑槽、散热孔、出线口、接口开孔。 文档化：工程图、装配说明、使用说明、BOM、固件烧录流程。 常用工具／技术：\nFreeCAD／OpenSCAD：开源CAD设计外壳。 Fusion 360：机械与电子一体化设计。 Protocase Designer：电子外壳专用模板工具。 Protocase Designer Free Downloadable 3D \u0026hellip; 电子外壳设计教程：Your Guide To Making Electronics Project Boxes 注意事项：\n外壳壁厚建议 2 ～ 2.5 mm 或更大（取决于材料）以保证强度。 考虑散热、接口、装配方便性、硬件固定点。 文档整理好：模块目录结构、使用说明、接线图、固件烧录流程。 开源DIY项目 嘉立创 开源广场 立创开发板 Maker Pro Maker Pro 参考链接 33 DIY Electronics Online Tools and Guides for Makers 硬件设计与选型 3D建模、打印 ","date":"2025-11-08T00:00:00Z","permalink":"https://loveleaves.github.io/p/diy_intro/","title":"【DIY】电子DIY设计与实现"},{"content":"3D建模 常用软件 Fusion360 教程 2023年最新 Fusion 360 教程 fusion360教程 SolidWorks 3D打印 ","date":"2025-10-13T00:00:00Z","permalink":"https://loveleaves.github.io/p/3d_modeling/","title":"【建模】 3D建模、打印"},{"content":"基础知识 零基础入门PCB设计PPT 嘉立创EDA设计教程 原理图\u0026amp;PCB设计教程 PCB设计流程 硬件设计从需求到PCBA 专业pcb设计流程 资料准备 原理图：硬件工程师提供的原理图设计文件结构图 DXF文件：用于确认板框大小，是否存在限高、禁布的信息 设计说明：PCB Layout设计要求、设计问题记录表等信息 器件手册：进行封装绘制，以及推荐布局或走线参考 网表导入\u0026amp;结构导入 网表导入：将原理图网表导入到pcb中，确保原理图中的网络连接性、器件完全导入成功。 结构导入：将结构文件导入到pcb中，确保板框大小、结构器件信息、禁布区、限高区等信息，默认1：1导入，单位mm 模块抓取 按功能或者模块将器件进行抓取，确认主控的位置和方向，将各个模块根据飞线情况放置在主控芯片的周围。 立创EDA快捷键：ctrl+shift+x 器件布局和布局优化 器件布局：优先进行结构器件布局—特殊器件—主要器件，布局采取先大后小的原则，主要IC器件按飞线流向放置合适位置，滤波器件靠近滤波管脚。 布局优化：对预布局文件进行布局优化，器件中心对齐，大器件尽量对齐原则，相同模块必须一致，保证PCB布局合理、美观。 叠层设置和规则设置 叠层设置：确定每层对应的放置层标识 规则设置：常用规则（线宽、线距、过孔） PCB扇孔 通常对于引脚多的芯片：先扇孔，再走线。\nPCB走线和PCB修线 PCB走线：走线优先对重要信号走线，优先参考GND层走线。对重要信号线进行包地打孔处理，如差分信号，时钟信号。电源信号最后处理。 PCB修线：对走线进行优化调整、检查，不存在信号跨分 割，满足载流要求，回流路径短，不存在环形、U形路径。走线需美观，尽量耦合。 PCB等长\u0026amp;布线优化 PCB等长：对高速信号，有时序要求的信号线进行等长，3W原则。差分信号等长尽量在不耦合处等长。 布线优化：对整板走线进行优化，检查关键信号、电源信号载流，路径、过孔是否合理，让其走线更加美观、合理。 DRC检查\u0026amp;丝印处理\u0026amp;光绘输出 DRC检查：对整板进行DRC检查，检查信号是否全部连通， 是否存在开短路。检查是否未避开禁布区、限高区。检查规则优先级是否正确 丝印处理：进行丝印调整，丝印方向一致，丝印清楚，不 能上阻焊。座子丝印添加是否正确。版本号、L0G0是否添加。 光绘输出： ASM:装配图文件夹 CAM:Gerber文件夹，用于制版 SMT:贴片文件夹，用于贴片 PRJ:工程文件夹 XXX.docx:制版工艺说明文件 元器件选型 立创商城（也可大部分数据手册查看） 电子设计选型 pcb版一般包含以下模块：\n单片机最小系统：是指用最少的元件组成的单片机可以工作的系统。对于51系列单片机来说，单片机+晶振电路+复位电路,便组成了一个最小系统。晶振电路是单片机的振荡电路，通过晶振来放大输出信号。复位电路则用于重置单片机的系统状态，使其重新初始化。 电源电路：单片机最小系统中的电源是指能量的来源，其中VCC(40脚)将电源的正极和GND(20脚)视为电源的输入和输出。接地端(电源负极)则用于与地面形成电气连接。 外围功能电路：包括按键检测，Led指示灯，排针引出等 PCB布局要求 PCB设计布局指导 如何设计 PCB 布局 1、必须根据元器件的电气特性和使用特点来布局（这点非常重要），举例如下：对于各种接口、按键和排针，需要放在板子边缘，方便插接，对于屏幕和主控芯片等，一般放在板子中央，对于电源电路，一般放在板子的电源输入旁边并且要注意电流路径和滤波电容位置（非常重要），对于晶振需要靠近单片机晶振引脚摆放等 2、不要把元器件看成二维物体，而是应该看成三维物体，有时空间有干涉的情况需要考虑 3、元件的布局应该采用模块化，也就是同一个模块电路的元件应该放同一个区域，按照就近原则来布局，不能东一个西一个 PCB布线要求 PCB设计走线指导 【硬件设计】布线篇\u0026mdash;\u0026ndash;超实用的PCB布局布线规则 线宽、过孔大小如何计算 布线顺序： 关键元件优先、关键信号线优先、密度优先\n(1) 密度优先原则：从单板上连线最密集的区域开始布线。 (2) 优先关键元器件：如DDR、射频等核心部分应优先布线，类似信号传输线应提供专层、电源、地回路。其他次要信号要顾全整体，不可以和关键信号相抵触。 (3) 关键信号线优先：电源、模拟小信号、高速信号、时钟信号和同步信号等关键信号优先布线。 布线要求：\n1.顶层优先原则：尽量在顶层布线 2.电源线原则上要加粗：因为电源线是要给电路板各个模块供电的，电源线加粗有利于电流在主干道上流通；在日常PCB设计中，在25℃时，对于铜厚为10z(盎司)的导线，10mil线宽能够承载0.65A电流，40mil线宽能够承载2.3A电流。 3.同一层内走线大于90°：同一层走线禁止90°或者走锐角，从原理上讲，锐角直角走线会造成走线阻抗不连续，对于信号的传输有影响，推荐走线135° 4.注意电流路径和电容的摆放位置：电源要先经过电容滤波再给后级，去耦电容要贴近芯片引脚放置，并就近接地。 5.高频信号线尽可能短，并做好与其他信号的屏蔽隔离。为了降低相邻走线之间的串扰，尽量避免相邻层平行走线，走线应遵循3W原则：相邻层信号线应采用正交方向。差分线布线尽量等距等长。 6.PCB布线要尽量远离安装孔与电路板边缘：在PCB钻孔加工中，很容易会切掉一部分导线，为了电路板功能，应尽量远离这些位置。 7.需要添加泪滴。 原件符号绘制 对于元件库中没有的但需要使用的原件，进行元件符号绘制与封装。\nPCB设计常用知识点 PCB设计常用知识点\n电子设计常用知识点\n数据手册（Datasheet） 组成部分 元器件的数据手册一般包含以下部分：\n功能简述 电气特性 功能描述 外观与封装 应用示例电路 查阅网站 芯查查 立创商城 芯片封装类型 芯片封装类型简介\n选择芯片的封装影响到PCB的布局及布线，所以在元件选型时就应该考虑。\n芯片封装是半导体产业中重要的一环，它不仅保护着脆弱的半导体芯片，还承担着散热、电气连接和信号传输等多重功能。从最早的通孔封装（如DIP）到表面贴装封装（如QFP），再到区域阵列封装（如BGA）和晶圆级封装（如WLCSP），芯片封装技术向着小型化、高性能的方向发展。\n差分信号 单端信号是相对于差分信号而言的，单端输入指信号有一个参考端和一个信号端构成，参考端一般为地端。当耦合噪声时，接收数据无法正常还原。\n差分信号是一种信号传输技术，区别于传统的一根信号线一根地线的做法，差分传输在两根数据线上都传输信号，这两个信号的振幅相等，相位相差180°（即相位相反）。在这两根线上传输的信号就是差分信号，而承载差分信号的那一对走线就称为差分走线。\n差分信号的优点有哪些？ 1、抗干扰能力强 共模干扰抑制：差分信号对外界噪声具有很强的抗扰能力，因为两条信号线上的噪声是相同的（共模），接收端通过计算差值有效消除了共模噪声。 磁场耦合小：差分信号线之间的电流方向相反，产生的电磁场互相抵消，因此对外部设备的电磁干扰（EMI）小。 2、信号完整度高 抑制信号反射：两条线之间的特性阻抗匹配更容易实现，从而减少信号反射，保证信号完整性。 减少地回路问题：差分信号不完全依赖地作为电流的回流路径，减少了地回路的干扰，对于差分信号而言，最大的影响是对地阻抗是否一致，也就是对地平衡度，它们之间相对的阻抗影响并不特别重要，之间分布电容大了只会衰落信号强度，不会引入噪声和干扰，也就是对信噪比不会产生很大影响。 3、支持信号高速传输 差分信号在高速传输中表现优异，能够更好地保持波形，减少信号失真和抖动，适合用于高速数据总线（如PCIe、USB、HDMI等）。 4、电磁辐射低 差分信号的电流方向相反，形成的磁场互相抵消，大大降低了电磁辐射，符合更高的EMC（电磁兼容性）要求。 5、传输距离远 在相同的条件下，差分信号比单端信号能传输更长的距离，同时保持良好的信号质量。 差分信号在PCB中的设计要求有哪些？ 认识差分信号，在PCB设计中，差分信号的命名通常有“+、-、P、N”等标识 差分信号往往是速度较快、且在整个项目中比较重要的，在布局走线时要重点考虑，尽量保证差分信号顺畅以及距离短。 差分信号在走线时，非必要不换层，若一定要换层，在换层的附近添加回流地过孔 差分信号内不能有其余走线，若有匹配电阻或上拉电阻，也应该对称摆放。一般差分线间距较小，在电阻的选择上一般不大于0603，否则器件本身尺寸就会导致差分线耦合效果差，影响信号完整性。 差分信号在走线时，不可避免会有拐弯或打孔导致线长不一致，差分信号是接收两根信号的差值，需要保证相位的同步，同组差分信号一般不超过±5mil误差，在进行差分等长时，尽量满足小于3W间距（3倍线宽）以及小于2H（2倍间距）规则。 特性阻抗 特性阻抗是电磁波在传输线中传播时遇到的阻抗，反映了信号传输的质量和效率。在PCB设计中，控制特性阻抗对确保信号完整性和减少反射至关重要。\n特性阻抗（Z₀）是传输线上电压波与电流波的比值。\n计算公式为：\n其中：\nR 为单位长度电阻； L 为单位长度电感； G 为单位长度电导； C 为单位长度电容； ω 为角频率（ω=2πf，f是频率）； j 为虚数单位（j²=-1）； 1、单端阻抗 单端阻抗（Single-Ended Impedance）是指信号线与参考平面（通常是地平面或电源平面）之间的阻抗，适用于单端信号传输。\n常见的单端阻抗值有50Ω、75Ω等。 在高频下，单端特性阻抗可简化为：\n其中：\nL 是单位长度的电感； C 是单位长度的电容； 2、差分阻抗 差分阻抗（Differential Impedance）是指一对差分信号线之间的阻抗，适用于差分信号传输。差分阻抗可分为差模阻抗和共模阻抗，其中差模阻抗用于描述正负差分信号之间的阻抗，而共模阻抗则用于描述这两个信号与地之间的阻抗。常用于高速信号传输（如USB、HDMI、PCIe、以太网等）。\n常见的差分阻抗值有90Ω、100Ω等。 在高频下，差分特性阻抗可简化为：\n其中：\nZ₀是单端阻抗； k 是两条差分线之间的耦合系数（通常为0.1~0.3）； 3、阻抗控制作用 在PCB设计中，如果不控制阻抗，可能会对信号完整性、系统性能和可靠性产生严重影响。\n当信号在传输线中遇到阻抗不连续点时（如阻抗不匹配），部分信号会被反射回源端。 阻抗不匹配会增加信号之间的电磁耦合，导致相邻信号线之间的干扰。 阻抗不匹配会导致信号能量损失，信号衰减，可能使得信号在到达接收端时，无法被正确识别。 阻抗不匹配会影响信号的传播速度和时序，时序偏差错乱，导致数据错误，系统无法同步。 阻抗不匹配会引起时钟信号的反射和失真，时钟信号不稳定或抖动，会导致系统时序紊乱，降低系统性能。 阻抗不匹配会影响电源分布网络，导致信号能量通过电源或地线传导，增加电源噪声，影响系统稳定性。 阻抗匹配 阻抗匹配是指在电路中，将负载的阻抗与信号源的输出阻抗调整为相等或接近相等，以实现信号能量的最大传输或减少信号反射的过程。\n造成阻抗不匹配的原因 电路设计不当，比如导线时宽时窄，或GND平面不完整。 器件特性不一致，走线以及PCB板会带有寄生电容、电感、电阻等，导致实际阻抗偏离 系统参数变化，比如从外接线缆到PCB板，器件本身材质不一致导致阻抗不一致。 阻抗不匹配的后果 在高频电路中，当传输线与发送端阻抗不匹配时，会产生反射、振荡、过充等现象，并叠加至原本的信号被接收端收到，此时可能会导致接收端数据异常。 在模拟信号中，若阻抗不匹配，同样容易造成电压抖动、过充等现象，导致ADC转换数据抖动或异常。 如何计算阻抗 影响阻抗的因素有很多，导线宽度、PCB板材、PCB层叠、GND平面等等，在进行阻抗匹配计算时，通常会使用专门的工具来进行计算。比如：嘉立创阻抗计算神器\n根据输入参数的不同，计算的结果也不相同，在进行阻抗计算时，首先需要清楚所需的阻抗，其次是传输线的传输方式（差分、单端），根据参考平面与阻抗线所在平面的区别分为共面或隔层参考。（共面参考主要影响是阻抗线到参考平面的距离、隔层参考主要影响是参考层与阻抗层的层叠）。\n阻抗计算结果可能不符合实际生产设计需要，此时需要不断调整相关参数，选择一个价格、设计折中的方案。\n常见高速信号阻抗计算 USB传输线阻抗要求：差分90欧姆，差分对内长度误差≤5mil；传输线长度尽量不超过1800mil； 网口传输线阻抗要求：差分100欧姆，差分对内长度误差≤5mil；传输线长度尽量不超过1500mil； HDMI传输线阻抗要求：差分100欧姆，单端50欧姆，差分对内长度误差≤5mil；传输线长度尽量不超过500mil； 射频天线阻抗要求：单端50欧姆，走线周围要有良好的接地平面，尽可能保证射频信号区域内净空无其他干扰。 PCB 层压结构 PCB叠层的基本组成 PCB 叠层（PCB Stack-up）通常由导电铜箔（信号层和电源/地层）和绝缘材料（介质层）交替组成。合理的叠层设计对PCB的电气特性、物理性能、机械性能、可靠稳定性和制造成本等至关重要。\n叠层设置的基本原则 主芯片相邻层为地平面，在布线时提供良好的参考地平面； 信号层尽量避免直接相邻，以减少串扰； 所有信号层最好与地平面相邻，以保证完整信号的回流； 主电源尽可能与地平面相邻，降低电源平面阻抗； 常见的叠层结构 以下是一些典型的PCB叠层结构，实际设计可根据实际应用情况进行调整。\n双层板：包含两层导电铜箔层，中间是绝缘介电层。 四层板：包含四层导电铜箔层，每两层铜箔中间夹一层绝缘介电层。通常在靠近元器件多的内层里铺一层完整的GND。 六层板及以上：增加了更多的导电铜箔层，每两层铜箔中间夹一层绝缘介电层，提供更好的电源分配和信号隔离。 3W \u0026amp; 20H 规则 高速板中3W规则和20H规则说明 高速设计中 3W 规则指的是什么？ 3W 规则是指：信号线的中心间距不少于 3 倍线宽时，则可保证 70%的电场不互相干扰，称为 3W 规则。使用原因是一般信号线间距足够大时，可以减少信号线之间的串扰。当满足 2W 间距时，可保证 50%的电场不互相干扰。如果要达到 98%的电场不互相干扰，则需使用 10W 间距。 注意 3W 指的是俩根信号线的中心距。例：嘉立创 EDA 软件规则设计中，例如：导线是 5mil，对应 3W 间距，线与线之间的中心距是 15mil,这里安全间 距也就是线与线之间最近的边缘间距是 10mil。图片所示均为中心间距。 在 PCB 设计中一般时钟线，差分线，视频信号线，音频信号线，复位信号等需满足 3W 规则；普通的信号线一般满足 2W 规则即可。 为什么要做 3W 处理？ 可以减少串扰：满足3W原则能使信号间的串扰减少70%。串扰是信号线之间的电场和磁场相互作用而产生的干扰，它会影响信号的完整性。通过增大线间距，可以降低这种干扰从而提高信号质量。 提高信号完整性：减少串扰有助于保持信号的完整性，降低噪声对信号的影响。这对于高速信号传输尤为重要，因为高速信号对噪声和干扰更为敏感。 优化PCB布局：遵循3W规则有助于优化PCB布局，使得信号线之间的距离更加合理，从而提高PCB的可靠性和稳定性。 高速板中常说的20H指的是什么？ 20H规则：将电源层相对于地层内缩，使电场只在接地层的范围内传导。其中，一个H（电源和地之间的介质厚度）为单位，内缩20H可将70%的电场限制在接地层边沿内。若内缩100H则可将98%的电场限制在内。 内缩原因是电源层和地层之间的电场是变化的，在板子的边缘会向外辐射电磁干扰，将电源层内缩，可以让电场只在接地层的范围内传导，有效提高了EMC。 一般，在PCB设计时把电源层比地层内缩1mm，或者必须≥20mil，优先40mil，基本就可以满足20H的原则。 PCB 铺铜 PCB铺铜是指在电路板上未布线的空白区域铺设一层铜箔（通常称为“铜皮”）的操作，目的是：\n为了减小地线阻抗，提高抗干扰能力； 降低压降，提高电源效率； 与地线相连，还可以减小环路面积； 提高散热性能及抗干扰能力； 增强板材的机械强度和稳定性等。 数字电路中存在大量尖峰脉冲电流，因此降低地线阻抗显得更有必要，普遍认为：\n对于全由数字器件组成的电路，应该大面积铺地； 但对于一些模拟电路，铺铜所形成的地线环路，反而会引起电磁耦合干扰得不偿失。因此，不是所有电路都需要铺铜操作的。 铺铜的连接形式 全填充铺铜：正常的实体铺铜填充样式，可以承载较大的电流，适用于需要大电流的电路‌。例如，在电源模块、地线连接等需要稳定电流和电磁屏蔽的场合，实体铺铜能够提供更好的性能表现‌。实体铺铜在过波峰焊时，由于热胀冷缩的拉力，可能会导致铜箔起泡或板子翘起‌。 网格铺铜：主要起到屏蔽作用，网格铺铜可以减少铜箔的使用量，从而降低制造成本‌，有助于提升散热性能，特别适用于需要良好散热的场合‌。由于铜箔呈网格状分布，其导电性能相对全填充会有所降低，适用于对导电性能要求不高的场合‌。 铺铜的注意事项 避免出现孤岛/游离/悬空铜，如果铺铜区域未连接到电源、信号或地，出现孤岛铜，会形成天线效应，引入干扰信号。 GND铺铜的尺寸应该足够覆盖整个电路板，并且尽可能均匀分布电流负载，使得PCB的散热分布均匀。 高多层板，为确保信号完整性，需要有完整的GND参考平面铜，多层铺铜可以提供更好的信号完整性。 在天线等高频信号区域，铺铜容易导致信号弱，容易受到干扰，铺铜的阻抗会影响到放大电路的性能，一般不会铺铜或者及放置禁止铺铜区域净空处理。 GND过孔 PCB设计时板边为什么要打地过孔？\n在PCB设计中，板边打地过孔是一种常见做法，目的是为了抑制电磁干扰（EMI）、提供接地路径、屏蔽与抗干扰、增强PCB的机械强度、增强PCB板的散热能力。这种设计方法对电路的整体性能和元件的稳定性有显著影响，具体原因如下：\n抑制电磁干扰（EMI） 板边的地过孔形成屏蔽，抑制电磁辐射的散射，有效减少电磁干扰（EMI），保证PCB的电磁兼容性。\n提供接地回流路径 地过孔为电路中的接地信号提供了一个直接的路径，有助于降低接地阻抗，确保电路中的接地信号能够稳定、高效地传递到地层，这有助于改善电路的整体性能，特别是在高频和高速电路中，降低接地阻抗能够减少信号干扰和噪声，提高信号的完整性和稳定性。\n屏蔽与抗干扰 地过孔在板边形成连续的接地网络，能够阻挡或减弱外部电磁场对电路内部信号的干扰。同时，它们还能将电路内部的干扰信号引导至地层，防止其干扰其他电路部分。\n增强 PCB 的机械强度 在 PCB 板边打地过孔还可以增强电路板各层之间的电气连接，使电路更加稳定可靠，避免在插拔、震动等情况下对电路造成机械损坏。\n增强 PCB 板的散热能力 板边打地过孔的设计不仅能提升 PCB 的电磁兼容性和机械强度，还能提供额外的散热路径，有效缓解高功率和高频电路的过热问题，提升电路的热稳定性和使用寿命。\n回流路径 回流路径 回流路径（return current path）设计是PCB设计中的核心问题之一，涉及信号完整性、电源完整性和EMI性能。\n回流路径是电流从负载返回电源的路径。当信号从驱动器传输到接收器时，电流会通过信号线流向负载，然后通过地平面、电源平面或其他导体返回电源，形成完整的回路。回流路径并不是“地”的专属概念，它可以是地平面、电源平面，甚至是相邻的信号线（例如差分信号的回流路径）。\n回流类型 地回流路径：电流通过地平面返回电源，适用于单端信号传输，地平面通常具有较低的阻抗，能够提供稳定的回流路径，需注意地平面完整，避免分割导致回流路径不连续。 电源回流路径：电流通过电源平面返回电源，适用于差分信号或某些高速信号，电源平面和地平面之间可以通过去耦电容形成高频回流路径，需要注意电源平面的噪声问题。 设计原则 最小回流路径：回流路径应尽可能短，以减小环路面积，降低辐射和感应噪声。 连续平面：尽量使用连续的地平面或电源平面，避免分割平面导致回流路径不连续。 避免跨分割：信号线不应跨越平面分割区，否则回流路径会绕行，增加环路面积和EMI。 差分信号：差分信号的回流路径相互抵消，能有效减少EMI。 跨平面分割 在PCB设计中，参考平面可能会被分割（例如模拟地和数字地的分割），此时，信号线跨越参考平面的分割区域，会导致回流路径不连续，回流路径被迫绕行，路径变长，环路面积增大，可能产生辐射噪声。\n解决方案：\n尽量避免参考平面的分割。 如果必须分割，确保信号线不跨越分割区域。 高速信号在分割区域附近放置桥接电容，一般采用100nF，不可使用0Ω电阻。在高频下，电容的阻抗非常低，能够有效传递高频信号。为高频回流电流提供低阻抗路径。 两层板设计规范 两层板设计规范\n对于两层板的设计，两层板的设计难点在于没有完整的GND参考平面和没完整的电源平面，对于电源和地的处理，在两层板当中尤为重要，其中高密度两层板的布线难度也相对较大，需要在有限的布线空间内完成所有信号的连接，同时还得保证信号的质量和完整性；那么如何去设计两层板，可以根据两层板设计规范去设计。\n布局设计 布局建议是进行单面布局，这样子可以在设计合理的情况下更好的节省成本而且没有层间信号干扰的问题，这有助于提高信号的稳定性和可靠性。如果不能保证，最起码主要的元器件要和主控芯片放在同一面，合理布局可以方便走线不拥挤，而且信号走线间距保持等距减少线间串扰。\n布线设计 优先考虑关键信号的布线，确保它们有最短、最直接的路径。 顶层尽量去处理大部分的信号线，能直接连接的条件下，就进行网络顶层直连。 合理利用顶层和底层的空间进行布线，必要时可以通过打孔来实现层间的连接。 对于关键信号可进行包地线处理，以提高关键信号的抗干扰能力。 电源设计 两层板的电源布线，建议是在顶层去进行布线，能在顶层处理完成最好，如不能满足，也可以通过打孔走线在地层，要注意避免大面积分割问题，尽量保持底层平面完整。 电源线应尽可能宽以满足电源载流，减少电源波动对电路的影响。 避免电源线与信号线交叉，电源线和敏感信号线要保持一定的安全间距，必要时候可以适当包地隔离处理，以减少干扰。 地层设计 底层一般是用于处理地网络，底层进行地铜全铺，提供一个完整的地参考平面有助于确保信号能够顺畅地回流，减少电磁干扰。 注意地平面跨分割免问题，因为分割会破坏回流路径的连续性，导致信号回流时产生不必要的绕行，增加电磁辐射，如不能保证平面跨分割问题，可以在跨分割的区域，使用地孔来连接分割的地层，确保回流路径的连续性。 顶层空白区域也可以进行一个铺地铜进行处理，同时打孔将顶层和底层的地网络连接起来，为信号提供一个更直接的回流路径。 两层板如何控制阻抗 两层板如何控制阻抗\n在 PCB 设计当中，两层板控阻抗是一个具有难度的问题，因为和多层板去相比，两层板没有一个专门的参考平面层构成地返回路径，那么两层板应该如何去控制阻抗？以下是关键的控制方法和控制阻抗验证：\n1、共面阻抗法（包地法） 设计方式：由于两层板没有专门的参考平面，因此需要人为地构造一个电流返回路径，即共面阻抗。这通常通过“包地”的方式实现，即在信号线的周围加上地线，以形成共面结构。 走线宽度：走线宽度对阻抗有直接影响。在两层板上，为了获得所需的阻抗值，可能需要调整走线的宽度。一般来说，走线越宽，阻抗越小；走线越窄，阻抗越大。但需要注意的是，走线宽度过窄可能会导致加工难度增加和可靠性降低。 包地间距：包地间距是指信号线与地线之间的距离。这个距离对阻抗也有重要影响。包地间距越小，分布电容越小，阻抗越大。因此，在设计时需要仔细调整包地间距以获得所需的阻抗值。 保持连贯性：为了保证阻抗的均匀性，包地应该保持连贯，不应出现断裂或不规则的情况。这有助于减少信号的反射和失真。 2、阻抗控制和验证 获取材料参数：与 PCB 厂家合作，获取实际使用的电路板材料的详细参数。这些参数对于阻抗计算至关重要，因为它们直接影响阻抗值的计算结果。 生产工艺咨询：了解厂家的生产工艺和限制，以便在设计时考虑到这些因素。例如，某些生产工艺可能会对阻抗值产生影响，需要在设计时进行补偿或调整。 阻抗测量与验证：在生产前，可以要求厂家进行阻抗测量，以确保设计的阻抗值与实际生产的电路板相符。这有助于及时发现并解决问题，提高电路板的可靠性和性能。 PCB设计中电流与线宽的关系 PCB设计中电流与线宽的关系 在 PCB 设计里，电流和线宽的关联十分紧密。通常情况下，线宽越宽，其能够承载的电流也就越大。不过，这个关系并非简单的线性关系，还会受到铜箔厚度、环境温度以及允许温升等诸多因素的综合作用。\n线宽增加 增加线宽，电流承载能力会有一定程度的提升。但这种提升并非呈线性，而是遵循类似指数的规律。比如，线宽从 1mm 增加到 2mm，电流承载能力大概会提高 40%，而不是翻倍。\n铜箔厚度 铜箔厚度一般用盎司（oz）来表示，常见的有1盎司（厚度约35um）、1.5盎司（厚度约50um）、2盎司（厚度约70um），在相同线宽条件下，铜箔越厚，载流能力越强，但加厚铜箔会导致成本上升较大，当电流较大且线宽加宽不了的情况时，一般常见的处理是在PCB顶底层共同处理电源或在表面开窗镀锡操作。对于多层板电源处理，需要注意默认内层铜厚仅为0.5盎司（约为17.5um），若需要在内层进行电源处理，需要保证较大面积的接触或增加铜厚。\n温升限制 导体温升（高出环境温度的温度增量）是影响PCB导体载流能力的决定性因素，PCB温升与导线电流、走线宽度、走线厚度、PCB板材、相邻走线、层间距离、有无涂层、环境条件等诸多因素的关系。相同条件下，电流越大，温升越高，载流能力下降（温度升高导致导线内阻增加）；加宽导线/铜箔变厚/改变板材可以带来更好的散热与载流能力。\n可前往嘉立创EDA微信公众号/实用工具/电子硬件助手/PCB走线宽度计算器进行相关计算，通常实际值会将其增加1.3被进行应用。\n过孔与电流的关系 在PCB设计中，连接顶底层的方式通常是使用过孔来进行，过孔内壁镀铜支持电流传输；过孔内径越大，单个过孔传输电流越大，常用过孔20/10mil（0.8A电流）、24/12mil（1A）、32/16mil（1.3A）；注意，过孔越大，占用空间越大，对平面分割越严重。\n同时，过孔传输电流,与导线一致，不是倍数递增，且电流传输并不是均匀分配到每一个过孔，比如2个24/12mil过孔理论上传输2A没问题，实际可能一个过孔走了1.4A，1个过孔只走了0.6A，这导致部分过孔存在载流风险，所以通常会增加过孔数量且均匀分布。\nPCB设计中的安全间距 ‌ 在PCB设计中，考虑到可制造性、电气安全性、可使用性等方面，会有各类间距要求需要遵守；包括导线到导线、导线到焊盘、过孔到导线、过孔到焊盘、电源爬电距离、铜皮到板框距离等等；\n‌ 下方是嘉立创EDA设计规则中默认的安全间距要求；在日常器件不密、板框较大的场合，通常都不需要修改规则；但当板框较小、器件较多且密集的适合，默认的间距规则就会成为设计阻碍，导致拉不出来线、产生drc报错。此时，可以尝试减少设计规则中安全间距、导线默认线宽等规则。注意，此时的修改一定是要在板厂的生产要求内，且留有一定余量，否则最终设计出来不能生产或次品率过高。\n常规PCB通常是2层、4层且铜厚1oz居多，此时最小线宽线距为4mil或3.5mil；注意，这里是“最小”，在实际项目设计过程中，不应挑战工艺极限，通常≤6mil；已经足够满足绝大多少项目设计需求了； 常规PCB设计中，常用过孔孔径有，8/16mil，10/20mil，12/24mil，11.8/19.7mil，13.8/27.6mil，23.6/39.4mil；在实际项目实际过程中，不建议随意改变过孔尺寸，因为工厂钻头都是有固定大小的，当设计尺寸不符合实际钻头尺寸时，切割就会有误差，可能影响不良率；同时免费打样过孔应大于11.8/17.7mil。 20H规则：将电源层相对于地层内缩，使电场只只在接地层的范围内传导。其中，一个 H（电源和地之间的介质厚度）为单位，内缩 20H 可将 70%的电场 限制在接地层边沿内。若内缩 100H 则可将 98%的电场限制在内。一般，在 PCB 设计时把电源层比地层内缩 1mm，或者必须≥20mil，优先 40mil，基本就可以满足 20H 的原则。当然，这是建立在内缩后GND平面仍然完整的前提下，若内缩后导致GND平面不完成或有割裂，则需减少内缩距离。 PCB常见丝印标识 PCB常见丝印标识\n1 元件位号标识 ANT：天线（Antenna）； B ：电池（Battery）； BT：蓝牙模块（Bluetooth）； BZ：蜂鸣器（Buzzer）； C : 电容（Capacitor）； D : 二极管（Diode）； DS：显示器件（Display）; F : 保险丝（Fuse）； FB：磁珠滤波器（Ferrite Bead）； FS：快速熔断保险丝（Fast Blow Fuse）; H : 排针排母（Header）； IR：红外二极管（Infared Diode）； JP：跳线（Jumper）； K : 继电器（Relay）； KEY：按键（Key）； L ：电感（Inductance）； M ：电机（Motor）； MIC：麦克风（Microphone）； NTC: 负温度系数热敏电阻（Negative Temperature Coefficient Thermistor）; LDR：光敏电阻（Light Dependent Resistor）； LED：发光二极管（Light Emitting Diode）； Q ：晶体管/三极管/场效应管（Transistor）； R : 电阻（Resistor）； RP：电位器/可调电阻； RN：排阻（Resistor Network）； RT：热敏电阻（Thermistor）； S ：开关（Switch）； T ：变压器（Transformer）； TP：测试点（Test Point）； U ：集成电路（IC）； USB：USB接口； X : 晶振（Crystal Oscillator）； ZD：稳压二极管/齐纳二极管（Zener Diode） 2 常见认证标识 3 常见符号标识 PCB开尔文走线 ‌开尔文走线（Kelvin Connection）是一种用于高精度测量低电阻的布线技术，通过分离电流传输路径和电压检测路径，消除引线电阻和接触电阻的影响，实现毫欧级甚至微欧级电阻的精确测量。\n‌ 首先，先了解采样电阻（也称为电流检测电阻），是一种专门用于将电流信号转换为可测量电压信号的低阻值精密电阻。它是电流测量电路中的核心元件，串联在待测电流路径中，通过测量电阻两端的压降（根据欧姆定律 U=I×R），间接计算出流经电路的电流值，采样电阻的阻值一般不会超过1Ω，典型范围在0.1mΩ ~ 100mΩ（毫欧级），精度比较高，一般在1%以内。\n‌ 在测量电流时，电流检测技术分为高侧（边）检测和低侧（边）检测。将测量采样电阻放置在电源与负载之间的检测方法称为高侧检测，将测量采样电阻放置在负载与接地端之间的检测方法称为低侧检测。这两种用于检测负载中电流的方法，如图1所示。\n图1 高/低边电流检测采样 ‌ 在普通的两线测量法中，测量电流和测量电压共用同一对引线，引线本身的电阻会叠加在被测器件的电阻上，导致测量值偏大。通过开尔文走线（四线检测法）是提高测量精度的关键，主干道用粗线或宽铜箔，大部分电流都是从主干道流过，电流测试支路用细线，测量通过采样电阻两端的压降，计算出流过的电流，显著提高采样电阻的采样精度。\n在电路板设计中，开尔文走线需遵循以下规则：\n采样电阻焊盘分离电流焊盘区和电压检测焊盘区，电流路径使用宽铜箔，电压测量线用细线连接；\n两条电压线应尽可能长度相等、线宽相同、保持对称布线，并远离噪声源，最好使用差分对布线规则，这有助于抑制共模噪声；\n采样电阻到电流检测芯片的布线长度应尽可能近，以减少测量误差；\n电压线从电阻焊盘中心直接引出，避开电流路径分支点，远离功率元件、大电流走线等热源，温度变化会影响测量精度；\n图2 开尔文走线接法 图3 开尔文走线接法（双通道） PCB常见模块设计参考 PCB常见模块设计参考\nLDO模块 LDO（low dropout regulator，低压差线性稳压器）。\n注意点：AMS1117一般输出电容使用钽电容而非陶瓷电容。 常用款型：1117（如AMS1117）、除了1117，还有哪些更好用的线性稳压器？ 优点： LDO外围器件少，电路简单，成本低，通常只需要加一两个旁路电容 LDO负载响应快，输出纹波小，噪声小 缺点： 效率低，输入输出压差不能太大 体积大、不支持mlcc、只能降压 静态电流过高、发热温度高 布局参考 LDO虽然电路结构简单，但其效率不高，工作时输入输出的电压差都被转换成热能消耗了。所以在进行LDO布局时需考虑散热问题，部分芯片还可添加散热片辅助散热。输入/输出电容尽量靠近输入/输出引脚摆放，滤波电容从大到小依次摆放，LDO两边的电容数量需要保持一致，这样才能保证电源的输入/输出端流入地的电流平衡，布局要点如下：\n按照电源信号的输入/输出路径，布局时按一字型或者L型摆放； 电容按先大后小顺序摆放，就近输入/输出管脚； 输入/输出电容GND引脚尽量朝一个方向，保持GND方向一致，减少回流路径。 走线参考 LDO走线时由于电路连接比较简单，走线时沿着信号方向引出即可，一般只需要考虑线宽是否能够承载整个系统的电流即可。在单面板设计时保持GND信号的完整性，输入输出部分可直接用铜皮填充；当使用双面板以及多层板设计时，需在GND焊盘附近整齐放置一些GND过孔。注意事项如下：\n电源输入输出信号可直接全填充或粗导线连接，确保铜皮宽度能够过系统最大电流； 走线尽可能直，避免不必要拐弯，必须拐弯时走钝角或圆弧； 走线时根据信号流向，输入信号先经过电容再到芯片，输出走线也需先过电容再输出； 双多层板设计时在加一些整齐统一的过孔保证各层间GND的连接； 走线后在板子上可根据电路需要添加必要丝印信息提示，避免焊错或接线错误。 DC-DC模块 DC/DC转换器一种是开关电源稳压器，指利用电容、电感的储能的特性，通过可控开关（MOSFET等）进行高频开关的动作，将输入的电能储存在电容（感）里，当开关断开时，电能再释放给负载，提供能量。\nDC-DC转换器（非隔离式DC-DC）根据其功能可分为三种基本拓扑结构：\n降压转换器(Buck Converter)：当输入电压高于所需输出电压时使用 升压转换器(Boost Converter)：当输入电压低于所需输出电压时使用 升降压转换器(Buck-Boost Converter)：当输入电压可能高于或低于输出电压时使用 优缺点：\n缺点： 外围器件多，电路复杂，成本高 负载响应比LDO慢，输出纹波大，噪声大 优点： 效率高，输入电压范围宽泛 支持降压和升压 输出电流高，功率大 布局参考 PCB的良好布局对DCDC电源非常至关重要，他能直接影响到产品的稳定性和转换的效果。总结规则如下：\n旁路去耦电容靠近输入/输出端摆放（如图4-15） 从数据手册中看出PH引脚是电源IC的开关节点，那么电感和环流二极管应尽量靠近PH引脚摆放，尽可能的缩小PCB的导体面积，防止电容过度耦合和减小电流环路面积（如图4-16所示） 在电源整体布局时，尽量横平竖直的摆放，不要将器件摆放的过于杂乱，避免增加电源路径（如图4-17所示） DC-DC电源芯片在工作时会产生一些热量，所以布局时，应提前注意是否有对热源敏感的器件，避免影响到其他器件的工作（如图4-19所示） 走线参考 DC-DC模块常用于大电流使用，且走线对其效果影响较大，注意事项如下：\n输入和输出的电源走线一定要计算好走线宽度，大电流/电压可以使用铺铜方式或者全填充 方式进行连接； 所有的走线尽可能的短和直，减少电源路径 GND焊盘使用铺铜或全连接连接，输入输出GND尽可能位于一块铜皮上，同时在底层铺铜（注意避开电感区域），在GND焊盘处打上过孔进行连通，缩短电源的回流路径； DC-DC反馈走线不能直接走在电感、二极管、大电容、IC芯片散热焊盘下面，也不能被大电流环包围，反馈线不是电源走线，不需要加粗，正常信号线宽度（10mil/15mil）即可）； 如果芯片下方有热焊盘，则需要使用多个过孔连接到底层，增强散热性 电感器件底部尽量不走线，避免电感产生的电磁信号影响到信号线的传输； USB接口 USB接口PCB设计全攻略\n为什么Type-c接口要加5.1K电阻下拉\nCC引脚不使用时，一般连接1个5.1K下拉电阻接地。\n晶振 布局参考 晶振电路布局需要优先考虑，整体紧凑摆放，晶振尽量靠近 IC，负载电容放置于晶振与 IC 之间，以减少时钟信号传输的延迟和干扰； 晶振摆放尽量远离板边和接口器件，减少其受外部物理因素干扰的风险，如物理撞击等； 晶振需远离干扰源，如电感、大功率驱动器、RF天线等； 晶振区域的底层不可放置其余器件，需保证晶振区域的净空； 布线参考 晶振电路的时钟信号走线越短越好，可以按照类差分走线，晶振的时钟走线不可打过孔走线连接； 在晶振走线周围通过GND过孔进行包地，每隔50-100mil间距整齐放置屏蔽地过孔，用以隔离吸收晶振辐射的噪声； 晶振区域同层需要净空处理，可以使用禁止铺铜区域进行隔离，晶振本体相邻层最好不要走线，保证地的完整性； USB 接口 实际项目参考：基于VL813的USB3.0-HUB设计 布局参考 USB接口应该靠近板边或按照结构定位摆放，方便插拔； USB输入接口和输出接口分开放置，便于分区和使用； 端接匹配电阻、ESD、共模电感、阻容器件靠近USB接口摆放； USB接口远离RF天线、摄像头等高电磁辐射EMI源摆放； 在布局时，尽量使差分线路最短，以缩短差分线距离； 布线参考 USB走线优先权高于其它器件和信号，优先考虑对高速USB差分（D+/D-、RX/TX）的布线； USB要走差分，阻抗控制为90Ω，并包地处理，包地线与差分线距离应大于20mil，每隔一段距离，打上一个回流地过孔，总长度最好不要超过1800mil，尽量缩短走线长度； USB的差分对需要做等长处理，长度一旦相差太多，将会影响时序偏差，引起共模干扰，降低信号质量，USB组内等长误差建议控制在±5mil以内； 差分对尽量少换层打过孔，过孔会造成阻抗改变和信号反射问题，若无法避免使用过孔，需要在打孔换层处加一对回流地过孔，用于信号回流换层，过孔建议不超过2个； 如果使用了外部终端电阻匹配阻抗时，请确保端接匹配电阻与主控制器信号输出引脚之间的距离小于200mil，以便更好地控制阻抗，避免信号反射； USB差分信号布线时，应远离板边或铺地边缘，至少保证90mil以上距离；远离电源网络、大电流信号、DDR、HDMI等高速信号，保证至少50mil的距离，以减小串扰； 电子设计常用知识点 0Ω电阻的功能 0Ω电阻又称跨接电阻，是一种标称阻值为0的特殊电阻，实际阻值通常在10-50mΩ（毫欧）之间，下面介绍0Ω电阻的用法和功能。\n1 方便调试与兼容设计 当电路引脚功能不确定时，可以通过0Ω电阻临时连接不同模块，待调试后确定出最终方案。\n2 参数匹配 在不确定电路参数时，用0Ω电阻作为过渡元件，待调试后更换为实际电阻值。 ‌\n3 模数电路单点接地 在模拟电路和数字电路等混合电路中，往往要求地分开，避免信号干扰，不同地线可以通过0Ω电阻单点连接在一起。\n4 跨接跳线 ‌ 在PCB布线时，可能会出现布线走不通的情况，如铝基板单层布线，不允许打过孔，可以使用0Ω电阻充当跳线进行连接。\n5 保护与熔断 ‌ 作为低成本熔丝使用，当电路过流时，优先熔断0Ω电阻，以保护其他元件。 ‌\n上拉电阻和下拉电阻 上拉电阻：将一个不确定的信号，通过一个电阻与电源VCC相连，固定在高电平。作用：上拉是对器件注入电流；灌电流；当一个接有上拉电阻的IO端口设置为输入状态时，它的常态为高电平。 下拉电阻：将一个不确定的信号，通过一个电阻与地GND相连，固定在低电平。作用：下拉是从器件输出电流；拉电流。当一个接有下拉电阻的IO端口设置为输入状态时，它的常态为低电平。 上拉电阻和下拉电阻2者共同的作用是：避免电压的“悬浮”，造成电路的不稳定。\n上拉（Pull Up ）或下拉（Pull Down）电阻（两者统称为“拉电阻”）**最基本的作用是：将状态不确定的信号线通过一个电阻将其箝位至高电平（上拉）或低电平（下拉）。**无论它的具体用法如何，这个基本的作用都是相同的，只是在不同应用场合中会对电阻的阻值要求有所不同，从而也引出了诸多新的概念。\n芯片IO口为什么需要接上拉电阻？ P0为什么需要上拉\nMCU的IO引脚在处于输出模式时，在硬件层面通常可以分为两种输出方式：\n推挽输出/推拉输出（push pull）： 开漏输出： 可以看到，但使用开漏输出时，无法输出高电平状态，可以在引脚上接一个上拉电阻，这样开漏输出的引脚就可以输出高电平了。\n为什么Type-c接口要加5.1K电阻下拉 usb official docs\nUSB是主从模式的结构，设备与设备之间、主机与主机之间是不能互联的，所有的数据传输都是由主机发起，而设备只能被动的负责应答，在USB-Typec接口中，是没有ID引脚来标识当前是主机或设备的，此时CC引脚就可以充当一个检测作用了。\n在主机和设备连接上后，主机的cc引脚检测到设备CC引脚的下拉电阻，表示接入到设备，此时主机可以打开Vbus的FET开关，输出Vbus电源给设备。\n事实上，CC引脚的作用远不止于此，在usb协议规范中指出，cc引脚用于连接检测、接口配置与Vconn功能；\nCC引脚的简要概述中指出，cc引脚主要用于一下目的：\n检测USB端口的连接，例如源端到接收端的连接； 解决电缆方向和扭转连接问题，以建立USB数据总线路由 在两个连接端口之间建立数据角色 发现并配置Vbus：USB Type-C电流模式或USB Power Deliver 配置Vconn 发现并配置可选的备用和辅助模式； 由于USB-Typec接口指出正反插，对于USB2.0标准，主机和设备接口的两组USB_DP/DM信需要各种短接以实现这一功能；在USB3.2超高速或USB4双通道传输方案中，主机和设备需要配置对应的传输通道，需要自行解决通道顺序问题，此时将单个CC引脚和1组TX/RX进行匹配，通过检测cc引脚的方向来实现正反插功能，从而判断使用哪组tx/rx信号进行传输，如果是双通道，也同样可以根据cc引脚判断通道序号，为0/1通道。\n为什么IC电源引脚旁边的电容的作用、选择和放置 芯片IC每个电源引脚旁边的电容的作用、选择和放置？\n在电源输入和IC引脚处，一般会加两个比如10uF和0.1uF的电容，进行滤波，大电容靠近电源，小电容靠近元件。一般不会直接放置一个电容，因为对于实际电容来说，它有等效串联电阻（ESR）和等效串联电感（ESL），从频率响应曲线来看，呈现V子形。一般电容越小，能过滤的频率越大，反之越小。\n电容尽量放到靠近IC引脚处，并且要经过电容再到IC 引脚，而且电源分支尽量在进电容前进行，因为如果摆放很远的话，电容滤除噪声后的电源会在这个段路径上又串扰进新的噪声，那么这个电容的作用就没有太大的意义。\nADC分压电阻的选择 \u0026amp;\u0026amp; ADC电量检测电路 ADC电量检测电路\nADC是模拟到数字转换器（Analog-to-Digital Converter）缩写，主要用于将连续传输的模拟信号转换为数字信号，便于数字系统（如中央处理器CPU、微控制器MCU等）对传输信息进行快速处理和分析。\n在实际使用过程中，adc分压检测常用于检测外部输入电压的变化，用于将一个外部电压分压到0~3.3V被adc采集到，一般用于低功耗设备或对电压敏感设备中，此时会有精度以及耗散电流的要求。虽然电阻分压简单易懂，但事实上，外部的输入电阻的选择会直接影响采样的精度和耗散电流的大小，它是搭配内部的adc电路一起工作的，这就要求我们需要对其adc检测原理进行一定的了解。\nADC 转换包括采样、保持、量化、编码四个步骤。采样阶段需要在规定的采样时间内将外部信号的电压完整无误的采样到 ADC 的采样电容上，即在采样开关 SW 关闭的过程中，外部输入信号通过外部的输入电阻 RAIN 和以及 ADC 采样电阻 Radc 对采样电容 Cadc 充电；每次采样过程可以简化为外部信号通过输入阻抗以及采样电阻对采样电容的充电。\n从上方的原理分析中可知，adc采集电路本质上是一个rc延迟电路，我们需要在使信号在规定的时间内被达到一个平稳的值。其中内部adc电阻以及内部adc电容是固定不变的，在数据手册中都能找到，那我们能改变的就只有外部的电阻值，也就是输入阻抗了。\n蓝牙模块 蓝牙模块\u0026amp;蓝牙串口小程序 使用教程\n元器件选型知识点 PCBA电子工程师必须要知道的元器件选型完全指南 电子元器件+模拟电路硬件零基础入门\n刚入门时应优先考虑：\n封装：封装与pcb板相关，封装不对会导致无法焊接使用 电气属性：比如电容等支持最大电压，如果设计的电路超过选型的电容，会导致击穿风险。 性价比和可重用：满足目前pcb的所需功能，并考虑后续可能的扩展复用。 电阻 电阻类型 固定电阻器：通过内部材料的特性来设定阻值，适用于稳定电流的场合。 可变电阻器（如电位器）：可调节电阻值，用于调节电路中的电流和电压。 热敏电阻（NTC、PTC）：其阻值随温度变化，广泛用于温度补偿与电流保护。 光敏电阻：其阻值受光照强度影响，主要用于光传感器电路。 电阻器的选型考虑因素 阻值：根据电路的工作要求选择合适的阻值。 功率：电阻器需要根据电路中流过的电流和电压承受相应的功率。过高的功率负载可能会导致电阻器过热，甚至损坏。功率单位是瓦特（W）。 精度：对于一些高精度电路（如测量电路），需要选择精度高的电阻器。电阻的精度通常以“±”表示，例如±1%、±5%等。 温度系数：电阻的阻值可能随温度的变化而变化，温度系数越小，电阻的稳定性越好，适合对温度变化敏感的电路。 耐用性与环境适应性：不同类型的电阻器适用于不同的环境，例如高温、高湿或腐蚀性环境。 电容 电容类型 电容器是用来储存电能并在需要时释放的电子元件。电容器的基本功能包括滤波、平滑电流、信号耦合等。根据其构造和材料的不同，电容器的类型也有所差异。\n常见的电容器种类有：\n陶瓷电容器：广泛用于高频电路，具有稳定的性能。 电解电容器：通常用于电源滤波电路，具有较大电容值，但需要注意极性。 薄膜电容器：适用于精密电路，具有较高的稳定性和低损耗。 铝电解电容器：主要应用于直流电源的滤波、去耦等方面，具有较大电容值。 钽电容器：与铝电解电容器类似，但具有更小的体积和更高的可靠性。 电容器的选型考虑因素 电容值：电容器的容量通常以法拉（F）为单位，常见的单位有微法（µF）、皮法（pF）等。电容值决定了电容器储存电能的能力，需要根据电路的需求来选择合适的电容值。 耐压值：电容器的耐压值需要高于电路中所施加的电压，避免电容器损坏。一般来说，选择一个比电路最大工作电压高20%-30%的耐压值是比较安全的。 ESR（等效串联电阻）：ESR是电容器的一个重要参数，影响电容器的效率和性能。低ESR值适用于高频电路。 温度范围与稳定性：对于高温或特殊环境下使用的电容器，需要选择具有较高温度稳定性的产品。 极性：部分电容器（如电解电容器）具有极性，在电路设计时需要特别注意极性连接。 电感 电感类型 电感器主要用于存储电能和限制电流的变化。电感器的基本功能包括滤波、耦合、储能和信号传输等。其工作原理是基于电流变化产生的磁场。\n常见的电感器类型有：\n固定电感器：用于稳定电流和滤波的电路中。 可调电感器：能够调节电感值，适用于频率调节电路。 铁氧体磁芯电感器：用于高频滤波和电源管理。 线圈电感器：多用于信号处理和电磁兼容性控制。 电感器的选型考虑因素 电感值：电感值的单位是亨利（H），常见的单位有毫亨（mH）和微亨（µH）。选择时需要考虑电路的工作频率以及电流大小。 工作频率：不同类型的电感器适用于不同频率范围的电路。高频电路通常需要选择小型化、低损耗的电感器。 电流承载能力：电感器的电流承载能力需要高于电路中流过的电流，否则可能会导致过热甚至损坏。 饱和电流：饱和电流是指电感器在工作时，电流超过一定值后其电感性能急剧下降的现象。选择时需要考虑电感器的饱和电流值，以确保其在工作过程中稳定性。 DC电阻（DCR）：电感器的DC电阻决定了其直流电流损耗，选择时需要确保DCR尽可能低，以提高电路的效率。 晶振 有源晶振和无源晶振该怎么选择\n晶振中的有源和无源到底有什么不同? 晶振是一种电子元件，它在电子设备中提供精确和稳定的时钟信号。晶振根据其工作原理的不同，可以分为有源晶振和无源晶振。\n**有源晶振( Oscillator)**是一种带有放大器的晶振，它可以提供更高的输出功率和更好的信号质量。有源晶振通常由振荡器、放大器和输出级组成。振荡器提供基本的振荡信号，放大器放大振荡信号的幅度，输出级将放大后的信号输出。有源晶振通常用在需要高信号质量和长距离传输的应用中，例如无线电通信、广播电视等。\n**无源晶振(Crystal)**是一种没有放大器的晶振，它的输出功率较低，但也具有精确和稳定的时钟信号。无源晶振通常由振荡器和输出级组成，没有放大器。无源晶振通常用在需要低功率和小体积的应用中，例如计算机、手机等。\n有源晶振和无源晶振在使用时需要注意以下几点：\n有源晶振通常需要使用外部电源，而无源晶振则可以直接使用被控设备的电源。 有源晶振的输出功率较高，可以传输更远的距离，但也需要更多的功耗。无源晶振的输出功率较低，但功耗也更小。 有源晶振的信号质量更好，但也更复杂，需要更多的电路来实现。无源晶振的信号质量较好，但也更简单，可以更容易地集成到电路中。 总之，有源晶振和无源晶振都是提供精确和稳定的时钟信号的重要元件。它们的区别在于工作原理的不同，有源晶振带有放大器可以提供更高的输出功率和更好的信号质量，而无源晶振没有放大器，功率较低，但更适合需要低功耗和小体积的应用。\n以下是一些选择有源晶振和无源晶振的建议：\n稳定性要求： 如果需要高精度、高稳定性的时钟信号，或者用于频率合成等要求较高的应用，建议选择无源晶振。因为无源晶振的稳定性更高，可以提供更加精确和稳定的输出信号。\n输出功率要求： 如果需要较高的输出功率和较低的输出阻抗，建议选择有源晶振。因为有源晶振具有较高的输出功率和较低的输出阻抗，可以满足一些需要高输出功率的应用场景。\n工作电压和电流要求： 有源晶振需要外部电源或电池等能量源来驱动，因此需要考虑其工作电压和电流要求是否符合实际应用需求。而无源晶振则无需外部电源驱动，因此不需要考虑其工作电压和电流要求。\n贴片晶振四个脚与两个脚的区别 类型的区别\n无源晶振\n两个脚的无源晶振：这种类型的晶振没有内置振荡电路，需要依赖外部电路来产生振荡信号。这种类型的晶振主要用于提供稳定的时钟信号，以确保电路的正常运行。 四个脚的无源晶振：尽管有四个脚，但真正称之为功能脚位的只有两个，即脚1和脚3。另外两个脚则起固定作用。这类无源晶振同样需要外部电路来产生振荡信号。 有源晶振\n四个脚的有源晶振：有源晶振是一种内部集成了振荡电路的晶振，不需要额外的外部元件就能产生振荡信号。它通常用于要求更高稳定性的场合，如GPS、蓝牙、WIFI等应用。 功能的区别\n无源晶振需要外部的时钟电路来产生振荡信号，而有源晶振内部集成了振荡电路，可以自行产生振荡信号。此外，有源晶振通常具有更高的稳定性和精度，但也需要额外的电源输入。\n使用方式的区别\n无源晶振在四脚的情况下，只有两个脚是功能脚，另外两脚是悬空的，用于接GND。而对于有源晶振，一般情况下有源晶振印字上面会标注脚位方向，即在左下角一个点，有点的代表引脚1。按逆时针(管脚向下)通常的用法是：一脚悬空，二脚接地，三脚接输出，四脚接电压。在安装有源晶振时，应当确保正确的脚位方向。如果安装错误，可能会导致晶振无法正常工作，甚至可能被电流击穿\n蓝牙BLE 蓝牙BLE开发实战教程-硬件电路设计\n天线 陶瓷贴片天线 PCB天线 FPC天线 SMA IPEX天线连接器 LED 结构： 发光二极管的核心部分是由P型半导体和N型半导体组成的晶片，在 P 型半导体和 N 型半导体之间有一个过渡层，称为PN结。在有些半导体材料的PN结中，注入的少数载流子与多数载流子复合时会把多余的能量以光的形式释放出来，从而把电能直接转换为光能。 发光原理： 当在LED即两端加上正向电压，电流从LED阳极流向阴极时，半导体晶体就发出从紫外到红外不同颜色的光线。光的强弱与电流有关，电流越大，发的光越强。 一般压降为2V（颜色不同不一样），安全电流20mA，所以一般需要串联一个限流电阻。Led计算器 OLED显示屏 LCD背光 LCD背光电路连接图分析\n为什么不能用 GPIO 直接驱动 LCD 背光？ ① 背光 LED 电流很大（几十 mA~几百 mA）\nSTM32（或任意 MCU）一个 GPIO 典型最大只允许 5mA~20mA。 而一块 LCD/IPS/TFT 屏的背光 LED 通常要： 20–40mA（小屏） 100–200mA（中等屏） 300mA+（亮度高的 IPS 屏） 如果用 GPIO 直推背光，会直接烧毁 MCU 引脚。\n② 背光需要恒流驱动或 PWM 调光\n许多 LCD 背光需要 PWM 调光（亮度控制），而 PWM 肯定要让电流快速变化。\nGPIO 不适合作为大电流 PWM 驱动器，原因包括：\n输出电流太小 输出阻抗高 开关速度不适合驱动大负载 在快速 PWM 下会发热甚至损坏 而 MOSFET/三极管非常适合做低损耗、大电流开关管。\n③ 背光供电电压可能不是 MCU 的电压\nMCU 多为 3.3V，LCD 背光常见：\n5V 6V 12V 甚至高达 20–30V（串联 LED 时） GPIO 根本不能承受 MCU 电压之外的电压。\n加 MOSFET/三极管后可轻松实现：\n3.3V 控制 5V/12V 背光 电平隔离 安全驱动不烧 MCU ④ GPIO 无法提供背光启动瞬间冲击电流\nLED 背光在启动的一瞬间会有 浪涌电流，比稳定电流大很多。\n如果直接接 GPIO：\n可能瞬间电压跌落 MCU 复位 GPIO 过流损坏 加开关管后，这些电流由 MOS 管承受，MCU 无压力。\n⑤ 设计需要保护与隔离\n使用 MOSFET/BJT 可以加入：\n限流电阻 TVS/ESD 保护 电流检测 软启动 PWM 调光 这些功能都不能由 GPIO 单独实现。\n典型背光驱动电路\n1 2 3 4 5 6 7 8 9 // 使用 N 沟道 MOS 管（最常用） // R1（1K）：限流作用，防止通向NMOS管栅极的电流过大，保护栅极。 // R2（10K）：作为下拉电阻，确保在BL引脚未被驱动时，NMOS管的栅极电压接近地电平，保持NMOS管处于截止状态。 MCU GPIO --R1---Rgate--|\u0026gt; MOSFET |----- LED+ (背光) | R2 | GND 特点：\nGPIO 只负责控制 栅极，几乎无电流压力 大电流由 MOS 承受 可高频 PWM 调光 可靠性强 排针 通常注意与pcb的以下参数对应：\n排针间距：一般为2.54mm 排针类型：圆针/方针 结构：一般为1 x N P，也可以掰开使用 MOS管 mos管的普遍实现为Nmos管，G输出高电平，D、S导通，G输出低电平，D、S关闭。Pmos管反之。\n关注参数：\n封装 Vgsth：打开nmos的GS电压，Vgsth应该小于高电平的电压值 Rdson：mos管被完全打开时的DS电阻值，越小越好，但价格越高、体积越大 Cgs：G、S之间的寄生电容，影响nmos的打开速度，容值大小一般与Rdson成反比 电子设计常用电路 电子设计常用电路\n外围电路 MCU（微控制单元，Microcontroller Unit）是嵌入式系统的\u0026quot;大脑\u0026quot;，但需通过外围电路实现供电、信号输入/ 输出、通信、存储等功能，才能构成完整的工作系统。外围电路的设计直接决定 MCU 的稳定性、功能扩展性和适用场景，入门需先掌握核心电路模块的作用与设计逻辑。\nMCU 与外围电路的关系 MCU 芯片内部集成了 CPU、RAM、ROM（或 Flash）、定时器、ADC 等核心模块，但存在两个关键局限：\n内部资源有限：如 IO 口驱动能力弱（通常仅能驱动 LED、小型传感器，无法直接驱动电机）、无外部供电接口、无远距离通信物理层等； 需外部交互：需接收传感器信号（如温度、按键）、控制外部执行器（如电机、继电器）、与其他设备（如电脑、模块）通信。 外围电路的核心作用：弥补 MCU 内部资源缺陷，搭建\u0026quot;大脑\u0026quot;与外部世界的连接桥梁，保障系统稳定运行。\n必学外围电路模块（按功能分类） 供电电路：MCU 的\u0026quot;能量来源\u0026quot; MCU 无法直接使用 220V 市电或锂电池（3.7V）等非标准电压，需供电电路将外部电压转换为 MCU 的核心工作电压（常见 3.3V 或 5V，需严格匹配芯片手册），同时滤除电压波动，避免 MCU 死机或损坏。\n核心组件与设计逻辑\n组件类型 作用 典型场景 线性稳压器（LDO） 输入电压＞输出电压，输出稳定、纹波小 对电压稳定性要求高的场景（如传感器采集），如 AMS1117-3.3V（输入 4.75-12V，输出 3.3V） DC-DC 转换器 输入电压可高于/ 低于输出电压，效率高 对功耗敏感的场景（如电池供电设备），如 MP1584（输入 4-28V，输出可调至 3.3V） 滤波电容 滤除电源线上的高频噪声，稳定电压 每个稳压器输出端并联 1 个 100nF 陶瓷电容（滤高频）+ 1 个 10μF 电解电容（滤低频） 电源指示灯\n直观判断供电是否正常 串联 1 个 1kΩ限流电阻+LED（电流＜20mA，避免烧毁） 入门注意：\n必须参考 MCU 手册的\u0026quot;供电参数\u0026quot;：如最大输入电压、工作电流，避免过压烧毁； 若系统有大功率模块（如电机），需单独为其设计供电电路，避免电流波动干扰 MCU。 复位电路：让 MCU\u0026quot;重启归零\u0026quot; 复位电路用于在系统上电、死机或异常时，强制 MCU 回到初始状态（类似电脑重启），保障程序正常运行。常见复位方式有上电复位 和手动复位。\n两种复位电路设计\n复位类型 核心组件 工作原理 适用场景 上电复位 电容（10μF）+ 电阻（10kΩ） 上电时电容充电，复位引脚（如 RST）短暂保持高电平，电容充满后变为低电平，MCU 开始运行 系统上电时自动复位，无需手动操作 手动复位 复位按键 + 电阻（10kΩ） 按下按键时，复位引脚接高电平，松开后恢复低电平，触发 MCU 复位 需要手动重启的场景（如程序调试） 入门注意：\n复位引脚电平需匹配 MCU 要求（多数 51 单片机为高电平复位，STM32 部分型号为低电平复位）； 复位时间需足够（通常＞1ms），避免电容/ 电阻参数过小导致复位不彻底。 时钟电路：MCU 的\u0026quot;心跳\u0026quot; MCU 的 CPU、定时器、UART 等模块需依赖 时钟信号 同步工作（类似人的心跳节奏），时钟频率决定 MCU 的运行速度（如 11.0592MHz 时钟下，51 单片机指令执行速度约 1MHz）。常见时钟源有__外部晶振__ （精准）和__内部 RC 振荡器__（便捷）。\n时钟信号 是 MCU 的 \u0026ldquo;电子节拍器\u0026rdquo; \u0026mdash;\u0026mdash; 它定义了 \u0026ldquo;时间单位\u0026rdquo;，让所有模块的动作在 \u0026ldquo;统一时间基准\u0026rdquo; 下有序执行，避免因 \u0026ldquo;动作不同步\u0026rdquo; 导致的功能混乱或错误\n两种时钟电路对比\n时钟类型 核心组件 优点 缺点 适用场景 外部晶振 晶振（如 11.0592MHz）+ 两个电容（22pF） 频率精准、稳定，适合串口通信（需精准波特率） 需额外焊接元件，占 PCB 空间 对时序要求高的场景（如 UART、SPI 通信） 内部 RC 振荡器 MCU 内部集成（无需外部元件） 无需外部元件，设计简单，成本低 频率误差较大（±5%），稳定性差 对精度要求低的场景（如 LED 闪烁） 入门注意：\n晶振频率需在 MCU 支持范围内（如 STM32F103 支持 4-16MHz 晶振）； 晶振与电容需靠近 MCU 时钟引脚（X1、X2），避免引线过长导致信号干扰。 IO 口扩展与驱动电路：连接外部设备 MCU 的 IO 口（输入/ 输出引脚）是与外部设备交互的\u0026quot;接口\u0026quot;，但存在两个核心限制：\n输出驱动能力弱：多数 IO 口最大输出电流仅 20-50mA，无法直接驱动电机、继电器等大功率设备； 输入信号敏感：需处理传感器的弱信号（如光敏电阻）或避免高压信号烧毁 IO 口。 因此需通过__驱动电路__扩展 IO 口能力，常见场景如下：\n典型 IO 口驱动场景\n外部设备 驱动电路设计 原理说明 注意事项 LED 指示灯 IO 口 → 限流电阻（1kΩ）→ LED → GND IO 口输出高电平时，电流通过 LED 发光（电流＜20mA） 电阻不可省略，否则 LED 过流烧毁；LED 正负极不可接反 按键（输入） IO 口 → 上拉电阻（10kΩ）→ VCC；按键另一端接 GND 未按按键时，IO 口通过上拉电阻接高电平；按下时接地，IO 口变为低电平，MCU 检测到按键动作 可使用 MCU 内部上拉电阻（如 STM32 的 PU 模式），减少外部元件 继电器/ 电机 IO 口 → 三极管（如 S8050）→ 继电器/ 电机；续流二极管（保护三极管） IO 口输出高电平驱动三极管导通，继电器/ 电机得电工作；续流二极管吸收电机断电时的反向电动势 三极管需匹配负载电流（如 S8050 最大集电极电流 1.5A）；大功率电机需加 MOS 管 通信接口电路：让 MCU\u0026quot;联网对话\u0026quot; MCU 需通过通信接口与其他设备（如电脑、传感器模块、显示屏）交换数据，常见通信协议有__UART（串口）__ 、I2C 、SPI，不同协议对应不同外围电路。\n三种常用通信接口对比\n通信协议 引脚数量 核心电路设计 特点 典型应用 UART（串口） 2（TX 发送、RX 接收） 电脑端需 USB 转串口模块（如 CH340），MCU 端直接接 TX/RX 异步通信，波特率需一致（如 9600bps），距离短（＜10m） 电脑与 MCU 通信（如程序下载、数据打印） I2C 2（SDA 数据线、SCL 时钟线） 两根线均需接 4.7kΩ上拉电阻到 VCC 同步通信，多主多从（可挂多个设备），距离短（＜1m） 连接传感器（如温湿度传感器 SHT30）、OLED 显示屏 SPI 4（SCK 时钟、MOSI 主发从收、MISO 主收从发、CS 片选） 无需上拉电阻，通过 CS 引脚选择通信设备 同步通信，速度快（＞10Mbps），距离短（＜1m） 连接 Flash 存储芯片（如 W25Q64）、LCD 显示屏 入门注意：\n通信引脚需\u0026quot;交叉连接\u0026quot;：MCU 的 TX 接 USB 转串口模块的 RX，MCU 的 RX 接模块的 TX； I2C 的上拉电阻不可省略，否则信号传输不稳定； SPI 的 CS 引脚需单独控制（同一时刻仅一个设备被选中）。 存储电路：保存数据\u0026quot;不掉电\u0026quot; MCU 内部 Flash/RAM 容量有限（如 51 单片机仅 4KB Flash、128B RAM），需外部存储电路扩展容量，用于保存程序、日志或参数（如设备配置信息）。常见存储芯片有__SPI Flash__ （存程序/ 大文件）和__EEPROM__（存小参数）。\n两种存储电路设计\n存储类型 核心芯片 通信协议 特点 典型应用 SPI Flash W25Q64（64MBit=8MB） SPI 容量大、擦写次数多（＞10 万次）、掉电不丢失 存储固件、图片、日志文件 EEPROM AT24C02（2KB） I2C 容量小、可字节级擦写、掉电不丢失 存储设备参数（如校准值、用户设置） 入门注意：\n存储芯片需通过对应通信协议（SPI/I2C）与 MCU 连接，程序中需调用相应驱动函数； EEPROM 擦写次数有限（通常 100 万次），避免频繁写入同一地址。 入门实践建议 从最小系统开始：先搭建 MCU 的\u0026quot;最小系统\u0026quot;（供电+ 复位+ 时钟），确保 MCU 能正常上电运行（如点亮一个 LED），再逐步扩展其他模块； 参考经典电路：新手可直接复用成熟设计（如 51 单片机最小系统、STM32 核心板电路），避免从零设计导致错误； 重视 datasheet：所有外围电路参数（如稳压器输入电压、晶振频率）需严格参考 MCU 和元件的 datasheet（芯片手册），这是设计的核心依据； 先仿真后焊接：使用 Proteus、Multisim 等软件仿真电路，验证功能正常后再实际焊接，减少硬件损坏风险。 电源防反接电路 在日常电子设计过程中，经常会使用排针、电池等容易插反的接口进行供电，此时一旦电源接反，会导致短路，通常芯片内部也无法承受反向电流，可能会导致芯片损坏。\n二极管防反接 使用一个二极管串联在电路中，当电源正常接入时，二极管导通,当电源反接时，二极管反向截止，整个系统不会有电流经过，但是此时二极管需要承担整个电路的电流，且自身会有压降，输出电压与实际输入电压会有一个二极管压差，建议综合电路负载电流情况选择不同型号的肖特基二极管。\n自动冷启动电路 自动冷启动电路\n有些MCU只会在冷启动（彻底掉电再启动）的某个阶段才会去检测RXD是否有合法的下载信号，然后下载烧录用户程序（如STC89C52RC）。如果要频繁下载程序，就需要频繁去手动冷启动，所以利用自动冷启动电路实现。\n开源项目 入门（2层板） 嘉立创-USB2.0拓展坞 基于VL813的USB3.0-HUB设计 Exlink最好用的嵌入式多功能调试器 EDA-Robot机器狗 STM32手表教程 进阶（4层板） 四层板PCB设计保姆级教程 3.0集线器 立创·梁山派开发板、立创梁山派4层PCB实战教程 工具 https://www.eetree.cn/tools 电路仿真 CircuitJS 电路分析工具 PCB走线载流计算器 PCB过孔载流计算器 嘉立创阻抗计算神器、Si9000 References 【教程】零基础入门PCB设计 【教程】大师篇-零基础入门PCB设计 Expert电子实验室 eda教程 pcb联盟网 嵌入式硬件知识 ","date":"2025-09-21T00:00:00Z","permalink":"https://loveleaves.github.io/p/embedded_programing_pcb/","title":"【Hardware】硬件设计与选型"},{"content":"References 分布式系统架构设计原理与实战：理解分布式系统的基本概念 https://cloud.tencent.com/developer/article/1905374 https://zhuanlan.zhihu.com/p/375847349 《Designing Data-Intensive Applications》，DDIA，设计数据密集型应用 DDIA解读 一文读懂分布式架构知识体系 背景介绍 分布式系统是指由多个独立的计算机节点组成的系统，这些节点通过网络连接在一起，共同完成某个任务或提供某个服务。分布式系统具有高度的可扩展性、高度的可靠性和高度的性能。因此，分布式系统已经成为现代信息技术的核心技术之一，广泛应用于互联网、大数据、人工智能等领域。\n然而，分布式系统也面临着很多挑战，如数据一致性、故障容错、负载均衡等。为了解决这些问题，需要深入理解分布式系统的基本概念和原理，并学习和掌握一些高级的分布式算法和技术。\n分布式系统的发展历程 分布式系统的发展历程可以分为以下几个阶段：\n基于消息传递的分布式系统（1970年代） 基于文件系统的分布式系统（1980年代） 基于Web的分布式系统（1990年代） 基于服务的分布式系统（2000年代） 基于云计算的分布式系统（2010年代至今） 每个阶段都有其特点和代表性的系统，如：\n基于消息传递的分布式系统：例如，ACTORS模型的系统。 基于文件系统的分布式系统：例如，Andrew文件系统。 基于Web的分布式系统：例如，Amazon的电子商务系统。 基于服务的分布式系统：例如，微软的.NET框架。 基于云计算的分布式系统：例如，阿里云、腾讯云、华为云等公有云服务。 分布式系统的特点 分布式系统具有以下特点：\n分布式性：节点分布在不同的计算机上，通过网络连接在一起。 并发性：多个节点可以同时执行任务，实现并行处理。 异步性：节点之间的通信可能存在延迟，需要处理异步问题。 故障性：单个节点的故障不会导致整个系统的宕机。 扩展性：通过增加节点，可以实现系统的扩展。 数据一致性：在分布式环境下，多个节点共享和修改同一份数据，需要保证数据的一致性。 分布式系统的分类 分布式系统可以分为以下几类：\n同步分布式系统：所有节点需要同时执行任务，实现并行处理。 异步分布式系统：节点之间可以自由地发送和接收消息，不需要同步。 有状态分布式系统：节点之间可以共享和修改状态信息，实现状态同步。 无状态分布式系统：节点之间不共享状态信息，实现无状态处理。 集中式分布式系统：有一个中心节点负责协调和管理其他节点，实现集中式控制。 去中心化分布式系统：没有中心节点，所有节点相互交互，实现去中心化管理。 核心概念与联系 在分布式系统中，有一些核心概念需要理解，如：\n节点（Node）：分布式系统中的基本组成单元。 网络（Network）：节点之间的连接。 通信（Communication）：节点之间的数据交换。 一致性（Consistency）：多个节点共享和修改同一份数据时，数据的一致性。 故障容错（Fault Tolerance）：单个节点故障不会导致整个系统宕机。 负载均衡（Load Balancing）：多个节点共同处理任务，实现资源利用率的均衡。 这些概念之间存在一定的联系，如：\n节点通过网络进行通信，实现任务的分布和协同。 通信是实现一致性和故障容错的关键。 负载均衡是实现系统性能和扩展的关键。 核心算法原理和实现流程 在分布式系统中，有一些核心算法需要理解，如：\n一致性算法：例如，Paxos、Raft等。 故障容错算法：例如，Chubby、ZooKeeper等。 负载均衡算法：例如，Round-robin、Least-connections、Random等。 一致性算法 一致性算法是用于实现数据一致性的算法，主要解决了分布式系统中多个节点共享和修改同一份数据时的一致性问题。\nPaxos算法 Paxos算法是一种一致性算法，可以在不需要时间顺序一致性的前提下，实现强一致性。Paxos算法的核心思想是通过多轮投票和选举来实现节点之间的协同。\nPaxos算法的主要组成部分包括：\n提案者（Proposer）：提出一个值进行决定。 接受者（Acceptor）：接受提案者的提案，并进行投票。 决策者（Learner）：收到多数接受者的支持，进行决策。 Paxos算法的具体操作步骤如下：\n提案者随机选择一个数字值，并向所有接受者发送提案。 接受者收到提案后，如果当前没有多数接受者支持其他提案，则支持当前提案，并向提案者报告支持情况。 提案者收到多数接受者的支持后，向决策者发送决策请求。 决策者收到多数接受者的支持后，进行决策，并向所有节点广播决策结果。 Raft算法 Raft算法是一种一致性算法，可以在有限的时间内实现强一致性。Raft算法的核心思想是通过选举来实现领导者的选举和数据复制。\nRaft算法的主要组成部分包括：\n领导者（Leader）：负责接收客户端请求，并向其他节点复制数据。 追随者（Follower）：等待选举，如果成为领导者，则向其他节点复制数据。 候选者（Candidate）：尝试成为领导者，通过选举来实现。 Raft算法的具体操作步骤如下：\n每个节点随机选择一个领导者标识，并向其他节点发送请求加入集群。 其他节点收到请求后，如果当前领导者已经存在，则将请求丢弃；如果当前领导者不存在，则将当前节点设置为候选者状态，并向其 节点发送自己为候选者的请求。 候选者收到多数节点的支持后，成为领导者，并向其他节点发送心跳消息。 追随者收到领导者的心跳消息后，更新自己的领导者标识，并设置为追随者状态。 客户端向领导者发送请求，领导者将请求广播给其他节点，并将数据复制到其他节点。 数学模型公式 Paxos和Raft算法的数学模型公式如下：\nPaxos算法： $$ n=3f+1n = 3f + 1n=3f+1 $$ 其中，n是节点数量，f是故障节点数量。\nRaft算法： $$ n=3fn = 3fn=3f $$ 其中，n是节点数量，f是故障节点数量。\n故障容错算法 故障容错算法是用于实现故障容错的算法，主要解决了分布式系统中单个节点故障不会导致整个系统宕机的问题。\nChubby算法 Chubby算法是一种故障容错算法，可以实现分布式系统中的共享锁和文件系统。Chubby算法的核心思想是通过集中式控制来实现故障容错。\nChubby算法的主要组成部分包括：\n主服务器（Master Server）：负责管理所有节点的状态。 备份服务器（Backup Server）：负责备份主服务器的状态。 客户端（Client）：与主服务器和备份服务器进行通信。 Chubby算法的具体操作步骤如下：\n客户端向主服务器发送请求，主服务器处理请求并返回结果。 主服务器在处理请求时，可以将请求委托给备份服务器处理。 主服务器和备份服务器之间通过心跳消息来实现故障检测和故障转移。 ZooKeeper算法 ZooKeeper算法是一种故障容错算法，可以实现分布式系统中的配置管理和集群管理。ZooKeeper算法的核心思想是通过多个服务器实现故障容错，并通过主备模式来实现高可用。\nZooKeeper算法的主要组成部分包括：\n主服务器（Leader）：负责处理客户端请求。 备份服务器（Follower）：负责备份主服务器的状态。 客户端（Client）：与主服务器和备份服务器进行通信。 ZooKeeper算法的具体操作步骤如下：\n客户端向主服务器发送请求，主服务器处理请求并返回结果。 主服务器在处理请求时，可以将请求委托给备份服务器处理。 主服务器和备份服务器之间通过心跳消息来实现故障检测和故障转移。 数学模型公式 Chubby和ZooKeeper算法的数学模型公式如下：\nChubby算法： $$ n=3fn = 3fn=3f $$ 其中，n是节点数量，f是故障节点数量。\nZooKeeper算法： $$ n=2f+1n = 2f + 1n=2f+1 $$ 其中，n是节点数量，f是故障节点数量。\n负载均衡算法 负载均衡算法是用于实现系统性能和扩展的算法，主要解决了分布式系统中多个节点共同处理任务的问题。\nRound-robin算法 Round-robin算法是一种负载均衡算法，可以实现基于轮询的请求分发。Round-robin算法的核心思想是将请求按顺序分发给节点。\nRound-robin算法的具体操作步骤如下：\n创建一个请求队列，将所有请求加入队列。 从队列中取出第一个请求，将其分发给第一个节点处理。 将请求队列中的下一个请求分发给第二个节点处理。 重复步骤2和3，直到队列中的所有请求都被处理。 Least-connections算法 Least-connections算法是一种负载均衡算法，可以实现基于最少连接数的请求分发。Least-connections算法的核心思想是将请求分发给连接数最少的节点。\nLeast-connections算法的具体操作步骤如下：\n创建一个节点状态表，记录每个节点的连接数。 从节点状态表中选择连接数最少的节点，将请求分发给该节点处理。 处理完请求后，更新节点状态表。 Random算法 Random算法是一种负载均衡算法，可以实现基于随机选择的请求分发。Random算法的核心思想是将请求随机分发给节点。\nRandom算法的具体操作步骤如下：\n创建一个请求队列，将所有请求加入队列。 从队列中随机选择一个请求，将其分发给一个节点处理。 重复步骤2，直到队列中的所有请求都被处理。 ","date":"2025-03-22T00:00:00Z","permalink":"https://loveleaves.github.io/p/distributed_system/","title":"【SE】 分布式系统架构设计"},{"content":"总结 个人总结，带有偏见，仅作参考\n事物的两面性：一件事从正反面甚至不同方面去思考，思考过程可以理性/感性，站在不同视角不受约束地为己方辩护，最后总结不要带有倾向性。 贴标签/去标签：给人或物贴标签，易于利用已有经验，但也会忽略关键细节。 利己性：根据自己（自发/他人诱发）的目的，调整自己的思想价值观念和行为，使其利于达成自己的目的。 行为复杂性：某一想法无法实现理论设想的行为，因为需要外物去执行，会因外物的利己性导致偏差，甚至替换原有想法。 思维超脱/物理现实 思维超脱：思维目的优先，结合现实手段进行实现，需要超理性 物理现实：物理需求优先，思维为超越生物欲望额外需求，借助理性获得物质，感性驱动 游戏 黑神话-悟空：反抗权威/命运，正义（胜利者）/邪恶（失败者） 因与果：整体目标/个体意义，潜意识 电影 夜行动物：遗传利弊（原生家庭/后天人格等）、价值观念差异、情感/理性 肖申克的救赎：改变的决心/毅力 盗梦空间：梦/心理暗示/潜意识 书籍 动漫 英雄联盟：双城之战：情感（什么是亲情/友情/爱情）、保守/革新 快乐时刻 人生不止眼前和远方的苟且，还有当下多巴胺的快乐。\n定期花点时间做点自己真正喜欢的事，不管其是否有意义 多接触新的人或事，了解年轻人的想法，保持年轻的心态 超脱时刻 人生不因止于生物欲望的满足，人类社会债务等的约束，也因思考超脱于人类社会以外的意义\n不要让物质欲望的追求成为囚禁自己牢笼，成为其他尝试的枷锁 思维的锻炼不能停止，追求超理性，利用感性=》理性思考=》移除感性 ","date":"2025-03-15T00:00:00Z","permalink":"https://loveleaves.github.io/p/life/","title":"记录一些可能对个人思考有用的东西"},{"content":"References https://segmentfault.com/a/1190000045332157 《实现领域驱动设计》 DDD分层架构：有效降低层与层之间的依赖 MVC和DDD的对比 实战一（上）：业务开发常用的基于贫血模型的MVC架构违背OOP吗？ https://zhuanlan.zhihu.com/p/343388831 https://zhuanlan.zhihu.com/p/342826364 DDD分层架构 分层架构的基本原则 每层只与位于其下方的层发生耦合。\n分层架构的分类 严格分层架构(Strict Layers Architecture) 某层只能与其直接下层耦合，即我的奴隶的奴隶，不是我的奴隶。 松散分层架构(Relaxed Layers Architecture) 允许任意上层与任意下层耦合。由于用户接口层和应用服务通常需要与基础设施打交道，许多系统都是该架构。 较低层有时也可与较高层耦合，但只限于采用观察者 (Observer)模式或者调停者(Mediator)模式场景。 较低层绝不能直接访问较高层。例如，在使用调停者模式时，较高层可能实现了较低层的接口，然后将实现对象作为参数传递到较低层。当较低层调用该实现时， 它并不知道实现出自何处。\n分层架构演进 传统四层架构 将领域模型和业务逻辑分离出来，并减少对基础设施、用户界面甚至应用层逻辑的依赖，因为它们不属业务逻辑。将一个夏杂的系统分为不同的层，每层都应该具有良好的内聚性，并且只依赖于比其自身更低的层。\n传统分层架构的基础设施层位于底层，持久化和消息机制便位于该层。 这里的消息包含\nMQ消息 SMTP 文本消息(SMS) 可将基础设施层中所有组件看作应用程序的低层服务，较高层与该层发生耦合以复用技术基础设施。即便如此，依然应避免核心的领域模型对象与基础设施层直接耦合。\n改良版四层架构 传统架构的缺陷\nDDD初创开发团队发现，将基础设施层放在最底层存在缺点，比如此时领域层中的一些技术实现就很困难：\n违背分层架构的基本原则 难以编写测试用例 何解？ 使用依赖反转设计原则：低层服务（如基础设施层）应依赖高层组件（比如用户界面层、应用层和领域层）所提供的接口。\n应用依赖反转原则\n依赖反转原则后的分层方式：基础设施层在最上方，可实现所有其他层中定义的接口 依赖反转原则真的可以支持所有层吗？ 有人认为依赖反转原则中只存在两层：最上方和最下方，上层实现下层定义的抽象接口。因此上图的基础设施层将位于最上方，而用户接口层、应用层和领域层应作同层且都位于下方。对此大家可保留自己意见。\n各层职责 用户接口层 一般包括用户接口、Web 服务等。\n只处理用户显示和用户请求，不应包含领域或业务逻辑。 有人认为，既然用户接口需验证用户输入，就无可避免应该包含业务逻辑。事实上，用户接口所进行的验证和对领域模型的验证不同：对那些粗制滥造且只面向领域模型的验证行为，应该予以限制。\n如果用户接口使用了领域模型中的对象，那么此时领域对象仅限于数据渲染展现。在采用这种方式时，可使用展现模型对用户接口与领域对象进行解耦。\n由于用户可能是人，也可能是其他系统，有时用户接口层将采用开放主机服务的方式向外提供API。用户接口层是应用层的直接用户。\n用户接口层在于前后端调用的适配。若你的微服务要提供服务给很多外部应用，而对每个外部应用的入参出参都不同，你不可能开发一堆一对一的应用服务，这时Facade接口就起到了很好的作用，包括DO和DTO对象的组装和转换。\n应用层 主要包含应用服务，理论上不应有业务规则或逻辑，而主要是面向用例和流程相关的操作。\n应用层位于领域层之上，因为领域层包含多个聚合，所以它可协调多个聚合服务和领域对象完成服务编排和组合，协作完成业务。 应用层也是微服务间的交互通道，它可调用其它微服务，完成微服务间的服务组合和编排。 开发设计时，不要将本该放在领域层的业务逻辑放到应用层。因为庞大的应用层会使领域模型失焦，时间一长，微服务就会退化为MVC架构，导致业务逻辑混乱。\n应用服务是在应用层，负责\n服务的组合、编排、转发、转换和传递，处理业务用例的执行顺序以及结果的拼装，以粗粒度服务通过API网关发布到前端 安全认证 权限校验 事务控制 发送或订阅领域事件 领域层 主要包含聚合根、实体、值对象、领域服务等领域模型中的领域对象。\n实现核心业务逻辑，通过各种校验保证业务正确性。领域层主要体现领域模型的业务能力，它用来表达业务概念、业务状态和业务规则。\n领域模型的业务逻辑主要由实体和领域服务实现：\n实体采用充血模型 实现所有与之相关的业务功能。 实体和领域服务在实现业务逻辑上不是同级，当领域中的某些功能，单一实体或值对象无法实现，就会用到领域服务，它可组合聚合内的多个实体或值对象，实现复杂业务逻辑。\n基础层 为其它各层提供通用技术基础服务：\n三方工具 驱动 MQ API网关 文件 缓存 DB 最常用的 基础层包含基础服务，它采用依赖反转，封装基础资源服务，实现应用层、领域层与基础层解耦。\nMVC架构由于上层应用对DB强耦合，很多公司在架构演进最怕换DB，一旦更换，可能需重写一堆代码。 但采用依赖反转，应用层即可通过解耦保持独立核心业务逻辑。当DB变更，只需更换DB基础服务。\n微服务架构演进 领域模型中对象的层次从内到外依次是：值对象、实体、聚合和限界上下文。\n实体或值对象的简单变更，一般不会让领域模型和微服务发生大变。但聚合的重组或拆分却可以。因为聚合内业务功能内聚，能独立完成特定业务。那聚合的重组或拆分，势必引起业务模块和系统功能变化。\n可以聚合为基础单元，完成领域模型和微服务架构的演进。 聚合可作为整体，在不同领域模型间重组或拆分，或直接将一个聚合独立为微服务。\n微服务架构的演进案例 现有\n微服务 1：包含聚合 a、b、c 微服务2： 微服务3：包含聚合 d、e、f 当发现微服务1中聚合a的功能经常被高频访问，以致拖累了整个微服务1的性能，可把聚合a，从微服务1中剥离，独立为微服务2以应对高性能场景\n随业务发展，发现微服务3的领域模型变化，聚合d会更适合放到微服务1的领域模型。即可将聚合d整体迁移到微服务1。注意定义好聚合间的代码边界\n架构演进后，微服务1从最初包含聚合a、b、c，演进为包含聚合b、c、d的新领域模型和微服务\n可见，好的聚合和代码模型的边界设计，可让你快速应对业务变化，轻松实现领域模型和微服务架构演进。\n微服务内服务的演进 在微服务内部，实体的方法被领域服务组合和封装，领域服务又被应用服务组合和封装。在服务逐层组合和封装的过程中，你会发现这样一个有趣的现象。 服务设计时，你并不一定能完整预测有哪些下层服务会被多少个上层服务组装，因此领域层通常只提供一些原子服务，比如领域服务a、b、c。但随系统功能增强和外部接入越来越多，应用服务不断丰富。终有一日，你会发现领域服务b和c同时多次被多个应用服务调用了，执行顺序也基本一致。这时你可以考虑将b和c合并，再将应用服务中b、c的功能下沉到领域层，演进为新的领域服务（b+c）。这样既减少了服务的数量，也减轻了上层服务组合和编排的复杂度。\n这就是服务演进，领域模型会越来越能适应需求快速变化。\n从MVC跨越到DDD 由于层间松耦合，可专注本层设计，而不必关心其它层，也不必担心自己的设计会影响其它层。即DDD成功降低层与层之间的依赖。\n分层架构使得程序结构更清晰，升级和维护更容易。修改某层代码时，只要本层接口参数不变，其它层不必修改。即使本层接口发生变化，也只影响相邻的上层，修改工作量小且可控。\n传统企业应用大多是单体架构，而单体架构则大多是三层架构。三层架构解决了程序内代码间调用复杂、代码职责不清的问题，但这种分层是逻辑概念，在物理上它是中心化的集中式架构，并不适合分布式微服务架构。\nDDD分层要类似三层架构，只是在DDD中，这些要素被重新划分了层，确定了层与层之间的交互规则和职责边界。 DDD分层架构相比MVC（只有API）在用户接口层新增了DTO，给前端提供了更多的可使用数据和更高的展示灵活性。\nDDD分层架构对三层架构的业务逻辑层进行了更清晰的划分，改善了三层架构核心业务逻辑混乱，代码改动相互影响大的情况。\nMVC架构向DDD分层架构演进，主要发生在业务逻辑层和数据访问层。 DDD分层架构将业务逻辑层的服务拆分到了应用层和领域层：\n应用层快速响应前端的变化 领域层实现领域模型的能力 数据访问层和基础层之间：\n三层架构数据访问采用DAO方式 DDD分层架构的数据库等基础资源访问，采用了仓储（Repository）设计模式，通过依赖倒置实现各层对基础资源的解耦。 仓储本身属基础层，但考虑到一个聚合对应一个仓储，为了以后聚合代码整体迁移方便，在微服务代码目录设计时，在聚合目录下增加一个Repository的仓储目录，跟仓储相关的代码都在这个目录下。 这个目录下的代码与聚合的其它业务代码是分开的。如果未来换数据库，只需将Repository目录下的代码替换。而如果聚合需要整体迁移到其它微服务中去，仓储的代码也会一并迁移。\n仓储又分为两部分：仓储接口和仓储实现。仓储接口放在领域层中，仓储实现放在基础层。原来三层架构通用的第三方工具包、驱动、Common、Utility、Config等通用的公共的资源类统一放到了基础层。\nMVC 到 DDD 具体操作如下：\n抽象数据存储层 一般将Data Access层做抽象，降低系统对DB的直接依赖。 举个例子：\n新建Account实体对象：一个实体（Entity）是拥有ID的域对象，除了拥有数据之外，同时拥有行为。Entity和数据库储存格式无关。 对象储存接口类AccountRepository：Repository只负责Entity对象的存储和读取，而Repository的实现类完成数据库存储的细节。通过加入Repository接口，底层数据库连接可以通过不同的实现类而替换。\n","date":"2025-03-10T00:00:00Z","permalink":"https://loveleaves.github.io/p/se_ddd/","title":"【SE】 DDD领域驱动设计实战-分层架构"},{"content":"References https://segmentfault.com/a/1190000045395117 消息中间件优缺点对比及选型 概念 什么是中间件？ 非底层操作系统软件，非业务应用软件，不是直接给最终用户使用的，不能直接给客户带来价值的软件统称为中间件。\n什么是消息中间件？ 关注于数据的发送和接受，利用高效可靠的异步消息传递机制集成分布式系统。\n消息中间件的作用（解耦，并发，削峰，异步） 可以在模块、服务、接口等不同粒度上实现解耦 订阅/消费模式可以在数据粒度上解耦 可提高系统的并发能力，集中力量办大事（同步部分），碎片时间做小时（异步部分） 可提高系统可用性，因为缓冲了系统负载 消息中间件的弊端 系统可用性降低：MQ宕机之后整套系统均不能正常使用，如果要保障队列可用，需要额外机制保障（双活或容灾） 系统复杂性提高：存在消息重复消费、消息丢失、消息传递顺序不能保证的问题 降低数据一致性：多个系统消费存在部分成功部分失败的问题，数据不一致了，如要保持强一致性，需要高代价的补偿（分布式事务，对账） 重复消费：系统发了两条，两条都插入了数据库 消息丢失：系统根本没法请求到目标系统 一致性问题：系统要再ABC三个系统都执行成功之后才返回成功，结果AB成功了，C失败了\n什么是消息队列?（Message queue，简称MQ） 从字面理解就是一个保存消息的一个容器。那么我们为何需要这样一个容器呢？\n其实就是为了解耦各个系统，我们来举个例子： 有这么一个简单的场景，系统A负责生成userID，并调用系统B、C。如果系统BC频繁变化是否需要userID参数，则系统A的代码就得不断变化，如果哪天又来了系统DEF……也需要这个参数，则系统A又要加入很多业务逻辑，这样子各他系统之间就容易产生相互影响，另外大量的系统与A发生交互也容易产生问题。 加了消息队列后，A只负责产生userID，至于谁要用这个参数，怎么用？系统A不管。对这个数据感兴趣的系统自己去取用即可，各个系统之间就实现了解耦。而且解耦后，整个服务业变成了一个异步的方式，系统A产生数据后，不用依次调用BCD来累计耗时，各系统可以同时来取用消息队列的数据进行处理，加大吞吐。\n消息队列的特点 先进先出：消息队列的顺序在入队的时候就基本已经确定了，一般是不需人工干预的。 发布订阅：发布订阅是一种很高效的处理方式，如果不发生阻塞，基本可以当成是同步操作。 持久化：持久化确保消息队列的使用不只是一个部分场景的辅助工具，而是让消息队列能像数据库一样存储核心的数据。 分布式：在现在大流量、大数据的使用场景下，支持分布式的部署，才能被广泛使用。消息队列的定位就是一个高性能的中间件。 消息队列的使用场景 消息队列的使用场景有很多，最核心的有三个：解耦、异步、削峰\n解耦：一个系统或者一个模块，调用了多个系统或者模块，相互之间的调用很复杂，维护起来很麻烦。此时可以考虑使用消息队列来实现多个系统之间的解耦 异步：系统A接受一个请求，需要在自己本地写库，还需要在系统BCD三个系统写库，同步操作比较费时。 削峰：高峰时段系统接收到的请求缓存到消息队列，供系统根据负载慢慢消化 如秒杀、发邮件、发短信、高并发订单等。 不适合的场景如银行转账、电信开户、第三方支付等。 关键还是要意识到队列的优劣点，然后分析场景是否使用。\n其他使用场景还有：\n最终一致性：先写消息再操作，例如预写日志（Write Ahead Log，WAL） 日志处理：比如 Kafka 的应用，解决海量日志传输和缓冲的问题。 消息通信：消息队列一般都内置了高效的通信机制 MQ的6种工作模式: 简单模式：一个生产者，一个消费者 work模式：一个生产者，多个消费者，每个消费者获取到的消息唯一。 发布/订阅模式（Pub/Sub）：一个生产者发送的消息会被多个消费者获取。 路由模式：发送消息到交换机并且要指定路由key ，消费者将队列绑定到交换机时需要指定路由key topic模式：将路由键和某模式进行匹配，此时队列需要绑定在一个模式上，“#”匹配一个词或多个词，“*”只匹配一个词。 消息队列常见问题 如何保证消息队列的高可用（High Available, HA）？ RabbitMQ基于主从的高可用，分为单机模式、普通集群模式、镜像集群模式三种\n普通集群模式：多台服务器部署RabbitMQ，一个queue只会保存在一个节点上，其他节点只会同步该queue的元数据，当请求从其他节点获取该queue的数据时，该节点会再次去存储该queue的节点上拉取所需数据。这样就导致使用时要么固定使用其中一个节点，要么随机节点再需要的时候拉取数据。如果存放数据的节点宕机了，其他节点就无法拉取数据，如果开启了消息持久化让RabbitMQ落地存储消息就不一定会丢失消息，得等这个实例恢复后才能继续从这个queue拉取数据。 镜像集群模式（高可用模式）：创建的queue会同步到所有实例上来实现高可用。这样会带来同步数据的开销和扩展性降低（扩展机器会导致新增的机器同步queue增加更多同步数据的开销）；配置方式可通过控制台配置。 Kafka的高可用：分布式消息队列 Kafka由多个broker组成，每个broker是一个节点，创建的一个topic划分为多个partition，每个partition可放在不同的broker上，每个partition只存放一部分数据。\n","date":"2025-03-10T00:00:00Z","permalink":"https://loveleaves.github.io/p/se_mq/","title":"【SE】 消息中间件"},{"content":"References https://www.imooc.com/article/361234 https://blog.csdn.net/qq_40610003/article/details/143609078 前言 本文档全面介绍了系统架构师教程，涵盖了角色与职责、所需技能、职业发展路径以及架构设计原则和模式等内容，旨在帮助读者从入门到实践系统架构设计。文章还深入探讨了架构评估与优化策略，并提供了实际案例分析和常用工具介绍，帮助读者全面提升系统架构设计能力。\n介绍 系统架构师（System Architect, SA, SAr）是软件开发团队中的关键角色，负责指导团队进行架构设计和实施。本文档从系统架构师的角色与职责开始，逐步深入到架构设计原则、评估与优化，以及实际案例分析，最后探讨如何通过实践和工具来提升架构设计能力。\nSE和SA区别 系统工程师SE：负责本版本系统分析与设计的所有活动，关注当前版本的所有需求，关注当前版本所有的技术方案，管理SE团队。 架构师SA：负责本产品的架构设计和架构维护，关注影响架构的当前需求和未来需求，关注影响架构的技术方案，负责领域架构在当前版本的落地和产品架构的生命周期管理。 角色与职责 系统架构师的主要职责包括但不限于：\n架构设计：设计系统的整体框架，包括模块划分、组件间关系、数据流等。 技术选型：根据业务需求和技术趋势选择合适的技术栈。 性能优化：确保系统在高并发、大数据量等场景下仍能保持高效运行。 安全性保障：确保系统的安全性，防止数据泄露等安全事件。 团队领导：指导开发团队按照架构设计进行开发，并解决开发过程中遇到的技术难题。 监控与维护：监控系统运行状态，及时发现并解决性能瓶颈等问题。 用户反馈：与用户沟通，收集反馈，并根据用户反馈不断优化架构设计。 需要掌握的技能 系统架构师需要掌握以下技能：\n编程语言：具备至少一种主流编程语言的深厚技术功底，如Java、Python、C#等。 数据结构与算法：深入理解常用的数据结构（如数组、链表、树、图等）和算法（如排序、查找等）。 计算机网络：掌握计算机网络基础知识，如TCP/IP协议、HTTP协议等。 数据库技术：精通至少一种关系型数据库（如MySQL、Oracle）或非关系型数据库（如MongoDB）。 操作系统：熟悉主流的操作系统（如Linux、Windows）和虚拟化技术。 架构设计：理解常用的设计模式（如单例模式、工厂模式等）和架构模式（如微服务、SOA等）。 性能优化：掌握常见的性能优化方法和技术。 安全性：了解常见的安全漏洞和防护方法。 DFX特性：可维可测可靠等。 团队协作：具备良好的团队协作能力和沟通能力。 职业发展路径 系统架构师的职业发展路径通常如下：\n软件工程师：从一个普通的软件工程师开始，逐步积累编程经验。 高级工程师：通过不断学习和实践，成为公司内部的技术专家。 系统架构师：在高级工程师的基础上，进一步提升自己的系统设计能力。 技术总监：成为公司内部的技术决策者，负责整体技术方向和架构设计。 CTO：成为公司的首席技术官，负责公司的技术战略和产品开发。 基础架构知识 架构设计原则与模式 设计模式及其应用 设计模式是面向对象编程中的概念，用于解决特定问题的通用解决方案。常见的设计模式包括单例模式、工厂模式、观察者模式等。\n架构设计基本原则 KISS原则：保持简单，使系统易于理解和维护。 YAGNI原则：避免过度设计，只实现当前需要的功能。 DRY原则：不要重复自己，通过抽象和模块化减少代码冗余。 单一职责原则：每个模块或类只负责一个功能，避免职责混杂。 高内聚、低耦合：模块内部高度紧密，模块间弱耦合。 面向接口编程：通过定义接口来解耦实现细节。 常见架构模式解析 微服务架构：将一个大型系统拆分成多个小型、相互独立的服务，每个服务负责一个特定的功能。 SOA（面向服务的架构）：将系统组件抽象为服务，通过服务间的通信实现业务流程。 事件驱动架构：通过事件触发系统组件的响应，适用于异步处理场景。 架构设计中的常见问题及解决方案 问题1：单点故障\n解决方案：通过冗余设计和故障转移机制来解决单点故障问题。 问题2：性能瓶颈\n解决方案：通过性能测试找出性能瓶颈，并通过缓存、负载均衡等方式进行优化。 问题3：安全性问题\n解决方案：通过输入验证、权限管理和加密传输等方式来提高系统安全性。 架构评估与优化 架构性能评估方法 性能测试：使用工具（如JMeter、LoadRunner）模拟不同负载下的系统性能。 代码审查：检查代码中是否存在潜在的性能瓶颈。 监控与日志：通过监控系统运行状态和分析日志来发现性能问题。 架构安全性与可靠性考虑 安全性考虑：\n输入验证：确保所有输入都经过验证，防止SQL注入、跨站脚本攻击等。 权限管理：确保用户只能访问其权限范围内的资源。 加密传输：使用SSL/TLS协议加密传输数据，防止数据被窃听。 可靠性考虑：\n冗余设计：通过冗余设计提高系统的可用性，如使用多副本存储数据。 故障转移：在主服务不可用时自动切换到备用服务。 容错处理：设计系统时考虑可能出现的异常情况，并提供相应的容错机制。 实际案例分析 典型系统架构案例解析 案例1：电商平台\n架构设计：包括用户中心、订单中心、支付中心等模块。 技术选型：使用Spring Boot进行后端开发，React进行前端开发，MySQL和Redis作为数据库和缓存。 案例解析：通过微服务架构将大型系统拆分为多个小型服务，每个服务专注于一个特定功能。 案例2：视频流媒体平台\n架构设计：包括内容分发、视频编码、用户界面等模块。 技术选型：使用Node.js进行后端开发，React Native进行前端开发，MongoDB和Elasticsearch作为数据库。 案例解析：通过事件驱动架构实现异步处理，提高系统响应速度。 架构设计中的常见问题及解决方案 问题1：单点故障\n解决方案：通过冗余设计和故障转移机制来解决单点故障问题。 问题2：性能瓶颈\n解决方案：通过性能测试找出性能瓶颈，并通过缓存、负载均衡等方式进行优化。 问题3：安全性问题\n解决方案：通过输入验证、权限管理和加密传输等方式来提高系统安全性。 架构优化路径 电商平台\n通过引入缓存机制来减少数据库查询压力。 使用负载均衡来提高系统响应速度。 通过事件驱动架构实现异步处理，减少阻塞等待时间。 视频流媒体平台\n通过使用CDN来提高内容分发速度。 通过使用容器化部署来提高系统的灵活性和可扩展性。 通过引入机器学习算法来优化视频编码效率。 实践与工具 常用架构设计工具介绍 架构设计工具：\nVisio：Microsoft提供的架构设计工具，支持多种图形设计。 Archimate：支持架构建模语言，能够进行企业架构设计。 Lucidchart：在线协作绘图工具，支持架构设计和流程图绘制。 架构设计的实践步骤 实践步骤：\n需求分析：与业务团队沟通，明确业务需求。 技术选型：根据业务需求和技术趋势选择合适的技术栈。 架构设计：设计系统的整体框架，包括模块划分、组件间关系等。 系统实现：根据架构设计进行系统实现。 测试与部署：进行性能测试、安全测试等，确保系统满足要求。 运维监控：监控系统运行状态，不断优化架构设计。 如何构建个人架构设计案例集 构建案例集：\n记录设计过程：记录每次架构设计的过程，包括需求分析、技术选型、架构设计等。 总结经验教训：总结每次设计中的经验教训，不断改进自己的设计能力。 分享交流：通过博客、GitHub等渠道分享自己的设计经验，与他人交流学习。 记录设计过程示例： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # 架构设计案例集 ## 案例1：电商平台架构设计 ### 需求分析 - 用户中心：提供用户信息管理功能。 - 订单中心：处理订单创建、支付、发货等业务逻辑。 ### 技术选型 - 后端：Spring Boot - 前端：React - 数据库：MySQL、Redis ### 架构设计 - 微服务架构：将系统拆分为多个小型服务，每个服务专注于一个特定功能。 - 负载均衡：使用Nginx进行负载均衡，提高系统可用性。 ### 测试与部署 - 性能测试：使用JMeter进行性能测试，确保系统满足性能要求。 - 安全测试：进行输入验证、权限管理等，确保系统安全性。 ### 运维监控 - 监控系统运行状态，及时发现并解决性能瓶颈等问题。 - 使用ELK（Elasticsearch、Logstash、Kibana）进行日志监控。 通过以上步骤，你可以构建出一个完整的个人架构设计案例集，记录自己的设计经验和心得，不断提升自己的架构设计能力。\n","date":"2025-03-08T00:00:00Z","permalink":"https://loveleaves.github.io/p/se_intro/","title":"【SE/SA】 系统架构师/系统工程师介绍"},{"content":"References tensorrt docs tensorrt api install tensorrt TensorRT samples Implementation of popular deep learning networks with TensorRT TensorRT推理部署方案 learning-cuda-trt code tensorRT quantization C++硬代码方案 代表：tensorrtx 流程：C++硬代码=》TRT API =》 TRT Builder =》 TRT Engine ONNX方案 流程：ONNX(libnvonnxparser.so) =》TRT API =》 TRT Builder =》 TRT Engine 一般思路： 导出模型onnx，查看输入和输出。 查看代码，找到onnx的预处理，分析预处理逻辑 利用上述信息实现onnx py推理实现 验证正常可实现转TRT模型用C++实现推理 TensorRT库文件 libnvinfer.so：TensorRT核心库 libnvinfer_plugin.so：nvidia官方提供的插件，github libprotobuf.so：protobuf库 libnvonnxparser.so：ONNX解析 TensorRT部署推理模型流程 模型构建 tensorrt的工作流程如下图： 首先定义网络 优化builder参数 通过builder生成engine,用于模型保存、推理等 engine可以通过序列化和逆序列化转化模型数据类型（转化为二进制byte文件，加快传输速率），再进一步推动模型由输入张量到输出张量的推理。 code structure 定义 builder, config 和network，其中builder表示所创建的构建器，config表示创建的构建配置（指定TensorRT应该如何优化模型），network为创建的网络定义。 输入，模型结构和输出的基本信息（如下图所示） 生成engine模型文件 序列化模型文件并存储 模型推理 执行推理的步骤：\n准备模型并加载\n创建runtime：createInferRuntime(logger)\n使用运行时时，以下步骤：\n反序列化创建engine, 得为engine提供数据：runtime-\u0026gt;deserializeCudaEngine(modelData, modelSize),其中modelData包含的是input和output的名字，形状，大小和数据类型\n1 2 3 4 5 6 class ModelData(object): INPUT_NAME = \u0026#34;data\u0026#34; INPUT_SHAPE = (1, 1, 28, 28) // [B, C, H, W] OUTPUT_NAME = \u0026#34;prob\u0026#34; OUTPUT_SIZE = 10 DTYPE = trt.float32 从engine创建执行上下文:engine-\u0026gt;createExecutionContext()\n创建CUDA流cudaStreamCreate(\u0026amp;stream)：\nCUDA编程流是组织异步工作的一种方式，创建流来确定batch推理的独立 为每个独立batch使用IExecutionContext(3.2中已经创建了)，并为每个独立批次使用cudaStreamCreate创建CUDA流。 数据准备：\n在host上声明input数据和output数组大小，搬运到gpu上 要执行inference，必须用一个指针数组指定input和output在gpu中的指针。 推理并将output搬运回CPU 启动所有工作后，与所有流同步以等待结果:cudaStreamSynchronize\n按照与创建相反的顺序释放内存\n重要接口使用说明：\nTR10中的节点索引变更为字符串，之前是数值 必须使用createNetworkV2,并指定为1（表示显性batch）。createNetwork已经废弃，非显性batch官方不推荐。这个方式直接影响推理时enqueue还是enqueueV2（TR10为V3） builder、config等指针，记得释放，否则会有内存泄漏，使用ptr-\u0026gt;destroy()（TR10使用delete）释放 markOutput表示是该模型的输出节点，mark几次，就有几个输出，addlnput几次就有几个输入。这与推理时相呼应 workspaceSize是工作空间大小，某些layer需要使用额外存储时，不会自己分配空间，而是为了内存复用，直接找tensorRT要workspace空间。 bindings是tensorRT对输入输出张量的描述，bindings = input-tensor + output-tensor。比如input有a, output有b,c, d,那么bindings = [a, b, c, d],bindings[0] = a, bindings[2] = c。此时看到engine- \u0026gt;getBindingDimensions(0)你得知道获取的是什么（TRT10改成了IOTensors） enqueueV2是异步推理，加入到stream队列等待执行。输入的bindings则是tensors的指针（注意是device pointer）。其shape对应于编译时指定的输入输出的shape(这里只演示全部shape静态)（TRT10使用enqueueV3） 动态shape 构建网络时：\n1.1. 必须在模型定义时，输入维度给定为-1，否则该维度不会动态。注意一下两点： 1.1.1. 若onnx文件，则onnx文件打开后应该看到为动态或者-1 1.1.2. 如果你的模型中存在reshape类操作，那么reshape的参数必须随动态进行计算。而大部分时候这都是问题。除非你是全卷积模型，否则大部分时候只需要为batch_size维度设置为动态，其他维度尽量避免设置动态 1.2. 配置profile: 1.2.1. create: builder-\u0026gt;createOptimizationProfile() 1.2.2. set: setDimensions()设置kMIN, kOPT, kMAX的一系列输入尺寸范围 1.2.3. add:config-\u0026gt;addOptimizationProfile(profile);添加profile到网络配置中 推理阶段时：\n2.1. 您需要在选择profile的索引后设置input维度：execution_context-\u0026gt;setBindingDimensions(0, nvinfer1::Dims4(1, 1, 3, 3)); （TR10为setInputShape） 2.1.1. 关于profile索引: 2.1.2. 在运行时，向engine请求绑定维度会返回用于构建网络的相同维度。这意味着，得到的还是动态的维度[-1, in_channel, -1, -1]： 1 2 engine.getBindingDimensions(0) // return [-1, 1, -1, -1] // execution_context-\u0026gt;getTensorShape // TR10接口 获取当前的实际维度，需要查询执行上下文： 1 context.getBindingDimensions(0) // return [3, 1, 3, 3] 检查正确性\n我们通常可以利用pytorch来校验是否发生了错误 ONNX模型操作 代码实战：\npytorch-gen-onnx.py：是之前讲过的从pytorch转换onnx格式的代码。 通过onnx-ml.proto和make-onnx-pb.sh了解onnx的结构 2.1. onnx是基于protobuf来做数据存储和传输,*.proto后缀文件, 其定义是protobuf语法，类似json。 2.2. 对于变量结构、类型等，我们可以参照onnx-ml.proto里面的定义。这个文件有800多行，放心我们只要搞清楚里面的核心部分就行： ModelProto:当加载了一个onnx后，会获得一个ModelProto。它包含一个GraphProto和一些版本，生产者的信息。 GraphProto: 包含了四个repeated数组(可以用来存放N个相同类型的内容，key值为数字序列类型.)。这四个数组分别是node(NodeProto类型)，input(ValueInfoProto类型)，output(ValueInfoProto类型)和initializer(TensorProto类型)； NodeProto: 存node，放了模型中所有的计算节点,语法结构如下： ValueInfoProto: 存input，放了模型的输入节点。存output，放了模型中所有的输出节点； TensorProto: 存initializer，放了模型的所有权重参数 AttributeProto:每个计算节点中还包含了一个AttributeProto数组，用来描述该节点的属性，比如Conv节点或者说卷积层的属性包含group，pad，strides等等； 2.3. 通过protoc编译onnx-ml.proto，产生onnx-ml.pb.cc文件 1 bash make-onnx-pb.sh create-onnx.py 3.1. create-onnx.py直接从构建onnx，不经过任何框架的转换。通过import onnx和onnx.helper提供的make_node，make_graph，make_tensor等等接口我们可以轻易的完成一个ONNX模型的构建。 3.2. 需要完成对node，initializer，input，output，graph，model的填充 3.3. 读懂creat-onnx.py以make_node为例： edit-onnx.py 4.1. 由于protobuf任何支持的语言，我们可以使用[c/c++/python/java/c#等等]实现对onnx文件的读写操作 4.2. 掌握onnx和helper实现对onnx文件的各种编辑和修改 增：一般伴随增加node和tensor 1 2 graph.initializer.append(xxx_tensor) graph.node.insert(0, xxx_node) 删： 1 graph.node.remove(xxx_node) 改： 1 input_node.name = \u0026#39;data\u0026#39; read-onnx.py 5.1 通过graph可以访问参数，数据是以protobuf的格式存储的，因此当中的数值会以bytes的类型保存。需要用np.frombuffer方法还原成类型为float32的ndarray。注意还原出来的ndarray是只读的。 ONNX Parser onnx解析器有两个选项，\nlibnvonnxparser.so或者 onnx-tensorrt parser(源代码)。 使用源代码的目的，是为了更好的进行自定义封装，简化插件开发或者模型编译的过程，更加具有定制化，遇到问题可以调试 onnx-tensorrt parser代码使用：\n什么是onnx: 先看名字：Open Neural Network Exchange(ONNX) 是一个开放的生态系统，使代码不被局限在框架和平台中。 具体一点：onnx可以把你的神经网络模型(PyTroch, TF, Caffe)统统转为标准的ONNX格式(一种protobuf格式)，然后就可在各种平台(云平台, windows, linux)和设备(cpu, gpu, npu)上运行 先看文件gen-onnx.py以pytorch构建的模型为例讲：pytorch模型转onnx格式 构建一个pytorch网络，并声明一个model对象 如果进行推理，将模型设为推理状态：这一点很重要，因为像dropout, batchnorm这样的算子在推理和训练模式下的行为是不同的。 导出为onnx模型：torch.onnx.export() 运行python脚本，生成onnx，在main.cpp中会对其进行解析 1 python gen-onnx.py 运行后的图示： Protobuf则通过onnx-ml.proto编译得到onnx-ml.pb.h和onnx-ml.pb.cc或onnx_ml_pb2.py 然后用onnx-ml.pb.cc和代码来操作onnx模型文件，实现增删改 onnx-ml.proto则是描述onnx文件如何组成的，具有什么结构，他是操作onnx经常参照的东西 再看文件main.cpp讲解如何解析onnx格式 使用onnx解析器：createParser的api在文件NvOnnxParser.h中 在这里使用onnx的结果填充到network中，而手动构建网络则是将输入和算子填入network中，区别如图所示： 导出后，可以使用netron软件进行打开查看:https://github.com/lutzroeder/Netron 除了构建过程的区别，makefile中，库文件也需要加上nvonnxparser： 注意：\nseverity_string 和 log仅是工具函数，无需过分关注 导出TRT模型 为了使用onnx导出网络有三种方式：\n我们使用自带的解析器，libnvonnxparser.so 从源代码编译：onnx-tensorrt，主要protobuf文件： onnx-ml.proto 来源 onnx-operators-ml.proto 来源 利用官方工具trtexec，YOLOv8部署推理案例 Usage: 1 2 3 4 5 /usr/src/tensorrt/bin/trtexec \\ --onnx=yolov8s.onnx \\ --saveEngine=yolov8s.engine \\ --fp16 # or --int8 Plugin 1.如何在pytorch里面导出一个插件 2.插件解析时如何对应，在onnx parser中如何处理 3.插件的creator实现 4.插件的具体实现，继承自IPluginV2DynamicExt 5.插件的序列化与反序列化\n量化 int8量化 对于int8，需要配置setFlag nvinfer1::BuilderFlag::kINT8，并且配置setInt8Calibrator 对于Int8EntropyCalibrator，则需要继承自IInt8EntropyCalibrator2 Int8EntropyCalibrator的作用，是读取并预处理图像数据作为输入 标定的原理，是通过输入标定图像I，使用参数WInt8推理得到输出结果PInt8，然后不断调整WInt8，使得输出PInt8与PFloat32越接近越好 因此标定时通常需要使用一些图像，正常发布时，一般使用100张图左右即可 常用的Calibrator Int8EntropyCalibrator2 熵校准选择张量的比例因子来优化量化张量的信息论内容，通常会抑制分布中的异常值。这是当前推荐的熵校准器。默认情况下，校准发生在图层融合之前。推荐用于基于 CNN 的网络。 Iint8MinMaxCalibrator 该校准器使用激活分布的整个范围来确定比例因子。它似乎更适合NLP任务。默认情况下，校准发生在图层融合之前。推荐用于NVIDIA BERT等网络。 计算机中的float计算量是非常大的，而改成int8后，计算量相比可以提升数倍 对于实际操作时，input[float32], w[int8], bias[float32], output[float32] 步骤如下： input[int8] = to_int8(input[float32]) y[int16] = input[int8] * w[int8] # 此处乘法会由计算机转换为int16，保证精度 output[float32] = to_float32(y[int16]) + bias[float32] 所以整个过程的只是为了减少float32的乘法数量以实现提速 对于to_int8的过程，并不是直接的线性缩放，而是经过KL散度计算最合适的截断点（最大、最小值），进而进行缩放，使得权重的分布尽可能小的被改变 可以参照这个地址：https://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf ","date":"2025-03-02T00:00:00Z","permalink":"https://loveleaves.github.io/p/tensorrt_deploy/","title":"【TensorRT】 TensorRT模型部署介绍"},{"content":"References onnx量化推理例子 onnxruntime docs onnxruntime quantization ONNX Runtime Quantization Example, Pre-processing step Overview Quantization in ONNX Runtime refers to 8 bit linear quantization of an ONNX model.\nDuring quantization, the floating point values are mapped to an 8 bit quantization space of the form: val_fp32 = scale * (val_quantized - zero_point)\nscale is a positive real number used to map the floating point numbers to a quantization space. It is calculated as follows:\nFor asymmetric quantization:\n1 scale = (data_range_max - data_range_min) / (quantization_range_max - quantization_range_min) For symmetric quantization:\n1 scale = max(abs(data_range_max), abs(data_range_min)) * 2 / (quantization_range_max - quantization_range_min) zero_point represents zero in the quantization space. It is important that the floating point zero value be exactly representable in quantization space. This is because zero padding is used in many CNNs. If it is not possible to represent 0 uniquely after quantization, it will result in accuracy errors.\nONNX Quantization introduction ONNX Runtime provides python APIs for converting 32-bit floating point model to an 8-bit integer model, a.k.a. quantization. These APIs include pre-processing, dynamic/static quantization, and debugging.\nPre-processing: Pre-processing is to transform a float32 model to prepare it for quantization. The goal of these steps is to improve quantization quality. Dynamic Quantization: Dynamic quantization calculates the quantization parameters (scale and zero point) for activations dynamically. Static Quantization: quantization parameters are calculated in advance (offline) using a calibration data set. Quantization Debugging: Quantization may negatively affect a model’s accuracy. A solution to this problem is to compare the weights and activations tensors of the original computation graph vs those of the quantized one, identify where they differ most, and avoid quantizing these tensors, or choose another quantization/calibration method. Create Float16 and Mixed Precision Models offical web Converting a model to use float16 instead of float32 can decrease the model size (up to half) and improve performance on some GPUs. There may be some accuracy loss, but in many models the new accuracy is acceptable. Tuning data is not needed for float16 conversion, which can make it preferable to quantization.\nORT model format The ORT format is the format supported by reduced size ONNX Runtime builds. Reduced size builds may be more appropriate for use in size-constrained environments such as mobile and web applications.\nBoth ORT format models and ONNX models are supported by a full ONNX Runtime build.\nONNX onnx tutorial ONNX文件介绍 1、.ONNX的本质，是一种Protobuf格式文件 2、Protobuf则通过onnx-ml.proto编译得到onnx-ml.pb.h和onnx-ml.pb.cc或onnx_ml_pb2.py 3、然后用onnx—ml.pb.cc和代码来操作onnx模型文件，实现增删改 4、onnx—ml.proto则是描述onnx文件如何组成的，具有什么结构，他是操作onnx经常参照的东西 details、concepts ONNX结构 model：表示整个onnx的模型，包含图结构和解析器格式、opset版本、导出程序类型model.graph：表示图结构，通常是我们netron看到的主要结构 model.graph.node:表示图中的所有节点，数组，例如conv、bn等节点就是在这里的，通过input、output 表示节点之间的连接关系 model.graph.initializer:权重类的数据大都储存在这里 model.graph.input：整个模型的输入储存在这里，表明哪个节点是输入节点，shape是多少model.graph.output：整个模型的输出储存在这里，表明哪个节点是输出节点，shape是多少 对于anchorgrid类的常量数据，通常会储存在model.graph.node中，并指定类型为Constant,该类型节点在 netron中可视化时不会显示出来 onnx文件及其结构、正确导出onnx、onnx读取、onnx创建、onnx修改、onnx解析器 ONNX文件操作 code 1.ONNX的主要结构：graph、graph.node、graph.initializer、graph.input、graph.output 2.ONNX的节点构建方式：onnx.helper,各种make函数 3.ONNX的proto文件：onnx-proto2 4.理解模型结构的储存、权重的储存、常量的储存、netron的解读对应到代码中的部分 ONNX模型文件正确导出 自pytorch2.5以后，onnx的导出有两个版本exporter实现：export_simple_model_to_onnx_tutorial\ntorch.onnx.export 对于任何用到shape、size返回值的参数时，例如：tensor.view(tensor.size(0),-1)这类操作，避免直接使用tensor.size的返回值，而是加上int转换，tensor.view(int(tensor.size(0),-1),断开跟踪 对于nn.Upsample或nn.functional.interpolate函数，使用scale_factor指定倍率，而不是使用size参数指定大小 对于reshape、view操作时，—1的指定请放到batch维度。其他维度可以计算出来即可。batch维度禁止指定为大于-1的明确数字 torch.onnx.export指定dynamic_axes参数，并且只指定batch维度，禁止其他动态 使用opset_version=11,不要低于11 避免使用inplace操作，例如y[·,0:2]=y[·,0:2]*2-0.5 尽量少的出现5个维度，例如ShuffleNet Module，可以考虑合并wh避免出现5维 尽量把让后处理部分在onnx模型中实现，降低后处理复杂度 掌握了这些，就可以保证后面各种情况的顺利了 这些做法的必要性体现在，简化过程的复杂度，去掉gather、shape类的节点，很多时候，部分不这么改看似也是可以但是需求复杂后，依旧存在各类问题。按照说的这么修改，基本总能成。做了这些，就不需要使用onnx—simplifer了\n以上导出方法在导出自定义算子后，如果要在onnxruntime导入使用，首先需要再定义onnxruntime算子\ntorch.onnx.export(\u0026hellip;, dynamo=True) using torch.export and Torch FX to capture the graph. It was released with PyTorch 2.5 1 2 3 4 5 6 7 8 9 10 # Export the ONNX model onnx_program = torch.onnx.export(torch_model, example_inputs, dynamo=True) # Optimize the ONNX model # The ONNX model can be optimized with constant folding, and elimination of redundant nodes. onnx_program.optimize() # Save the ONNX model onnx_program.save(\u0026#34;demo.onnx\u0026#34;) # Check the ONNX model onnx_model = onnx.load(\u0026#34;deom.onnx\u0026#34;) onnx.checker.check_model(onnx_model) Example Simple Example The example has three parts:\nPre-processing Quantization Debugging YOLOv8 code Static Quantization ","date":"2025-02-20T00:00:00Z","permalink":"https://loveleaves.github.io/p/onnx_quantization/","title":"【ONNX量化】 ONNX量化推理介绍"},{"content":"计算机体系结构 存储器层次结构设计 分层思想/模型：分层解构，更好地理解，分工明确，可维护性高。 缓存（Cache） 特点\n硬件上：成本高但性能好 使用上：部分内容会被重复利用 应用场景：CPU 性能和Cache Line 算法的六种思想 https://www.cnblogs.com/zhaojinhui/p/18264853\n递归算法(Recursive Algorithm) 递归算法是一种自我调用的算法。\n在解决问题时，它将问题拆分成更小的子问题，并通过调用自己来解决这些子问题。每个子问题又可以进一步拆分，直到达到基本情况，然后逐层返回结果，最终得到整个问题的解决方案。 贪心算法(Greedy Algorithm) 贪心算法是一种通过在每一步选择当前最优解来解决问题的策略。\n它不考虑全局最优解，而是希望通过每次选择局部最优解来达到整体最优解。 分治算法 (Divide and ConquerAlgorithm) 分治算法是一种将复杂问题划分为更小的独立子问题，并对这些子问题进行解决的策略。\n它将问题分解为多个部分，然后对每个部分进行处理，最后将它们合并成最终的解决方案。 回溯算法(Backtracking Algorithm) 回溯算法是一种通过不断尝试所有可能的解决方案，直到找到满足条件的解决方案的方法。\n如果尝试的当前解决方案不满足条件，它会回溯到上一步，并继续尝试其他可能的选择，直到找到解决方案或者确定无解。 枚举算法(Brute Force Algorithm) 枚举算法是一种通过穷举所有可能的解决方案来解决问题的方法。\n它不利用任何特定的策略，而是尝试所有可能的选择，直到找到满足条件的解决方案。 动态规划(Dynamic Programming) 动态规划是一种通过将问题拆分为更小的子问题，并将其解决方案存储起来，避免重复计算来优化求解过程的方法。\n它使用一个表格或数组来保存子问题的解决方案，以便在需要时快速查找和使用。\n通过解决子问题，动态规划能够逐步得到整个问题的解决方案。 ","date":"2025-02-18T00:00:00Z","permalink":"https://loveleaves.github.io/p/cs/","title":"【CS】计算机科学重要思想、成果记录"},{"content":"References Int8量化-介绍 量化方法汇总 从TensorRT与ncnn看CNN卷积神经网络int8量化算法 谷歌量化白皮书：Quantizing deep convolutional networks for efficient inference: A whitepaper、Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference ppq 神经网络 - 量化与部署 模型压缩：模型量化打怪升级之路 - 1 工具篇 神经网络加速基础知识 计算机体系结构/组成原理 主要了解以下部分：\n指令系统 计算机组成原理和结构 流水线技术 指令耗时/热点指令 现代处理器 经典CPU体系结构 x86架构（CISC） 胶水typo，多核心 部分组件公共化，提高集成度 GPU架构 共享指令译码和控制，ALU运行的指令相同（分支发散问题） GPU架构介绍 ASIC专用芯片架构 继续移除非必要指令（浮点、图形支持等） 特定领域设计 异构计算与主从设备交互 找到性能瓶颈（performance bottleneck） 高算力场景=》用ASIC等芯片，提高算力，高延迟 低延迟场景=》用FPGA等芯片，降低延迟，低算力 性能热点分析工具 torch Profiler Nsight Compute Nsight Compute 量化硬件实现 量化算子 基本公式：\n1 2 3 4 float value = 1.0; // 输入值 float scale = 0.1; // 用于缩放输入值（尺度因子） int32 qt_32 = round_fn(value/scale); // 取整 int8 qt_8 = clip(qt_32, Q_MIN, Q_MAX); // 范围截断 取整函数round_fn比较特别，在不同硬件上有不同的取整模式（主要对中间值，如1.5，-2.5等），常见取整模式：\nRound half to even，torch 、 C使用，向偶数方向取整 Round half away from zero，向正负无穷方向取整 Round half toward zero，向0方向取整 Round half down，向下取整 Round half up，向上取整 量化子图与全精度子图（quantized subgraph） 权重是可以直接计算出来的，推理的时候只要计算一下量化算子即可\n通常情况，量化算子全部支持场景： 存在不支持量化算子，可用子图分割分离不支持运算子图分开计算，但会导致访存开销为热点 反量化算子 基本公式：\n1 2 3 Char value = 1; // 量化算子/运算输出值 float scale = 0.1; // 用于缩放输入值（尺度因子） Float deq = (value * scale); 量化模式（量化与反量化） 对称量化：基本量化模式，分布对称 1 2 3 int32 qt_32 = round_fn(value/scale); // 取整 int8 qt_8 = clip(qt_32, Q_MIN, Q_MAX); // 范围截断 // 反量化对应量化反向操作，类似encode《=》decode 非对称量化：充分利用int8数值范围（如relu负数范围） 1 2 int32 qt_32 = round_fn(value/scale) + offset; // 取整 uint8 qt_8 = clip(qt_32, Q_MIN, Q_MAX); // 范围截断 整数量化（power of two）：部分硬件不支持浮点运算，用整数运算替换 1 2 int32 qt_32 = round_fn(value * (2 \u0026lt;\u0026lt; shift)); // 取整，shift-定点位 int8 qt_8 = clip(qt_32, Q_MIN, Q_MAX); // 范围截断 指数量化\u0026hellip; tensor量化与通道量化 以上对称/非对称量化、整数量化中的offset、shift可以整个数据为粒度进行量化（可能数值偏差大，量化差），也可以采用其他粒度进行量化：\ntensor量化（per-tensor）：以单个tensor为粒度 通道量化（per-channel）：以单个channel为粒度 量化计算怎么写 整数运算：在许多硬件上，整数运算的微指令条数和指令吞吐量等可能和浮点差不多甚至比浮点差 访存：量化后数据传输耗时少 向量化技术：SIMD/SIMT，代码向量化网站 量化计算一般是：量化+反量化，目的是为了保证量化计算的逻辑与原来一致 量化乘法（quantized mul） 正常int8计算会溢出，所以先反量化成float计算乘法再量化，即量化计算一般要加上rescale操作\n1 2 3 4 5 6 7 // 原本量化运算 ouput[i][j]=inputa[i][j]*inputb[i][j]; //in/out均为int8 // 如果采用对称量化 ouput[i][j]=clip(round_fn(inputa[i][j]*scale_a * inputb[i][j] * scale_b / scale_c)); // scale 为float 即 ouput[i][j]=quantizied(inputa[i][j] * inputb[i][j] / scale_abc); // scale_abc 可提前算 // 同理，如果采用整数量化 即 ouput[i][j]=quantizied(inputa[i][j] * inputb[i][j] \u0026lt;\u0026lt; round(log2 scale_abc)); // scale_abc 可提前算 量化加法 加法要求两个操作数的scale必须一致 1 2 3 4 5 6 // 原本量化运算 ouput[i][j]=inputa[i][j]+inputb[i][j]; //in/out均为int8 // 如果采用对称量化 ouput[i][j]=clip(round_fn((inputa[i][j]*scale_a + inputb[i][j] * scale_b) / scale_c)); // scale 为float // 这里加法要求scale_a和scale_b必须一致（两个操作数的scale） 即 ouput[i][j]=quantizied((inputa[i][j] + inputb[i][j]) / scale_ab); // scale_ab 可提前算 量化激活函数 要求输入输出的scale必须一致 1 2 3 4 5 6 7 // 原本clip量化运算 ouput[i][j]=max(inputa[i][j], min); //in/out均为int8 // 如果采用对称量化 ouput[i][j]=clip(round_fn((inputa[i][j]*scale_in + min) / scale_out)); // scale 为float // 这里加法要求scale_in和scale_out必须一致 即 ouput[i][j]=inputa[i][j] + min / scale_in; // 注意这里没有round_fn、clip操作，min被动量化 // 这时这类算子被称为被动量化算子，如clip、relu、concat等 量化矩阵乘（quantized Gemm） int8输入=》int16/32计算乘法=》int32/64保存求和结果=》量化为int8输出 量化非线性运算 算子包含非线性运算。如：exp、tanh、sigmoid、softmax等 非线性运算：用int无法替代float计算求得结果 CPU、GPU上，不做量化，以全精度模式运行 FPGA、ASIC、DSP上，不支持浮点运算，需要更改算子计算逻辑，以线性运算拟合或直接查表 计算图 算子 常见算子：https://github.com/onnx/onnx/blob/main/docs/Operators.md 最小调度单位 算子融合加速：减少访存调用栈开销，优化计算逻辑 常见计算图优化（算子融合） 计算图优化实践：https://www.bilibili.com/video/BV1Kr4y1n7cy/\n激活函数融合：Computing Op -\u0026gt; Activation =\u0026gt; ConputAct 常见OP：Conv、ConvTranpose、Gemm 常见Act：Relu、Clip（relu6）、Prelu、Tanh、Sigmoid、Switsh 移除batchnorm和dropout 常量折叠：把常量融合进行计算 矩阵乘融合 conv-add融合：Conv + any =\u0026gt; Y = Wx + (Y2 + B) Conv：Y1=WX+B any：Y2 联合定点 用于支持多后端使用，保留原始计算图信息和量化后的计算图信息\n图调度（Graph Dispatching） 误差分析后发现部分算子的误差较大，可将其单独调度到非量化平台计算 图模式匹配 一个计算图可以表示为一个由节点、边集、输入边、输出边组成的四元组 C = {N, E, I, O}。\n我们往往需要在计算图中寻找指定结构。\n如何用一个严谨的方式定义结构？ 如何设计计算模式匹配法，使得其尽可能高效？ 图模式匹配是量化算法、算子融合、算子调度的基础。 图模式匹配可用方法：子图匹配、遍历模式匹配 例子 想象一个场景，onnx不支持swish算子，其可能用以下算子组合实现： 这样有一个问题，量化时会将这三个算子都量化一遍，但其实只需要量化最后一个mul算子即可。这里就可以利用图模式匹配匹配到这个替代的swish结构，并针对性进行处理。\n1 2 3 4 5 6 7 8 9 // 匹配swish，子图模式匹配 search_engine = SearchableGraph(graph) results = search_engine.pattern_matching( patterns = lambda x: x.is_computing_op(\u0026#39;Sigmoid\u0026#39;, \u0026#39;Mul\u0026#39;), edges = [[0, 1], [1, 2], [0, 2]], exclusive = True ) for computing_op, sigmoid, mul in results: ... 遍历模式匹配 匹配模式：起点表达式=》中继点..=》终点..，自动机 步骤：图拆成树，树拆成链，在每个链上进行模式匹配，期间可用动态规划优化 子图模式匹配 子图同构问题为NP-Hard问题，使用近似算法 避免模式pattern多义性，保持互斥 算子调度 SOI正向传播：从开始算子往后找，可能有多个匹配 正向传播的反方向，从终点算子开始往前找 调度争议区：既可以量化，又不可以量化 调度约束： 激活函数与计算节点保持同一平台 NMS、shape、TOPK、MAX与计算节点保持同一平台 参与图融合的算子保持同一平台 孤立计算节点不量化 多输入算子所有输入同平台 手动调度：权衡精度和速度，考虑硬件支持情况 神经网络部署 运行时（runtime） 实际硬件执行库，针对不同硬件有不同实现\n神经网络部署 各厂商的训练框架、推理框架、硬件厂商 部署流程：训练框架训练模型=》导出统一中间表达模型（可选）=》指定推理框架=》指定硬件执行 部署建议：\n确保你的网络可以被Onnx表示，避免其中出现复杂条件逻辑及循环逻辑。 学会自定义算子，以备不时之需，（包括自定义算子的推理实现）。 避免使用各种小Trick，额外加入的算子很可能会破坏图优化。 神经网络能跑多快是Runtime决定的，神经网络加速应当根据runtime进行。 用一下 Onnx Simplifier。 写一个固定的 batchsize大小（latency和吞吐）。 ONNX部署推理 onnxruntime TensorRT Develop Guide, docs、quantization 连贯量化区：不要在网络中过度使用不可量化算子 网络结构设计、量化点插入不能破坏图融合 Tensor对齐 Profiler工具分析：Nsight System 自定义算子，必要时自己写plugin：https://github.com/NVIDIA/TensorRT/tree/release/10.8/plugin 量化理论分析 量化参数选择 假设 Ln/s用比值来评估量化偏差，忽略实际值的大小 int8实际应为-128，这里为了对称写成-127 注意这里的截断边界条件为.5，如127.5，-127.5，为了尽可能保留原精度 最大值截断 也就是说最大值截断在元素值趋于无限时，会出现误差发散的情况。 分位数截断 实际运用时，结合3-sigma原则取近似sigma值 最优截断 Bernard Widrow公式 最优估计问题：\n最优截断要求pdf的三阶积分，并求导令上式为0，对于大部分分布而言，无法顺利求得解析解。 同时在很多情况下，局部的MSE最优并不是全局MSE最优的。 数据量小时，估计的方差很大。 枚举最优截断 梯度优化截断 量化误差分析 https://www.bilibili.com/video/BV1V94y117Ej/\n量化框架 PPQ PPQ框架介绍 大模型LLM推理加速 TensorRT pytorch-quantization ONNX onnxruntime quantization onnx介绍 NCNN NCNN Conv量化详解 ","date":"2025-02-10T00:00:00Z","permalink":"https://loveleaves.github.io/p/quantization/","title":"【量化】 神经网络量化介绍"},{"content":"设计模式简介 以下引用自菜鸟教程、design pattern\n设计模式（Design pattern）代表了最佳的实践，通常被有经验的面向对象的软件开发人员所采用。设计模式是软件开发人员在软件开发过程中面临的一般问题的解决方案。这些解决方案是众多软件开发人员经过相当长的一段时间的试验和错误总结出来的。\n设计模式是一套被反复使用的、多数人知晓的、经过分类编目的、代码设计经验的总结。使用设计模式是为了重用代码、让代码更容易被他人理解、保证代码可靠性。 毫无疑问，设计模式于己于他人于系统都是多赢的，设计模式使代码编制真正工程化，设计模式是软件工程的基石，如同大厦的一块块砖石一样。项目中合理地运用设计模式可以完美地解决很多问题，每种模式在现实中都有相应的原理来与之对应，每种模式都描述了一个在我们周围不断重复发生的问题，以及该问题的核心解决方案，这也是设计模式能被广泛应用的原因。\n什么是 GOF（四人帮，全拼 Gang of Four）？ 在 1994 年，由 Erich Gamma、Richard Helm、Ralph Johnson 和 John Vlissides 四人合著出版了一本名为 Design Patterns - Elements of Reusable Object-Oriented Software（中文译名：设计模式 - 可复用的面向对象软件元素） 的书，该书首次提到了软件开发中设计模式的概念。\n四位作者合称 GOF（四人帮，全拼 Gang of Four）。他们所提出的设计模式主要是基于以下的面向对象设计原则。\n对接口编程而不是对实现编程。 优先使用对象组合而不是继承。 设计模式之美-王争 知识概览图 课程目录 文章导览 简介 开篇词 | 一对一的设计与编码集训，让你告别没有成长的烂代码！ 01 | 为什么说每个程序员都要尽早地学习并掌握设计模式相关知识？ 02 | 从哪些维度评判代码质量的好坏？如何具备写出高质量代码的能力？ 03 | 面向对象、设计原则、设计模式、编程规范、重构，这五者有何关系？ 04 | 理论一：当谈论面向对象的时候，我们到底在谈论什么？ 05 | 理论二：封装、抽象、继承、多态分别可以解决哪些编程问题？ 06 | 理论三：面向对象相比面向过程有哪些优势？面向过程真的过时了吗？ 07 | 理论四：哪些代码设计看似是面向对象，实际是面向过程的？ 08 | 理论五：接口vs抽象类的区别？如何用普通的类模拟抽象类和接口？ 09 | 理论六：为什么基于接口而非实现编程？有必要为每个类都定义接口吗？ 10 | 理论七：为何说要多用组合少用继承？如何决定该用组合还是继承？ 11 | 实战一（上）：业务开发常用的基于贫血模型的MVC架构违背OOP吗？ 12 | 实战一（下）：如何利用基于充血模型的DDD开发一个虚拟钱包系统？ 13 | 实战二（上）：如何对接口鉴权这样一个功能开发做面向对象分析？ 14 | 实战二（下）：如何利用面向对象设计和编程开发接口鉴权功能？ 15 | 理论一：对于单一职责原则，如何判定某个类的职责是否够“单一”？ 16 | 理论二：如何做到“对扩展开放、修改关闭”？扩展和修改各指什么？ 17 | 理论三：里式替换（LSP）跟多态有何区别？哪些代码违背了LSP？ 18 | 理论四：接口隔离原则有哪三种应用？原则中的“接口”该如何理解？ 19 | 理论五：控制反转、依赖反转、依赖注入，这三者有何区别和联系？ 20 | 理论六：我为何说KISS、YAGNI原则看似简单，却经常被用错？ 21 | 理论七：重复的代码就一定违背DRY吗？如何提高代码的复用性？ 22 | 理论八：如何用迪米特法则（LOD）实现“高内聚、松耦合”？ 23 | 实战一（上）：针对业务系统的开发，如何做需求分析和设计？ 24 | 实战一（下）：如何实现一个遵从设计原则的积分兑换系统？ 25 | 实战二（上）：针对非业务的通用框架开发，如何做需求分析和设计？ 26 | 实战二（下）：如何实现一个支持各种统计规则的性能计数器？ 27 | 理论一：什么情况下要重构？到底重构什么？又该如何重构？ 28 | 理论二：为了保证重构不出错，有哪些非常能落地的技术手段？ 29 | 理论三：什么是代码的可测试性？如何写出可测试性好的代码？ 30 | 理论四：如何通过封装、抽象、模块化、中间层等解耦代码？ 31 | 理论五：让你最快速地改善代码质量的20条编程规范（上） 32 | 理论五：让你最快速地改善代码质量的20条编程规范（中） 33 | 理论五：让你最快速地改善代码质量的20条编程规范（下） 34 | 实战一（上）：通过一段ID生成器代码，学习如何发现代码质量问题 35 | 实战一（下）：手把手带你将ID生成器代码从“能用”重构为“好用” 36 | 实战二（上）：程序出错该返回啥？NULL、异常、错误码、空对象？ 37 | 实战二（下）：重构ID生成器项目中各函数的异常处理代码 38 | 总结回顾面向对象、设计原则、编程规范、重构技巧等知识点 39 | 运用学过的设计原则和思想完善之前讲的性能计数器项目（上） 40 | 运用学过的设计原则和思想完善之前讲的性能计数器项目（下） 41 | 单例模式（上）：为什么说支持懒加载的双重检测不比饿汉式更优？ 42 | 单例模式（中）：我为什么不推荐使用单例模式？又有何替代方案？ 43 | 单例模式（下）：如何设计实现一个集群环境下的分布式单例模式？ 44 | 工厂模式（上）：我为什么说没事不要随便用工厂模式创建对象？ 45 | 工厂模式（下）：如何设计实现一个Dependency Injection框架？ 46 | 建造者模式：详解构造函数、set方法、建造者模式三种对象创建方式 47 | 原型模式：如何最快速地clone一个HashMap散列表？ 48 | 代理模式：代理在RPC、缓存、监控等场景中的应用 49 | 桥接模式：如何实现支持不同类型和渠道的消息推送系统？ 50 | 装饰器模式：通过剖析Java IO类库源码学习装饰器模式 51 | 适配器模式：代理、适配器、桥接、装饰，这四个模式有何区别？ 52 | 门面模式：如何设计合理的接口粒度以兼顾接口的易用性和通用性？ 53 | 组合模式：如何设计实现支持递归遍历的文件系统目录树结构？ 54 | 享元模式（上）：如何利用享元模式优化文本编辑器的内存占用？ 55 | 享元模式（下）：剖析享元模式在Java Integer、String中的应用 56 | 观察者模式（上）：详解各种应用场景下观察者模式的不同实现方式 57 | 观察者模式（下）：如何实现一个异步非阻塞的EventBus框架？ 58 | 模板模式（上）：剖析模板模式在JDK、Servlet、JUnit等中的应用 59 | 模板模式（下）：模板模式与Callback回调函数有何区别和联系？ 60 | 策略模式（上）：如何避免冗长的if-else/switch分支判断代码？ 61 | 策略模式（下）：如何实现一个支持给不同大小文件排序的小程序？ 62 | 职责链模式（上）：如何实现可灵活扩展算法的敏感信息过滤框架？ 63 | 职责链模式（下）：框架中常用的过滤器、拦截器是如何实现的？ 64 | 状态模式：游戏、工作流引擎中常用的状态机是如何实现的？ 65 | 迭代器模式（上）：相比直接遍历集合数据，使用迭代器有哪些优势？ 66 | 迭代器模式（中）：遍历集合的同时，为什么不能增删集合元素？ 67 | 迭代器模式（下）：如何设计实现一个支持“快照”功能的iterator？ 68 | 访问者模式（上）：手把手带你还原访问者模式诞生的思维过程 69 | 访问者模式（下）：为什么支持双分派的语言不需要访问者模式？ 70 | 备忘录模式：对于大对象的备份和恢复，如何优化内存和时间的消耗？ 71 | 命令模式：如何利用命令模式实现一个手游后端架构？ 72 | 解释器模式：如何设计实现一个自定义接口告警规则功能？ 73 | 中介模式：什么时候用中介模式？什么时候用观察者模式？ 74 | 总结回顾23种经典设计模式的原理、背后的思想、应用场景等 75 | 在实际的项目开发中，如何避免过度设计？又如何避免设计不足？ 76 | 开源实战一（上）：通过剖析Java JDK源码学习灵活应用设计模式 77 | 开源实战一（下）：通过剖析Java JDK源码学习灵活应用设计模式 78 | 开源实战二（上）：从Unix开源开发学习应对大型复杂项目开发 79 | 开源实战二（中）：从Unix开源开发学习应对大型复杂项目开发 80 | 开源实战二（下）：从Unix开源开发学习应对大型复杂项目开发 81 | 开源实战三（上）：借Google Guava学习发现和开发通用功能模块 82 | 开源实战三（中）：剖析Google Guava中用到的几种设计模式 83 | 开源实战三（下）：借Google Guava学习三大编程范式中的函数式编程 84 | 开源实战四（上）：剖析Spring框架中蕴含的经典设计思想或原则 85 | 开源实战四（中）：剖析Spring框架中用来支持扩展的两种设计模式 86 | 开源实战四（下）：总结Spring框架用到的11种设计模式 87 | 开源实战五（上）：MyBatis如何权衡易用性、性能和灵活性？ 88 | 开源实战五（中）：如何利用职责链与代理模式实现MyBatis Plugin？ 89 | 开源实战五（下）：总结MyBatis框架中用到的10种设计模式 90 | 项目实战一：设计实现一个支持各种算法的限流框架（分析） 91 | 项目实战一：设计实现一个支持各种算法的限流框架（设计） 92 | 项目实战一：设计实现一个支持各种算法的限流框架（实现） 93 | 项目实战二：设计实现一个通用的接口幂等框架（分析） 94 | 项目实战二：设计实现一个通用的接口幂等框架（设计） 95 | 项目实战二：设计实现一个通用的接口幂等框架（实现） 96 | 项目实战三：设计实现一个支持自定义规则的灰度发布组件（分析） 97 | 项目实战三：设计实现一个支持自定义规则的灰度发布组件（设计） 98 | 项目实战三：设计实现一个支持自定义规则的灰度发布组件（实现） 99 | 总结回顾：在实际软件开发中常用的设计思想、原则和模式 100 | 如何将设计思想、原则、模式等理论知识应用到项目中？ 加餐一 | 用一篇文章带你了解专栏中用到的所有Java语法 加餐二 | 设计模式、重构、编程规范等相关书籍推荐 春节特别加餐 | 王争：如何学习《设计模式之美》专栏？ 加餐三 | 聊一聊Google是如何做Code Review的 加餐四 | 聊一聊Google那些让我快速成长的地方 加餐五 | 听一听小争哥对Google工程师文化的解读 加餐六 | 什么才是所谓的编程能力？如何考察一个人的编程能力？ 加餐七 | 基础学科的知识如何转化成实际的技术生产力？ 加餐八 | 程序员怎么才能让自己走得更高、更远？ 加餐九 | 作为面试官或候选人，如何面试或回答设计模式问题？ 加餐十 | 如何接手一坨烂业务代码？如何在烂业务代码中成长？ 结束语 | 聊一聊机遇、方向、能力、努力！ MVC和DDD 两者对比：业务开发常用的基于贫血模型的MVC架构违背OOP吗\nMVC MVC 模式代表 Model-View-Controller（模型-视图-控制器） 模式。这种模式用于应用程序的分层开发。 https://www.runoob.com/design-pattern/mvc-pattern.html 基于贫血模型的传统的开发模式，是一种彻彻底底的面向过程的编程风格 DDD 领域驱动设计（Domain Driven Design，简称DDD） 基于充血模型的开发模式，面向对象编程风格 软件建模 软件建模介绍 将想法通过模型可视化地表达出来，方便记忆和进一步分析，方便团队/同事交流，口语交流容易失真。 软件建模体现了软件设计的思想，在需求和实现之间架起了一座桥梁，通过模型指导软件系统的具体实现。 模型并不是软件系统的一个完备表示，而是所研究系统的一种抽象。 如何进行软件建模 软件建模原则\n1、选择正确的模型，模型要与现实相联系 2、从不同的视角，使用不同的模型去表示一个系统 3、模型是抽象的，是选取系统某个最显著的特征并进行简化表示，因此需要通过不同的视角采用不同模型表示： **外部视角：**对系统上下文或环境进行建模 **交互视角：**对系统及其环境或者系统的构件之间的交互进行建模，建立用例模型 **结构化视角：**对系统的组织或者系统所处理的数据的结构进行建模，建立静态模型 **行为视角：**对系统的动态行为以及系统如何响应事件进行建模，建立动态模型 软件建模方法 在不同的领域和场景下有不同的软件建模方法，其各自的建模思想和采用的建模工具也不尽相同，如：\n结构化方法（Structured Method） 面向对象方法（Object Oriented Method） 基于构件方法（Component Based Method） 面向服务方法（Service Oriented Method） \u0026hellip; 面向对象软件建模方法 UML介绍 UML：Unified Modeling Language（统一建模语言），是面向对象的软件建模工具，使用UML进行建模的作用：\n可以更好的理解问题 可以及早的发现错误或者被遗漏的点 可以更加方便的进行组员之间的沟通 支持面向对象软件开发建模，可以更好的描述显示编程的情景。 对于复杂的系统来说，如果概要模型做的好，那么整个系统的模型也就很清晰明了。 UML一共有10种图，可分为四大类： 用例图 静态图：类图、对象图、包图 行为图：状态图、活动图、交互图，交互图分为序列图和协作图。 实现图：部署图、构件图 主要包括4种关系: 关联关系(association) 依赖关系(dependency) 泛化关系(generalization) 实现关系(realization) 4种视角 References UML实战教程 UML教程 UML2.5笔记 ＵＭＬ基础与建模实践教程 Tools 在线免费：draw.io UML常用图介绍 假设用UML建模以下场景：\n“机票订购系统是一个允许用户在线查询航班、购票、管理行程及退票的平台。系统区分了访客（未登录用户）与注册用户的功能权限：访客仅能浏览航班信息，而注册用户在登录后，还能进行购票、查看已购票以及退订操作。此外，系统内置了与外部信用评分系统的接口，该接口用于监控用户退票行为，若用户一个月内退票超过两次，其在信用评分系统中的等级会下调，信用等级过低时，系统将限制其继续购票。”\n用例图（Use Case Diagram）\n用例图是用来描述客户的需求，从用户的角度描述系统的功能，并指出系统的执行者，强调谁在使用系统，系统执行者完成了哪些功能。用例图包括角色、用例和关系。 类图（Class Diagram）\n用来展示系统中的类、类之间的关系（如继承、关联、聚合等）。适用于系统设计阶段，帮助开发人员理解系统的数据结构和类之间的关系。 顺序图（时序图，sequence diagram）\n描述对象之间如何通过消息交互以完成特定任务。适用于详细设计阶段，用于展示操作的时间顺序。 状态图（State Diagram）\n描述对象或系统在不同状态之间的转移和条件。适用于描述对象生命周期，特别是在系统的状态变化较为复杂时。 活动图（Activity Diagram）\n描述工作流、业务流程或操作的顺序。适用于系统行为建模、业务流程建模等。 构件图（组件图, Component Diagram）\n用来展示系统的物理组件及它们之间的依赖关系。适用于高层设计阶段，帮助理解系统的模块化结构。 部署图（Deployment Diagram）\n描述系统的硬件架构及其部署情况，显示硬件节点、节点之间的通信和软件组件部署到硬件节点的情况。适用于系统部署和运维阶段。 ","date":"2025-02-09T00:00:00Z","permalink":"https://loveleaves.github.io/p/design_pattern/","title":"【软件设计】 设计模式介绍"},{"content":"ARM SIMD ARM平台基于ARM v7-A架构的ARM Cortex-A系列处理器(Cortex-A5, Cortex-A7,Cortex-A8, Cortex-A9, Cortex-A15)上的NEON加速：\n针对C/C++语言：循环展开等编译优化，-O2启用 针对NEON intrinsics：NEOM SIMD C/C++语言接口，针对架构启用V向量扩展，选择浮点处理器和ABI（application Binary Interface）接口类型 针对汇编语言：内联汇编，直接操作neon指令和寄存器 路线：了解相应编译优化=》使用intrinsic接口，学习对应汇编代码=》内联汇编，在编译器汇编代码基础上（否则可能反优化）学习并优化 references NEON Programmer\u0026rsquo;s Guide Cortex-A Series Programmer\u0026rsquo;s Guide 算子源码 AI算子：腾讯ncnn 数据处理算子：numpy simd 图像处理算子：Nvidia carotene，OpenCV third party 理论学习 指令流水线 经典的五级流水线模型 1、取指（IF）\n以程序计数器（PC）中的内容作为地址，从存储器中取出指令并放入指令寄存器（IR）； PC值加4（假设每条指令占4字节），指向顺序的下一条指令。 2、指令译码/读寄存器周期（ID）\n对指令进行译码，并用IR中的寄存器地址去访问通用寄存器组，读出所需的操作数； 对IR中的立即数进行扩展 3、执行/有效地址计算周期（EX）\nALU对上一个周期中准备好的操作数进行运算或处理。在这个阶段，不同类型的指令进行的操作不同。\n（1）load和store指令：ALB把指令中所指定的寄存器的内容与偏移量相加，形成访存有效地址； （2）寄存器-寄存器 ALU 指令：ALU按照操作码指定的操作对从通用寄存器组中读出的数据进行运算； （3）寄存器-立即数 ALU 指令：ALU按照操作码指定的操作对从通用寄存器组中读出的操作数和指令中给出的立即数进行运算； （4）分支指令：ALU把指令中给出的偏移量与PC值相加，形成转移目标的地址。同时，对在前一个周期读出的操作数进行判断，确定分支是否成功。 4、存储器访问/分支完成周期（MEM）\n（1）load和store指令：load指令根据上一个周期计算出的有效地址从存储器中读出的相应的数据；store把指定的数据写入这个有效地址对应的存储单元。 （2）分支指令：如果分支“成功”，就把前一个周期中计算好的转移目标地址送入PC。分支指令执行完成；否则，就不进行任何操作。 5、写回周期（WB）\n把结果写入通用寄存器组。对于ALU运算来说，这个结果来自ALU，而对于load指令来说，这个结果来自存储器。 SIMD加速原理 《计算机体系结构：量化研究方法》。Neon是ARM平台的SIMD（Single Instruction Multiple Data，单指令多数据流）指令集实现，书中4.1~4.3讨论了SIMD，推荐阅读。 之所以能加速的原因总结：\n（1）通过加长的寄存器减少数据的读取/写入次数，从而减少将数据读入寄存器的时间开销。例如Neon可以一次性将16个int8（16*8=128bit）数据读入专用寄存器，这一次读取时间开销，明显少于16个int8数据一个一个地读入的时间之和。写入同理。（注意不要和cache的减少访存时间的原理混淆。从cache读取余下的第2~第16个int8数据到寄存器仍然是要花费时钟周期的）。 （2）执行SISD（single instruction, Single data，单指令流单数据流，这里可理解为标量计算）指令时，需要完成（时间开销大的）冒险（hazard）检查。既然使用SIMD指令计算，就暗示这些数据之间无依赖性，也就从指令集层面回避了不必要的时间开销。 了解硬件决定的速度极限：Software Optimization Guide 我们可能还要关心，我们所编写的Neon Intrinsics，可以将手头上硬件的性能发挥到多少水平？是否还有提升空间？这些是好问题。\n在讨论一个问题前，先插入一个使笔者拍案叫绝的相关案例：在另一本计算经典《深入理解计算机系统》 （一般简称 CS:APP）的第5章 优化程序性能 中，该书作者考虑若干计算机硬件特性，将矩阵乘法连续优化了6个版本，直至优化到了该x86 CPU的吞吐量上限（注：对于某种指令，延迟latency 主要关注单条该指令的最小执行时间，吞吐量throughout主要关注单位时间内系统（一个CPU核）最多执行多少条该指令。因为AI计算的数据量比较大，我们更关注吞吐量） 回到问题，我们需要知道我们的吞吐量上界是多少。ARM官方为每个CPU架构（手机CPU一般大核是A7X架构，小核是A5X架构）提供对应的Software Optimization Guide，里面有进行各种运算的latency和throughout。以A76架构（采用该架构作为大核架构的CPU例如骁龙855，麒麟980）为例子，从ARM官网下载对应的pdf（https://developer.arm.com/documentation/swog307215/a/?lang=en） 翻到ASIMD（Advance SIMD）那里，就能查阅各条Neon指令相应的latency和throughout。不同架构的吞吐量上界会有所不同，其他架构请自行在ARM官网文档中心下载。 理论数据有了，至于如何通过实验测试峰值，可参考BBuf的文章 如何判断算法是否有可优化空间？ （https://zhuanlan.zhihu.com/p/268925243）\n反汇编分析生成代码质量 可通过反汇编的方式查看Intrinsics 生成的汇编是否满足预期，如果不满足预期则进行手写汇编优化。具体操作可参考梁德澎的文章 移动端arm cpu优化学习笔记第4弹\u0026ndash;内联汇编入门（https://zhuanlan.zhihu.com/p/143328317）\nmaterials （1）研讨会视频 \u0026ldquo;Performance Analysis for Optimizing Embedded Deep Learning Inference Software,\u0026rdquo; a Presentation from Arm - Edge AI and Vision Alliance，建立优化分析思维 （2）研讨会视频 LCU14-504: Taming ARMv8 NEON: from theory to benchmark results （3）研讨会视频 HKG15-408: ARM v8-A NEON optimization （4）Ne10（ARM官方的计算库）：https://github.com/projectNe10/Ne10 （5）Arm Optimized Routines（ARM官方的计算、网络、字符串库）：https://github.com/ARM-software/optimized-routines （6）Neon优化Chromium的案例：https://developer.arm.com/documentation/101964/developer.arm.com NEON 介绍 ARM NEON 是 ARM 架构的一种 SIMD（Single Instruction, Multiple Data）扩展，旨在加速多媒体、数字信号处理（DSP）、图像处理、音视频编解码、加密算法等高并发计算任务。NEON 是 ARMv7 （ARMv7-A只支持单精度，32x64-bit寄存器；Armv8-A AArch64支持双精度，32x128-bit寄存器，针对浮点操作的Vector Floating Point，VFP）及之后版本的处理器的标准扩展，广泛用于智能手机、嵌入式设备、平板电脑以及其他移动设备中，尤其是处理需要并行化的计算密集型应用时，它能显著提高性能。\n重要概念 lane：如一个float32x4_t类型的变量float32x4_t v = {1.0f, 2.0f, 3.0f, 4.0f}，它占用 128 位，存储 4 个 32 位的浮点数，在这个向量寄存器 v 中，每个值依次存储在不同的lane序号为0、1、2、3中。 NEON 寄存器 定义：NEON 使用专门的寄存器来存储向量数据，这些寄存器通常用于处理多个数据元素，ARMv7-A只支持单精度，32x64-bit寄存器；Armv8-A AArch64支持双精度，32x128-bit寄存器。 作用：NEON 寄存器组包含了 128 （Q字母）或 64（D字母） 位宽的寄存器，可以存储多个 8 位、16 位、32 位、64 位整数或浮点数据。 例子： Q0-Q15：128 位宽的 NEON 寄存器，用于存储 8 位、16 位、32 位、64 位的数据（整数或浮点数）。 D0-D15：64 位宽的 NEON 寄存器，也用于存储 64 位数据。 向量和标量操作 定义：NEON 支持对向量（多个元素）和标量（单个元素）进行操作。 作用：标量操作是普通的逐元素操作，而向量操作则允许一次性处理多个数据元素。 例子： vadd.f32：向量浮点加法操作。 vadd.i32：向量整数加法操作。 NEON 数据类型 定义：NEON 支持多种数据类型，包括整数、浮点数、双精度浮点数和混合类型数据。 作用：不同的数据类型适应不同的应用需求，如 8 位整数、32 位浮点数等。 例子： i8, i16, i32, i64：不同宽度的整数类型。 f32, f64：浮点数类型，支持单精度和双精度浮点数。 NEON 指令集 定义：NEON 提供了一组专门的指令来处理数据并执行并行计算。NEON 指令包括加法、乘法、减法、移位、汇聚（归约）、比较、选择、数据类型转换等。 作用：这些指令能够加速处理向量数据，尤其是应用于图像处理、音频处理、视频编解码、加密算法等领域。 例子： vadd：向量加法指令。 vmul：向量乘法指令。 vsub：向量减法指令。 vmax：向量最大值选择指令。 扩展数据类型 定义：NEON 提供了扩展数据类型的支持，如高/低16位扩展、饱和算术、向量数据类型转换等。 作用：这种扩展数据类型用于在计算过程中执行高效的数据操作和转换，避免数据溢出或精度丢失。 例子： vshl：向左移位操作。 vqadd：饱和加法指令，防止数据溢出。 数据载入和存储指令 定义：NEON 提供了一些专门的加载（load）和存储（store）指令，用于从内存中加载数据到寄存器，或将寄存器中的数据存储回内存。 作用：这些指令能够优化内存访问，支持从多个内存地址加载和存储数据。 例子： vld1：加载向量数据指令。 vst1：存储向量数据指令。 数据汇聚和归约操作 定义：NEON 提供了对向量数据的汇聚（归约）操作，例如求和、最大值、最小值等。 作用：这些操作通常用于计算总和、平均值、最大值等统计量，广泛应用于信号处理和数据分析中。 例子： vaddv：对向量元素进行加法归约，返回所有元素的和。 vmaxv：对向量元素进行最大值归约，返回最大值。 条件执行 定义：NEON 支持条件执行，通过设置条件码（flags），可以对某些指令的执行进行条件限制。 作用：可以根据特定的条件执行指令，避免不必要的计算，提高性能。 例子： vsel：根据掩码（mask）选择性地执行指令。 SIMD 聚合指令（广播操作） 定义：NEON 支持广播操作，允许单一标量值扩展到整个向量中。广播操作使得标量与向量的数据处理更加简便。 作用：通过广播操作，标量可以与向量中的每个元素进行计算，提高了指令的灵活性。 例子： vdup：将一个标量值复制到整个向量中。 NEON 浮点数运算 定义：NEON 支持单精度浮点数和双精度浮点数的运算，符合 IEEE 754 标准。 作用：这些浮点数运算指令可用于科学计算、图像处理等应用。 例子： vadd.f32：单精度浮点数向量加法。 vmul.f32：单精度浮点数向量乘法。 数据类型转换 定义：NEON 支持多种类型之间的转换操作，如浮点与整数类型之间的转换。 作用：这种转换对于不同数据类型之间的运算非常重要，可以确保类型匹配并避免数据丢失。 例子： vcvt.f32.s32：将 32 位整数转换为 32 位单精度浮点数。 vcvt.s32.f32：将 32 位单精度浮点数转换为 32 位整数。 向量掩码 定义：NEON 支持通过掩码控制哪些向量元素应该被操作。掩码机制允许在处理多个数据时根据特定条件选择性地操作某些元素。 作用：掩码可以控制并行操作的粒度，提高计算的灵活性。 例子： vmla：向量乘加指令，根据掩码控制哪些元素参与计算。 NEON Intrinsic 兼容armv7和v8（部分指令可能不兼容），所以不同架构之间迁移方便，不需要改代码\nReferences NEON-Intrinsics Neon Programmer Guide for Armv8-A Coding for Neon intrinsics检索，用来查看接口和支持架构 ARM Neon Intrinsics 学习指北：从入门、进阶到学个通透 numpy simd 数据和计算指令类型的格式 1、向量数据类型格式：\u0026lt;type\u0026gt;\u0026lt;size\u0026gt;x\u0026lt;number of lanes\u0026gt;_t\n比如float32x4_t，=float,=32,=4 向量数据类型： 2、向量数组类型：\u0026lt;type\u0026gt;\u0026lt;size\u0026gt;x\u0026lt;number of lanes\u0026gt;x\u0026lt;length of array\u0026gt;_t\n比如 1 2 3 4 struct int16x4x2_t { int16x4_t val[2]; }; 向量指令格式：\u0026lt;opname\u0026gt;\u0026lt;flags\u0026gt;_\u0026lt;type\u0026gt;\n比如vmulq_f32，=vmul，=q,=f32 Note 普通计算逻辑考虑优化编译器优化、类型量化等 循环一般用do-while的形式 对于非整数倍元素个数的解决方法： leftovers 使用 NEON 的广播操作，避免显示复制数据 使用 NEON 的饱和操作，避免数据溢出 利用数据类型转换操作，并合理进行量化 利用shift、insert、mask等 计算机组成结构运行相关（通用） 并行\n充分利用计算机流水线：去除数据依赖 逻辑操作代替分支选择（分支预测） 数据预加载（预取/并行） 资源利用\n充分利用寄存器资源，分块处理数据，但避免寄存器溢出(Register Spilling）（测试时开启O2优化使编译器允许寄存器存储临时变量） 内存合理对齐分配，按对应寄存器长度读取 多线程处理，如OpenMP（并行/数据共享） 利用数据连续特性、利用cache NEON 汇编 可用__aarch64__宏区分是armv8，否则armv7，针对性编写代码\nReferences 移动端arm cpu优化学习笔记第4弹\u0026ndash;内联汇编入门 arm 内联汇编使用 arm内联汇编的一般格式，detail、docs\n1 2 3 4 5 6 7 8 __asm__ qualifiers ( // 汇编代码部分 : OutputOperands //在内联汇编代码中被修改的变量列表 : InputOperands //在内联汇编代码中用到的变量列表 : Clobbers //在内联汇编代码中用到的寄存器列表 ); Note 先写intrinsic代码反汇编，学习编译器优化后的汇编代码，再优化 重点关注指令流水线排布，避免CPU的Hazard ","date":"2025-02-06T00:00:00Z","permalink":"https://loveleaves.github.io/p/arm-neon/","title":"【SIMD】 ARM SIMD指令集NEON等介绍"},{"content":"References The RISC-V Instruction Set Manual Volume II: Privileged Architecture RVV spec Xuantie+900+Series+RVV-0.7.1+Intrinsic+Manual 算子源码 ARM-software/CMSIS, CMSIS-DSP Nuclei-software/NMSIS Note illegal instruction：修改CSR的mstatus标志位 important concepts VLEN (Vector Length) 定义：向量寄存器的长度，表示每个寄存器可以存储的最大元素数量，通常是硬件设定的，例如 128 位、256 位或 512 位。 作用：决定向量寄存器的容量和能处理的数据量。 例子： 如果 VLEN 为 256 位且每个元素为 32 位整数，则每个寄存器最多存储 8 个元素（256 / 32 = 8）。 SLEN (Stride Length) 定义：元素在内存中的步长，即两个连续元素之间的内存偏移量。 作用：影响内存访问模式，特别是在访问非连续内存时，SLEN 决定了元素之间的间隔。 例子： 假设一个向量寄存器存储 4 个元素，每个元素大小为 32 位，而 SLEN 设置为 2，这意味着每个向量元素在内存中的位置间隔为 2 个 32 位单元。 ELEN (Element Length) 定义：每个向量元素的大小（单位：比特），决定了每个元素占用多少内存。 作用：影响向量中每个元素的数据类型大小，在指令中用e表示，如e32。 例子： 如果 ELEN 设置为 32 位，则每个向量元素为 32 位宽，可以是一个 32 位整数或 32 位浮点数。 如果 ELEN 为 64 位，则每个元素占 64 位，适用于较大数据类型（如 64 位整数或浮点数）。 LMUL (Vector Register Grouping Factor) 定义：向量寄存器的分组因子，控制每个向量寄存器内元素的数量，决定寄存器的并行度。 作用：LMUL 会影响每个向量寄存器中包含的元素数量，从而影响并行性，在指令中用m表示，如m1。 例子： LMUL = 1：每个寄存器存储最大数量的元素（假设 VLEN = 256 位，ELEN = 32 位，则每个寄存器存储 8 个元素）。 LMUL = 2：每个寄存器只存储 4 个元素，寄存器总数增加，适合提高并行度。 LMUL = 4：每个寄存器只存储 2 个元素。 VL (Vector Length Register) 定义：VL 是一个寄存器，用来控制当前向量指令的长度，即当前指令能处理的元素数量。 作用：在 RVV 指令中，VL 决定了向量运算的迭代次数，向量操作将执行 VL 次。 例子： 如果 VL = 4，那么该指令将对前 4 个向量元素执行操作，用setvl(max)指令可以得到指令类型的最大元素数量，其中每个指令指定vl可处理不同数量的元素。 VTYPE (Vector Type Register) 定义：VTYPE 控制向量操作的类型，如元素长度 (ELEN) 和 LMUL 的配置。 作用：配置向量操作的具体参数，帮助硬件理解如何处理向量指令。 例子： VTYPE 设置为 ELEN = 32 位，LMUL = 1，表示每个向量寄存器存储 32 位元素，且每个寄存器的并行度为 1。 Vector Mask (vmsk) 定义：向量掩码用于控制哪些向量元素应该被操作，哪些应该被忽略。 作用：掩码机制使得程序能够选择性地执行向量操作。 例子： 若 vmsk = 11110000（二进制），则只有前 4 个向量元素会被操作，后 4 个元素将被忽略。 Vector Registers (v0 - vn) 定义：向量寄存器用于存储向量数据，RISC-V 定义了 v0 到 v31 的向量寄存器。 作用：这些寄存器用于存储和处理向量数据，数量和大小可由硬件决定。 例子： v0 和 v1 可以分别存储 256 位的向量数据，适用于不同长度的数据类型。 Vector Load/Store Instructions 定义：向量加载和存储指令，用于将数据从内存加载到向量寄存器，或将向量寄存器中的数据存储回内存。 作用：支持各种内存访问模式，如连续或非连续访问。 例子： vlb：加载字节数据到向量寄存器。 vsb：将字节数据存储回内存。 Vector Arithmetic Instructions 定义：向量算术指令用于执行向量加法、减法、乘法、除法等算术运算。 作用：向量算术指令在多核处理器中并行执行运算。 例子： vadd：向量加法，执行两个向量的逐元素加法。 vmul：向量乘法，执行两个向量的逐元素乘法。 Vector Compare Instructions 定义：向量比较指令用于比较向量中的元素，返回布尔掩码结果。 作用：常用于条件判断和控制流。 例子： vseq：判断两个向量的元素是否相等，结果返回掩码。 vsgt：判断向量元素是否大于另一个向量，返回布尔掩码。 Vector Reduction Instructions 定义：向量归约指令用于将向量中的多个元素归约为一个单一结果，如求和、求最大值等。 作用：常用于矩阵运算、图像处理等应用。 例子： vredsum：求和，将向量中所有元素相加。 vredmax：求最大值，返回向量中的最大元素。 Vector Scatter/Gather Instructions 定义：用于从非连续的内存地址中加载数据或将数据存储到非连续的内存地址。 作用：提高对非连续内存的访问效率。 例子： vscatter：将向量元素存储到不连续的内存位置。 vgather：从不连续的内存位置加载数据到向量寄存器。 Vector-Scalar Operations 定义：向量与标量之间的操作，允许标量与每个向量元素进行逐一运算。 作用：通过标量与向量元素的结合，处理常数数据。 例子： vaddvi：将一个标量与向量中的每个元素相加。 vmulvi：将一个标量与向量中的每个元素相乘。 Vector Predication 定义：根据掩码或布尔条件选择性执行向量操作。 作用：通过掩码决定哪些元素进行计算，哪些跳过。 例子： vmand：与掩码进行与运算，满足条件的元素进行计算。 Vector Tail \u0026amp; Masking 定义：当 VL 不能完全填充向量寄存器时，通过尾部掩码控制哪些元素需要操作。 作用：避免浪费计算资源，确保运算的有效性。 例子： 如果 VL = 5，而寄存器有 8 个元素，掩码将控制只操作前 5 个元素。 Vector Unit (VU) 定义：向量单元是硬件中的计算单元，负责执行向量指令。 作用：处理向量计算，提高处理器的并行度。 例子： 在支持 RVV 的处理器中，向量单元可以同时处理多个向量运算。 Note 常见使用方式 以float32类型dot计算为例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 void riscv_dot_prod_f32( const float32_t * pSrcA, const float32_t * pSrcB, uint32_t blockSize, float32_t * result) { float32_t sum = 0.0f; size_t blkCnt = blockSize; size_t l; vfloat32m8_t v_A, v_B; vfloat32m8_t vsum; l = __riscv_vsetvlmax_e32m8(); vsum = __riscv_vfmv_v_f_f32m8(0.0f, l); for (; (l = __riscv_vsetvl_e32m8(blkCnt)) \u0026gt; 0; blkCnt -= l) { v_A = __riscv_vle32_v_f32m8(pSrcA, l); pSrcA += l; v_B = __riscv_vle32_v_f32m8(pSrcB, l); pSrcB += l; vsum = __riscv_vfmacc_vv_f32m8(vsum, v_A, v_B, l); } l = __riscv_vsetvl_e32m8(1); vfloat32m1_t temp00 = __riscv_vfmv_v_f_f32m1(0.0f, l); l = __riscv_vsetvlmax_e32m8(); temp00 = __riscv_vfredusum_vs_f32m8_f32m1(vsum, temp00, l); sum = __riscv_vfmv_f_s_f32m1_f32(temp00); *result = sum; } ","date":"2025-02-06T00:00:00Z","permalink":"https://loveleaves.github.io/p/rvv/","title":"【SIMD】 Risc-v SIMD指令集RVV介绍"},{"content":"环境准备 1.1 Git下载 前往【Git官网】，下载安装程序 一直点下一步，默认安装即可\nHugo下载 前往【Hugo Github Tags】，选择对应版本下载，下载后解压即可 Windows下载版本：hugo_extended_xxxxx_windows_amd64.zip\n搭建博客 创建博客 （1）在hugo.exe所在文件夹的地址栏敲打cmd，然后Enter唤起命令行\n（2）敲打命令hugo new site xxxx创建hugo文件\n（3）敲打命名cd xxxx切换目录，并把hugo.exe复制到刚生成的文件夹中\n（4）敲打命令hugo server -D启动服务，访问http://localhost:1313，Ctrl+C停止服务 （hugo默认是没有主题的，需要进行主题配置）\n配置主题 （1）前往【Hugo Themes】，查找自己喜欢的主题，进行下载\n（2）这边以【Stack主题】为例，将下载好的主题解压，放到/themes文件夹中\n（3）将exampleSite样例数据中的 Content 和 hugo.yaml 复制到主文件夹中，并删掉hugo.toml和content/post/rich-content\n（4）修改 hugo.yaml 中的 theme，将他修改为跟主题文件夹同名\n（5）再次启动hugo服务，查看主题，具体主题配置修改 hugo.yaml，这里不细说，感兴趣可自行查找相关文章\n启用 Giscus 评论 Giscus 是利用 GitHub Discussions 实现的评论系统，开源、无跟踪、无广告、永久免费。\nHugo 对 Giscus 有很好的支持，在 hugo-theme-jane 主题中配置启用Giscus 很简单。\n要启用 Giscus 请先确保：\n仓库是公开的，否则访客将无法查看 discussions。 giscus app 已安装，否则访客将无法评论和回应。 Discussions 功能已在你的仓库中启用。 前面搭建的博客仓库就是公开的，满足了第一点，接下来要做的就是安装 Giscus app 和启用 Discussions。\nReferences https://www.codeaer.com/post/enable-giscus-comments-in-hugo/ 配置 Giscus 根据版本有所不同，0.143.1版本可使用以下模板\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # repoId、categoryId参考网址修改：https://giscus.app/zh-CN giscus: repo: \u0026#34;xxx/xxx.github.io\u0026#34; repoId: \u0026#34;xxx\u0026#34; category: \u0026#34;General\u0026#34; categoryId: \u0026#34;xxx\u0026#34; mapping: \u0026#34;pathname\u0026#34; # comment value is the default value strict: 0 reactionsEnabled: 1 # emitMetadata: 0 inputPosition: \u0026#34;top\u0026#34; theme: \u0026#34;dark\u0026#34; lang: \u0026#34;zh-CN\u0026#34; lazyLoading: true crossorigin: \u0026#34;anonymous\u0026#34; Github部署 常规部署 （1）前往【Github官网】，创建仓库 {github用户名}.github.io\n（2）前往Setting -\u0026gt; Pages -\u0026gt; Branch选择main分支，然后保存，会自动开启 https://{github用户名}.github.io 的地址，这地址也是以后访问博客的地址\n（3）回到hugo文件中，执行命令hugo -D，会生成 public 静态资源文件夹\n（4）在 public 执行以下命令上传到github仓库上，第一次上传可能需要输入账号密码\n1 2 3 4 5 6 git init git add . git commit -m \u0026#34;first commit\u0026#34; git branch -M main git remote add origin {你的github仓库地址} git push -u origin main （5）上传成功后访问 https://{github用户名}.github.io，成功搭建属于自己的Hugo博客\nGithub Action自动部署 （1）Github创建一个新的仓库，用于存放Hugo的主文件\n（2）前往Setttings -\u0026gt; Developer Settings -\u0026gt; Personal access tokens，创建一个token(classic)\n（3）token选择永不过期，并勾选 repo 和 workflow 选项\n（4）为保证安全，将生成的token，保存的仓库的变量中，前往Settings -\u0026gt; Secrets and variables -\u0026gt; Actions中设置\n（5）在hugo主文件创建一个.github/workflows/xxxx.yaml文件，将以下内容复制进去，想具体了解更多，可查看【Github Action文档】\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 name: deploy # 代码提交到main分支时触发github action on: push: branches: - main jobs: deploy: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v4 with: fetch-depth: 0 - name: Setup Hugo uses: peaceiris/actions-hugo@v3 with: hugo-version: \u0026#34;latest\u0026#34; extended: true - name: Build Web run: hugo -D - name: Deploy Web uses: peaceiris/actions-gh-pages@v4 with: PERSONAL_TOKEN: ${{ secrets.你的token变量名 }} EXTERNAL_REPOSITORY: 你的github名/你的仓库名 PUBLISH_BRANCH: main PUBLISH_DIR: ./public commit_message: auto deploy （6）在hugo主文件创建.gitignore文件，来避免提交不必要的文件\n1 2 3 4 5 6 7 # 自动生成的文件 public resources .hugo_build.lock # hugo命令 hugo.exe （7）将hugo的主文件上传到仓库，上传成功后会触发Github Action，来自动部署你的静态页面\n1 2 3 4 5 6 git init git add . git commit -m \u0026#34;first commit\u0026#34; git branch -M main git remote add origin {你的github仓库地址} git push -u origin main Reference https://letere-gzj.github.io/hugo-stack/p/hugo/custom-blog/ ","date":"2025-02-05T00:00:00Z","permalink":"https://loveleaves.github.io/p/hugo-blog/","title":"Hugo + Github 免费部署自己的博客"},{"content":"References C进阶：https://www.bookstack.cn/read/whyilearnc/README.md 中科大超算中心资料手册：https://scc.ustc.edu.cn/zlsc/ https://www.zhihu.com/question/33576416 https://heptagonhust.github.io/HPC-roadmap/ HPC指南：https://github.com/l0ngc/hpc-learning 高等数值分析（高性能计算，并行计算）：https://math.ecnu.edu.cn/~jypan/Teaching/ParaComp/ https://developer.nvidia.com/hpc-sdk-downloads 概念 并行计算三要素\n硬件：并行计算机/体系结构 算法：并行算法设计/应用问题的并行度 软件：并行编程环境/Linux/Fortran/C/C++/MPI/OpenMP 代码优化 分支预测：https://blog.csdn.net/yaojingqingcheng/article/details/120913601 simd优化： 简单例子：https://blog.csdn.net/csdn546229768/article/details/128728780 https://blog.csdn.net/yaojingqingcheng/article/details/121616954 数据结构布局优化：https://blog.csdn.net/yaojingqingcheng/article/details/122418208 循环优化技术：输入值嵌入、分支消除、减少子过程调用次数、循环合并、子过程合并、改变循环变量的迭代顺序、改变数组维数、循环展开、循环分块（提高cache命中率，利用cache line） video 利用局部性原理 CPU寄存器、内存、外存：优化缓存，如用restrict关键字说明指针间地址不存在关联。 并行编程 概念 指令并行：CPU流水线 分布式并行：MPI 共享存储式并行：OpenMP、OpenCL、OpenACC SIMD（Single Instruction Multi-Data） SIMD是CPU实现DLP（Data Level Parallelism）的关键 x86架构 SSE指令集（Streaming SIMD Extensions系列），使用XMM寄存器 AVX指令集（Advanced Vector Extensions系列），使用YMM寄存器，相比SSE扩充浮点 arm架构 neon v7/v8 riscv架构 riscv-v 优点：更高速的计算方法 缺点：更高的开发复杂度，专用的CPU组件 SIMT（Single Instruction Multi-Threads） CUDA/ROCM CUDA：NIDIA ROCM：AMD OpenMP（Open Multi-Processing） openMP介绍 高性能计算入门：OpenMP并行编程技术（一）:https://www.bilibili.com/video/BV1ss4y1K7q1?p=1 OpenMP编程三要素： 编译指导（Compiler Directive）：包含并行域指令、工作共享指令、同步指令、数据环境 运行库函数（Runtime Library Routines） 环境变量（Environment Variables） OpenMP模式：fork-join，是针对CPU的并行编程模型，基于线程 硬件内存模型： CPU在主存上有L1、L2、L3多级缓存 L3为多核共有，但L1和L2为每个核心私有，所以存在缓存一致性问题（False Sharing） OpenCL（Open Computing Language） 跨平台 基于C/C++语言 OpenACC 针对GPU，OpenMP模型在GPU上的扩展，支持AMD GPU openACC介绍 MPI MPI，Message Passing Interface，消息传递接口，主要用于进程间的消息传递（或数据传递） MPI介绍 性能分析 程序流程分析 静态分析：即对代码进行数据对象、函数接口封装和调用分析，工具understand 动态分析：即程序实际调用过程中分析执行的函数及流程，工具gprof ","date":"2025-01-09T00:00:00Z","permalink":"https://loveleaves.github.io/p/hpc/","title":"【HPC】高性能计算总结"},{"content":"References C++ Core Guidelines 核心 C++ 语言构造的参考手册 现代 C++ 大典 cs106b C++知识总结 C++ 语言参考 优秀开源 github cpp trending Abseil Abseil 是一个由 Google 开源的 C++/python 库，提供了一组常用的工具和基础设施，用于补充 C++ 标准库中缺失的功能或提高效率。 C++17的string_view前身就是来自absl::string_view abseil-cpp Dlib Dlib 是一个流行的 C++ 库，主要用于机器学习、计算机视觉和图像处理。它提供了丰富的机器学习功能，支持多种算法和优化方法。 dlib XGBoost XGBoost 是一个高效的机器学习库，特别适用于梯度提升决策树（GBDT）算法，广泛用于回归、分类和排序任务。 xgboost Google\u0026rsquo;s glog (Google Logging Library) 功能：glog 是一个 C++ 日志库，提供了强大的日志记录功能，包括日志级别（DEBUG、INFO、WARNING、ERROR 和 FATAL）、日志输出到文件、以及日志格式化。 特点：简单易用，支持日志级别设置、日志输出的细粒度控制，还可以在生产环境中非常高效地记录日志。 链接：glog GitHub spdlog 功能：spdlog 是一个非常快速的 C++ 日志库，旨在提供低延迟、高性能的日志记录功能。 特点：spdlog 提供线程安全的日志记录支持，能够输出到控制台或文件，支持日志级别、日志格式等功能。 链接：spdlog GitHub folly (Facebook Open-source Library) 功能：folly 是 Facebook 开发的一个 C++ 库，包含了许多高性能的组件和工具，适用于大规模系统的开发。它包括内存管理、并发、容器、算法、IO等多个方面的扩展。 特点：folly 提供了大量与系统底层交互的工具，具有很高的性能，适用于对性能要求极高的应用。 链接：folly GitHub Boost 功能：Boost 是一个广泛使用的 C++ 库集合，提供了许多扩展标准库的功能，包括智能指针、正则表达式、线程、文件系统、算法等。 特点：Boost 提供了丰富的功能，经过多年的开发和优化，成为了 C++ 生态中非常重要的工具库之一。很多 C++ 标准库中的特性都源自 Boost（如 std::shared_ptr 和 std::filesystem）。 c++17中的std::filesystem、std::any、std::varient等直接来自于boost中。boost::program_options用于处理控制台的输入参数也是很方便 链接：Boost官网 fmt 功能：fmt 是一个现代化的、快速的格式化库，提供了类似 Python 中的 f-string 或 C# 中的 string interpolation 的功能。 特点：它允许开发者使用更加简洁和类型安全的方式进行字符串格式化。fmt 库的速度非常快，而且 API 设计符合现代 C++ 风格。 链接：fmt GitHub gflags 功能：gflags 是一个 Google 提供的命令行参数解析库，广泛用于解析应用程序启动时的命令行选项。 特点：gflags 提供了易于使用的命令行选项定义和管理功能，支持复杂的命令行解析需求，例如布尔值选项、枚举类型选项等。 链接：gflags GitHub tbb (Threading Building Blocks) 功能：tbb 是 Intel 提供的一个 C++ 并行编程库，旨在帮助开发者利用多核处理器，简化并行编程。 特点：提供了线程池、并行算法和数据结构，能够方便地进行并行化计算，且通过自动负载平衡使多核资源得到高效利用。 链接：TBB GitHub cppcoro 功能：cppcoro 是一个支持 C++20 协程的 C++ 库，提供了多种并发控制结构，如 task 和 awaiter，用于协程的高效实现。 特点：使 C++ 开发者能够高效地编写异步代码，同时保持代码简洁和易于理解。适合需要异步操作的场景。 链接：cppcoro GitHub Eigen 功能：Eigen 是一个高效的 C++ 数学库，专门用于矩阵运算、线性代数和数值计算。 特点：Eigen 提供了一个高性能的模板库，支持多维数组、矩阵操作以及高级的线性代数功能，广泛用于科学计算、机器学习等领域。 链接：Eigen GitHub C++语言参考 类和结构（class and struct） 类成员 override：重写父类虚函数 final：表示一个虚函数不能被进一步重写，或者表示一个类不能被继承 delete：禁用某个函数，如默认构造 default：明确地请求编译器为类生成默认实现 explicit：防止隐式转换调用其他函数 纯虚函数：virtual void doStep() = 0; // 要求派生类必须实现，否则派生类也将变成抽象类，无法实例化。 常成员函数const：只能调用其他常成员函数，不能修改成员变量（除mutable） static静态成员函数：不用实例化，可被直接调用 inline：内联请求，将代码插入调用函数处，较少调用栈开销 noexcept：表示该函数不会抛出异常 mutable：修饰变量可在常成员函数中修改 constexpr：表示该函数在编译时计算结果 friend：友元函数/类，允许外部函数或类在需要时访问类的私有实现 operator ：用于定义或重载类的运算符 自定义迭代器类：通常需重载==、!=、++、*解引用，实现begin、end 三/五/零之法则(rule of three) 因为C++类的特殊成员特性在实际应用可能产生用户非预期的效果，所以总结为法则用以规避可能出现的潜在风险 cpp references 资源获取即初始化(Resource Acquisition Is Initialization, RAAI) 与托管语言不同，C++ 没有自动回收垃圾机制，易导致内存泄露 C++ 将资源的生命周期与对象的生命周期所绑定（构造获取资源/析构释放资源，利用了栈上的变量在离开作用域的时候会析构的特性） c++11后的四大smart_point(shared_ptr、unique_ptr、weak_ptr、auto_ptr(在17中废除))采用了这种思想 善于利用析构特性进行自动内存回收管理，如std::lock_guard、Std::make_unique、Std::make_share等 运行时类型识别(Run Time Type Identification，RTTI) c++中RTTI的一些体现typeid、dynamic_cast、type traits 具体可以看runtime的库的函数__RTtypeid，rtti把所需的type_info(不同编译器会有所不同)信息放在vtable前，大概也是dynamic_cast要求父类必须有虚函数的原因吧 注意，取虚函数表地址时 （此处请注意环境在32位和64位下的区别，在32/64位下取对象a(带有虚函数的基类的实例)的首地址(虚函数表地址)有区分，即(int )\u0026amp;a 和 (long )\u0026amp;a的不同，为避免也可直接，（int）(int)(\u0026amp;classname)替换成（intptr_t）(intptr_t)(\u0026amp;classname)）** 用于编译时封装的 Pimpl Pimpl（Pointer to Implementation）是一种设计模式，常用于C++编程中以隐藏类的实现细节。Pimpl模式通过将实现细节移到一个私有的实现类中，从而提高代码的可维护性、降低编译时间以及实现二进制兼容性。\n编译依赖项的最小化。 接口和实现的分离。 可移植性。 一般实现： 1 2 3 4 5 6 7 // 头文件定义private类指针，源文件进行实现类impl // my_class.h class my_class { // ... all public and protected stuff goes here ... private: class impl; unique_ptr\u0026lt;impl\u0026gt; pimpl; // opaque type here }; 匿名函数 lambda表达式 引用 reference 指针 异常 模板 template 变长参数模板： 1 2 3 4 5 template \u0026lt;typename... Modules\u0026gt; explicit Sequential(Modules \u0026amp;\u0026amp;...modules) { modules_.reserve(sizeof...(Modules)); pushBack(std::forward\u0026lt;Modules\u0026gt;(modules)...); } // 递归展开，调用基础pushBack方法 完美转发：std::forward，保留原来值类型（左值/右值） std::optional：处理可能为null等值情况 std::enable_shared_from_this：在对象的成员函数中获取指向自身的智能指针，增加对象的引用计数，确保对象在异步操作或回调过程中不会被销毁 STL（Standard Template Library，标准模板库） 容器（Containers） vector.reserve 和 resize Vecotr.emplace_back和push_back std::reference_wrapper存储引用 std::initializer_list 轻量级初始化列表，不可修改; 算法（Algorithms） 迭代器（Iterators） 函数对象（Function Objects） C++ 14新特性 cppreference C++ 17新特性 cppreference C++17完全指南 并行算法库 不同编译器对并行算法库的支持: https://en.cppreference.com/w/cpp/compiler_support/17#C.2B.2B17_library_features 支持的算法 依赖tbb 使用示例 1 2 3 4 5 6 7 8 #include \u0026lt;algorithm\u0026gt; #include \u0026lt;execution\u0026gt; // std::sort(begin, end, comp); // origin std::sort(exe_policy, begin, end, comp); // 执行策略(execution policy)可选： // 1、std::execution::seq（顺序执行） // 2、std::execution::par（并行执行） // 3、std::execution::par_unseq（并行和向量化执行） C++ 20新特性 cppreference 其他 宏定义 #、#@、##、VA_ARGS 应用 1 2 3 4 5 6 7 8 9 10 #define Conn(x,y) x##y // 表示x连接y #define ToChar(x) #@x // 给x加上单引号 #define ToString(x) #x // 给x加上双引号 char* str = ToString(123132); // str=\u0026#34;123132\u0026#34;; int n = Conn(123,456); //n=123456; char* str = Conn(\u0026#34;asdf\u0026#34;, \u0026#34;add\u0026#34;) //str = \u0026#34;asdfadf\u0026#34;; char a = ToChar(1); // a=\u0026#39;1\u0026#39;; // char a = ToChar(123); // 编译器报错 #define debug(...) printf(__VA_ARGS__) // 用于宏定义中代表可变参数 运行时类型反射(Run Time Type Reflection, RTTR) 反射是一个进程检查、反省和修改其自身结构和行为的能力 众所周知，java、c#、Go等语言在语言层面支持了反射特性。而c++不支持反射，因为C++没有在语言层面提供返回类的metadata的能力，所以很多属性要靠手动注册，于是乎有人自造轮子搞了个反射机制（UE中的U++通过UHT和UBT来支持反射） ","date":"2025-01-09T00:00:00Z","permalink":"https://loveleaves.github.io/p/c_plus_plus/","title":"【编程语言】 C++高级特性及实战"},{"content":"References mpi tutorial, github OMP、MPI培训文档 并行程序设计导论 介绍 MPI，Message Passing Interface，消息传递接口，主要用于进程间的消息传递（或数据传递） 是一种库描述，不是语言 是一种标准或规范，不是具体的实现（如Intel MPI，OpenMPI等） 是一种消息传递编程模型，并成为这种编程模型的代表和事实上的标准。 并行效率不降反增（加速比下降）：负载不均衡、并行粒度的选择 MPI四类通讯模式 逻辑进程排列：MPI虚拟进程拓扑 ","date":"2025-01-06T00:00:00Z","permalink":"https://loveleaves.github.io/p/mpi/","title":"【HPC】 MPI介绍"},{"content":"References openACC resources OpenACC 笔记 OpenACC Programming and Best Practices Guide openacc-training-materials 介绍 Open Accelerators，OpenACC 编译器：PGI，nvc或nvc++，linux下PGI编译器安装 针对GPU，OpenMP模型在GPU上的扩展，支持AMD GPU 入门例程 示例代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 // 代码 1：test.cpp，使用命令编译：nvc++ -o test -gpu=mem:managed test.cpp #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;openacc.h\u0026gt; // Function to initialize the vectors with values void initialize(std::vector\u0026lt;double\u0026gt;\u0026amp; a, std::vector\u0026lt;double\u0026gt;\u0026amp; b, int n) { for(int i = 0; i \u0026lt; n; ++i) { a[i] = static_cast\u0026lt;double\u0026gt;(i); b[i] = static_cast\u0026lt;double\u0026gt;(2 * i); } } // detect if GPU is actually running void detect_gpu() { double a[100], b[100]; #pragma acc parallel loop for (int i = 0; i \u0026lt; 100; ++i) { if (i == 10) { if (acc_on_device(acc_device_not_host)) printf(\u0026#34;Executing on GPU.\\n\u0026#34;); else printf(\u0026#34;Not executing on GPU.\\n\u0026#34;); } a[i] += b[i]; } } int main() { const int n = 1000000; // Size of the vectors std::vector\u0026lt;double\u0026gt; a(n), b(n), c(n); double *pa = a.data(), *pb = b.data(), *pc = c.data(); // Initialize vectors a and b initialize(a, b, n); detect_gpu(); // Using OpenACC to offload the following computation to an accelerator // and explicitly handle data movement #pragma acc data copyin(pa[0:n], pb[0:n]) copyout(pc[0:n]) { #pragma acc parallel loop for(int i = 0; i \u0026lt; n; ++i) pc[i] = pa[i] + pb[i]; } // Display the first 10 results for(int i = 0; i \u0026lt; 10; ++i) { std::cout \u0026lt;\u0026lt; \u0026#34;c[\u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34;] = \u0026#34; \u0026lt;\u0026lt; c[i] \u0026lt;\u0026lt; std::endl; } } 编译器选项 -ta=tesla: Compiler option to target NVIDIA Tesla GPUs. -Minfo=accel: Provides feedback about the code generated by the compiler. 常用命令 循环\n#pragma acc parallel: GPU 并行运算 #pragma acc kernels: Identifies a code block for parallelization, allowing the compiler to automatically manage parallelism. #pragma acc loop: Used within parallel or kernels regions to indicate loops that should be parallelized. 函数和变量 #pragma acc routine: 让一个函数可以在 GPU 代码中被调用（也可以在 CPU 代码调用）。 #pragma acc declare: Used for declaring variables or creating a data region. 数据传输 #pragma acc data: Manages data movement to and from the GPU. #pragma acc enter data: Specifies data that should be moved to the GPU. #pragma acc exit data: Specifies data to be moved back from the GPU. #pragma acc update: Synchronizes data between the host and the GPU. copy, copyin, copyout, create, present: Clauses for data construct to define how data is handled (e.g., whether it\u0026rsquo;s copied to/from the GPU or just created there). 线程精细控制 gang, worker, vector: Used with loop directive to control how loop iterations are distributed over parallel execution units. collapse(n): Collapses nested loops to enhance parallelism. reduction(operator:list): Performs a reduction operation (like sum, max) across parallel elements. ","date":"2025-01-06T00:00:00Z","permalink":"https://loveleaves.github.io/p/openacc/","title":"【HPC】 OpenACC介绍"},{"content":"References OMP、MPI培训文档 并行程序设计导论 OpenMP tutorial github OpenMP详细介绍专栏 OpenMP official website openMP paper 介绍 OpenMP属于片上通信、跨核并行、共享存储式并行，支持跨平台共享内存方式的多线程编程接口（规范）。\nOpenMP编程三要素 编译指导（Compiler Directive，19）：包含并行域指令、工作共享指令、同步指令、数据环境 运行库函数（Runtime Library Routines，32） 环境变量（Environment Variables，9） Note OpenMP编程模型：内存共享模型，OpenMP是专为多处理器/核，共享内存机器所设计的。底层架构可以是UMA和NUMA。即(Uniform Memory Access和Non-Uniform Memory Access) 基于线程的并行性 显示的控制并行，极高的控制度 共享变量（默认），私有变量（通过private指定），数据竞争（Race Condition）（通过critical和atomic等同步机制） CPU在主存上有L1、L2、L3多级缓存，L3为多核共有，但L1和L2为每个核心私有，所以存在缓存一致性问题（False Sharing） Fork-Join模型：以一个主线程开始，通过fork创建并行线程组，通过join同步合并只留下主线程。 使用 编译 1 2 3 4 #include \u0026lt;omp.h\u0026gt; \u0026gt; gcc -fopenmp test.c \u0026gt; export OMP_NUM_THREADS=n # 指定线程数量，omp_get_num_threads()获取 \u0026gt; ./a.out 编译器指令 OpenMP编译器指令用于各种目的： 产生平行区域 在线程之间划分代码块 在线程之间分配循环迭代 序列化代码段 线程之间的工作同步 格式如下 #pragma omp \u0026lt;directive\u0026gt; [clause[[,] clause] ...] 通用规则：\n区分大小写 指令遵循编译指令的C/C++规则 每个指令只能指定一个指令名 每个指令最多使用一个后续语句，该语句必须是结构化块 通过在指令行末尾用反斜杠（“\\”）转义换行符，可以在后续行上“继续”长指令行 OpenMP基本指令 1、定义并行区域 #pragma omp parallel 用途: 定义一个并行区域，启动多个线程并行执行该区域中的代码。 示例： 1 2 3 4 #pragma omp parallel { // 并行执行的部分 } #pragma omp for 用途: 将循环的迭代分配给多个线程并行执行。 示例： 1 2 3 4 #pragma omp parallel for for (int i = 0; i \u0026lt; n; i++) { // 并行执行的循环体 } #pragma omp single 用途：指定代码块只由第一个到达线程执行，其他线程跳过该代码块。 2、同步机制(Synchronization) #pragma omp critical 用途: 定义一个临界区，保证代码块在同一时刻只被一个线程执行，以防止竞争条件。 1 2 3 4 5 6 7 8 9 10 11 12 13 float res; #pragma omp parallel { float B; int i, id, nthrds; id = omp_get_thread_num0; nthrds=omp_get_num_threads0; for(i=id;i\u0026lt;niters;i+=nthrds){ B= big _job(i); #pragma omp critical //Threads wait their turn. Only one at a time calls consume() res += consume (B); } } #pragma omp barrier 用途: 强制所有线程在此处同步，确保所有线程都执行到这一步后，才继续执行后续代码。 1 2 3 4 5 6 7 #pragma omp parallel { int id=omp_get_thread_num0; A[id] = big_calc1 (id); #pragma omp barrier // 只有当所有线程都到达barrier的时候才会继续运行 B[id] = big_calc2(id, A); } #pragma omp atomic 1 2 3 4 5 6 7 8 #pragma omp parallel { double tmp, B; B= DOITO; tmp = big ugly(B); #pragma omp atomic X+= tmp; } #pragma omp for 1 2 3 4 5 6 7 #pragma omp parallel { #pragma omp for for(i=0;i\u0026lt;n;i++){ // i is private by default do...; } } 3、变量的作用域 shared：默认情况下，并行区域外申明的变量在并行区域中是共享的，可以使用shared子句显式指定变量为共享的。 示例： 1 2 3 4 5 int a; #pragma omp parallel for shared(a) for (int i = 0; i \u0026lt; n; i++) { // a为公有变量 } private：每个线程在并行区域中有自己独立的变量副本，线程之间相互独立，互不干扰。并行区域内申明的变量默认为私有的，并行区域外申明的变量需要显式申明private 示例： 1 2 3 4 5 6 int a; #pragma omp parallel for private(a) for (int i = 0; i \u0026lt; n; i++) { int b; //a,b均为私有变量 } reduction： 用于将每个线程的私有变量在并行区域结束时进行归约（如求和、求最大值等），最终将结果存储到共享变量中。 示例： 1 2 3 4 5 int sum = 0; #pragma omp parallel for reduction(+:sum) for (int i = 0; i \u0026lt; 10; i++) { sum += i; } 4、调度方法 static：静态调度将循环的迭代均匀分配给所有线程，并且相邻的迭代会被分配在同一个线程，分配方式在程序开始执行时就已经确定。 示例: 1 2 3 4 #pragma omp parallel for schedule(static, 3) for (int i = 0; i \u0026lt; n; i++) { // 每个线程执行3个连续的迭代 } dynamic：动态调度在执行时分配迭代，每当一个线程完成当前分配的迭代时，它会动态获取下一个块的迭代。 guided：引导调度是一种动态调度的变体，但块大小（chunk size）随着任务的完成而逐渐减小。 auto：自动调度将调度策略的选择权交给编译器或运行时库，由它们决定最佳的调度方式。 runtime：运行时调度允许在程序运行时通过环境变量设置调度策略。 环境变量 OMP_SCHEDULE：负责规定调度方式。 OMP_NUM_THREADS：设置执行期间要使用的最大线程数。 OMP_PROC_BIND：启用或禁用线程绑定到处理器。有效值为TRUE或FALSE。 OMP_STACKSIZE：控制创建（非主）线程的堆栈大小。 ","date":"2025-01-06T00:00:00Z","permalink":"https://loveleaves.github.io/p/openmp/","title":"【HPC】 OpenMP介绍"},{"content":"References HPC/ML/OS/SW性能工具总结 汇编/嵌入式编程工具 在 Linux 环境下进行汇编或嵌入式编程时，涉及的工具和程序非常广泛，包括编译器、调试工具、构建系统、性能分析工具等。下面是一些常见的汇编或嵌入式编程工具的详细介绍：\n汇编工具 (Assembly Tools) GAS (GNU Assembler) GAS 是 GNU 编译器集合（GCC）的一部分，专门用于将汇编语言转换成机器代码（即目标文件）。它支持多种体系结构，适用于嵌入式系统开发，通常与 GCC 配合使用。\n常用选项：\n-o \u0026lt;file\u0026gt;: 指定输出文件。 -g: 生成调试信息。 -D \u0026lt;macro\u0026gt;: 定义宏。 示例：\n1 as -o main.o main.s 交叉编译工具链 (Cross Compiler Toolchain) GCC (GNU Compiler Collection) GCC 是用于 C、C++、Fortran 等语言的编译器，常用于嵌入式编程中。它可以生成目标平台的代码，并支持交叉编译（cross-compilation），即在一种平台上为另一种平台编译代码。\n常用选项：\n-o \u0026lt;file\u0026gt;: 输出文件。 -mcpu=\u0026lt;target\u0026gt;: 指定目标架构（如 ARM、MIPS）。 -m32 或 -m64: 设置生成的代码是 32 位或 64 位。 示例：\n1 arm-none-eabi-gcc -o my_program.elf main.c Clang Clang 是 LLVM 项目的一部分，作为 GCC 的替代品，Clang 提供了高效的编译功能，并且具有更加友好的错误报告。它同样支持交叉编译，特别适合现代嵌入式编程和集成开发环境（IDE）中使用。\n常用选项：\n-target \u0026lt;target\u0026gt;: 指定目标平台。 -o \u0026lt;file\u0026gt;: 输出文件。 示例：\n1 clang -target arm-none-eabi -o my_program.elf main.c Binutils Binutils 是一组二进制工具，包括汇编器、链接器、调试器等，广泛用于嵌入式系统开发。ld 和 as 工具是其中最常用的，负责汇编、链接以及生成目标文件。\n常用工具：\nas：汇编源文件。 ld：链接目标文件生成最终的可执行文件。 objcopy：将目标文件转换为不同格式。 示例：\n1 2 as -o main.o main.s ld -o my_program.elf main.o 调试工具 (Debugging Tools) GDB (GNU Debugger) GDB 是一个功能强大的调试工具，适用于 C/C++ 等程序的调试。在嵌入式开发中，GDB 通常配合交叉编译工具链和硬件调试器（如 JTAG、SWD）一起使用。\n常用命令：\nrun：启动程序。 break \u0026lt;line\u0026gt;：在指定行设置断点。 step：单步执行，进入函数。 next：单步执行，不进入函数。 示例：\n1 gdb my_program.elf OpenOCD OpenOCD 是一个用于与目标硬件（如 ARM 处理器）进行通信的调试工具。它支持通过 JTAG 或 SWD 接口进行调试，可以与 GDB 配合使用。\n常用命令：\ntargets：列出连接的目标。 reset halt：复位并停止目标。 flash write_image erase \u0026lt;file\u0026gt; 0x0：将固件烧录到设备。 示例：\n1 openocd -f interface/stlink-v2.cfg -f target/stm32f4x.cfg JLink (SEGGER J-Link) JLink 是 SEGGER 提供的一个商业级调试器，支持 JTAG 和 SWD 接口，用于调试各种嵌入式设备。它提供高性能的调试功能，广泛用于工业和开发中。\n主要功能：\n支持高速调试，能够快速读写内存和寄存器。 与多种 IDE（如 Keil、IAR、Eclipse）兼容。 提供强大的脚本支持和自动化功能。 常用命令：\nconnect: 连接目标硬件。 r: 重置目标设备。 loadfile : 加载二进制文件到目标设备。 示例：\n1 JLinkExe -device STM32F407VG -if SWD LLDB LLDB 是 LLVM 提供的调试工具，类似于 GDB，但更加现代化。它具有更高效的性能，特别适用于基于 Clang 的编译工具链。\n常用命令：\nrun：启动程序。 breakpoint set：设置断点。 step：单步执行。 示例：\n1 lldb my_program.elf 构建工具 (Build Tools) Make Make 是一个非常常用的构建工具，使用 Makefile 管理项目的编译过程。它会根据 Makefile 中的规则自动执行编译、链接等操作，尤其适合嵌入式项目。\n常用命令：\nmake：构建项目。 make clean：清理编译结果。 示例：\n1 make -f Makefile CMake CMake 是一个跨平台的自动化构建工具，可以生成适用于不同平台的构建文件（如 Makefile、Ninja 文件等）。它在现代嵌入式开发中非常流行，支持复杂的项目构建配置。\n常用命令：\ncmake .：生成构建文件。 make：执行构建。 示例：\n1 2 cmake . make 其他工具 nm nm 是一个用于列出二进制文件（例如可执行文件、共享库、目标文件等）符号表的工具。它显示文件中定义和引用的符号，包括函数、变量等。通过 nm，用户可以查看符号的类型和地址信息。\n常用选项：\nnm : 列出指定文件的符号。 -g: 只显示全局符号。 -n 或 \u0026ndash;numeric-sort: 按地址排序符号。 示例：\n1 nm my_program|grep func Objdump objdump 用于反汇编和查看目标文件的详细信息。它可以显示汇编代码、符号表、段信息等，帮助开发人员理解程序的低级结构。\n常用命令：\nobjdump -d \u0026lt;file\u0026gt;：反汇编文件。 objdump -t \u0026lt;file\u0026gt;：显示符号表。 示例：\n1 objdump -d my_program Readelf readelf 用于显示 ELF 文件的详细信息，提供比 objdump 更加专注于 ELF 文件结构的查看。它支持查看 ELF 头、段、节区、符号表等信息。\n常用命令：\nreadelf -h \u0026lt;file\u0026gt;：显示 ELF 文件头信息。 readelf -S \u0026lt;file\u0026gt;：显示节区信息。 示例：\n1 readelf -h my_program 热点分析 热点分析通常通过使用 性能分析工具 来实现，工具会提供每个函数、方法、代码块的执行时间、调用次数、CPU 占用率等信息，帮助开发人员识别耗时最多的部分。\n常见的热点分析方法 调用图（Call Graph）：通过调用图分析函数之间的调用关系，找到调用最频繁的部分。 性能剖析（Profiling）：通过工具生成程序运行时的性能数据，分析哪些函数或代码块占用了最多的时间或资源。 热代码路径（Hot Code Path）分析：关注那些频繁执行的路径或分支，优化这些路径的性能。 内存热点分析：分析程序中哪些数据结构或对象频繁创建、销毁，导致内存管理不善或频繁的垃圾回收。 实际操作 函数级分析：分析程序中的每个函数，找出耗时最多的函数并进行优化。 多线程/并发分析：对于并发程序，热点分析还要考虑线程的执行时间、锁竞争和同步问题，识别线程间的性能瓶颈。 内存分析：分析内存的分配和释放，找出内存泄漏或频繁的内存分配导致的性能瓶颈。 性能分析工具 gprof：GNU profile工具 适用语言：C、C++、Pascal、Fortran 介绍：用于程序的性能优化以及程序瓶颈问题的查找和解决。通过分析应用程序运行时产生的“flat profile”，可以得到每个函数的调用次数，每个函数消耗的处理器时间，也可以得到函数的“调用关系图”，包括函数调用的层次关系，每个函数调用花费了多少时间。 缺点：对并行程序支持较差，不能提供细粒度的分析，主要适用于函数级别的性能分析。 使用步骤：\n1、用gcc、g++、xlC编译程序时，使用-pg参数，如：g++ -pg -o test test.cpp。编译器会自动在目标代码中插入用于性能测试的代码片断，这些代码在程序运行时采集并记录函数的调用关系和调用次数，并记录函数自身执行时间和被调用函数的执行时间。 2、执行编译后的可执行程序，如：./test。该步骤运行程序的时间会稍慢于正常编译的可执行程序的运行时间。程序运行结束后，会在程序所在路径下生成一个缺省文件名为gmon.out的文件，这个文件就是记录程序运行的性能、调用关系、调用次数等信息的数据文件。 3、使用gprof命令来分析记录程序运行信息的gmon.out文件，如：gprof test gmon.out。则可以在显示器上看到函数调用相关的统计、分析信息。 Perf 适用语言： C, C++ 平台： Linux 特点： Perf 是 Linux内置的性能分析工具，可用于分析 CPU 使用率、内存访问、系统调用等。它是一个命令行工具。适用于深度的 Linux 系统级性能分析。 缺点：需要一定的学习成本，报告可能较为复杂。 Perf是一个很大的工具，此处仅展示分析某个应用的的用法。 使用步骤（使用gprof的那个可执行文件）：\n1、perf record ./test，部分性能参数需要root权限 2、perf report References\nhttps://www.brendangregg.com/perf.html perf tutorial WSL2安装perf perf原理及火焰图 perf分析c热点函数 perf简单例子-程序调用栈火焰图\n1 2 3 4 5 6 7 8 9 10 11 perf record -F 99 -p 2347 -g -- sleep 30 # perf record表示采集系统事件, 没有使用 -e 指定采集事件, 则默认采集 cycles(即 CPU clock 周期), -F 99 表示每秒 99 次, -p 2347 是进程号, 即对哪个进程进行分析, -g 表示记录调用栈, sleep 30 则是持续 30 秒. # 统计每个调用栈出现的百分比, 然后从高到低排列 perf report -n –stdio # 解析perf收集的信息 perf script -i perf.data \u0026amp;\u0026gt; perf.unfold # 生成折叠后的调用栈 # 使用开源软件：https://github.com/brendangregg/FlameGraph.git ./stackcollapse-perf.pl perf.unfold \u0026amp;\u0026gt; perf.folded # 生成火焰图 ./flamegraph.pl perf.folded \u0026gt; perf.svg perf简单例子-分析热点函数、指令\n1 2 3 4 5 6 7 8 # 通过-g选项保留源代码信息 gcc -g test.c -o test # 通过perf record命令对test程序采样，-g表示采样调用栈 perf record -F 999 ./test # 查看热点分布 perf report # 查看热点函数testA中的热点指令及语句 perf annotate --stdio --symbol=testA Intel VTune Profiler 适用语言： 多语言支持 平台： Windows、Linux 特点： Intel VTune Profiler 是一个功能强大的性能分析工具，可用于分析 CPU 使用率、内存访问、多线程性能等。适用于 Intel 处理器。 可以看到 perf 看不到L3cache 等硬件特性 references\nhttps://www.cnblogs.com/bandaoyu/p/16751995.html https://zhuanlan.zhihu.com/p/12642264312 https://blog.csdn.net/yaojingqingcheng/article/details/120335335 TAU 适用语言： C、C++、python 官网：https://www.cs.uoregon.edu/research/tau/home.php 特点： 是一个面向MPI与OpenMP并行程序的profiler，在目前看到的OpenMPI的Profiler中算是比较健全的一个。相比于Intel的vtune面向OpenMPI的时候会有些限制，TAU可以根据不同的MPI发行版重新编译。 references\nTAU Profiler安装 python 使用性能测试工具TAU测试MPI程序记录 深入解析TAU工具 GPU 分析工具 官方全部工具\ncuda-gdb cuda-gdb -g -G编译选项 Nsight Compute nvprof，计算能力8.0以下使用 注意系统要求（如win11 ws2才支持）：system-requirements、unknown-error-on-device-0-when-running-ncu-on-wsl 用于深入分析单个 CUDA 内核的性能瓶颈，帮助你优化内核代码。 通常，你可以使用 Nsight Systems 先找到瓶颈的 CUDA 内核，然后使用 Nsight Compute 对这些内核进行详细的性能分析。 使用方案：\n1、用户界面：使用ncu-ui命令 2、CLI方式 Nsight System 用于高层次的系统级性能分析，帮助你识别整个应用的瓶颈，例如 GPU 内核启动延迟、数据传输等问题。 UserGuide 使用方案：\n类似Nsight Compute，但支持jupyter等 可以支持应用执行中的很多系统调用情况 ComputeSanitizer https://docs.nvidia.com/compute-sanitizer/ComputeSanitizer/index.html#id1\n功能正确性检查工具，包含：memcheck、racecheck、initcheck、synccheck等 cuda12.0以下内存检查使用CUDA-MEMCHECK，以上使用ComputeSanitizer HPCtoolkit 适用语言： C、C++、CUDA 官网：HPCToolkit 特点：支持CUDA 内存分析工具 gdb：-g源码调试 tsan TSan（ThreadSanitizer）是一个用于检测多线程程序中的 数据竞争 和 线程安全问题 的工具。它是由 Google 开发的，用于帮助开发者发现并修复多线程程序中的并发问题，这些问题可能导致难以复现的错误和难以调试的行为。\n1. 什么是数据竞争？ 数据竞争是指多个线程并发地访问同一块内存区域，并且至少有一个线程在写入该内存区域，而其他线程可能在读或写该内存。数据竞争通常会导致不可预测的程序行为，比如程序崩溃、结果错误等。数据竞争的问题尤其难以发现，因为它们通常依赖于程序执行的特定时序和上下文。\n2. TSan 的功能 ThreadSanitizer 是一种 动态检测工具，它能够监测并发程序中的线程交互，并在检测到数据竞争时，给出详细的报告。它通过 **插桩（Instrumentation） **方式，插入检查代码，追踪每个线程对共享内存的访问，以此来检测潜在的数据竞争。\n具体功能包括：\n检测数据竞争：在多线程程序中，TSan 能够发现不同线程对同一内存位置的并发访问（读-写或写-写），并且报告潜在的数据竞争。 报告细节：当 TSan 检测到数据竞争时，会提供详细的错误报告，包含竞争发生的栈信息、线程信息、访问的内存位置等，帮助开发者定位问题。 跨平台支持：TSan 支持 Linux、macOS、Android 和其他平台，通常与 Clang 和 GCC 编译器兼容。 3. 如何使用 TSan TSan 是通过编译器插件实现的，因此需要在编译程序时启用它。以下是启用 TSan 的基本步骤：\n1 2 3 4 5 6 7 8 9 10 # 1、编译时启用 TSan，使编译器将 TSan 插桩到代码中，在程序运行时启用线程安全检查。 g++ -fsanitize=thread -g test.cpp -o test # 2、运行时，程序将被 TSsan 监控，检测线程间的竞争。 ./test # 如果程序中有数据竞争，TSan 会输出类似以下的报告： ThreadSanitizer: data race in function \u0026#39;foo\u0026#39; at address 0x601000000020 #0 0x7f89b5cb5e6f in foo #1 0x7f89b5cb5e79 in bar #2 0x7f89b5cb5f89 in main TSan 会提供详细的栈跟踪，指出哪些线程、哪些内存地址、在哪些函数中发生了数据竞争。 4. TSan 的工作原理 TSan 通过对程序进行 插桩，在程序中每次内存访问（读/写）时插入检查代码，追踪每个线程对内存的访问。它会记录每个线程对共享内存的访问并进行比较，以判断是否存在数据竞争。\n主要机制：\n内存访问追踪：TSan 会追踪每个线程对内存地址的访问情况，记录访问的时间戳和线程标识。 同步原语检测：TSan 会检查线程之间的同步操作（如 mutex、lock、atomic）是否正确使用，确保线程安全。 数据竞争检测：如果两个线程访问同一内存位置，并且至少一个是写操作，TSan 会标记为潜在的数据竞争。 5. TSan 检测的线程安全问题 除了检测数据竞争，TSan 还可以帮助识别以下并发编程中的常见问题：\n死锁：如果两个线程因相互等待而导致死锁，TSan 也可以通过检测锁的顺序和依赖关系来帮助识别死锁。 非原子操作：在多线程环境中，如果某些操作不是原子的，可能会导致竞态条件。TSan 可以通过对同步操作的检查，帮助发现这些问题。 不当的内存同步：如果线程没有适当的同步机制（如 mutex 或 atomic）来协调对共享数据的访问，可能会出现竞态条件，TSan 会标记这些不安全的内存访问。 6. TSan 的优点和限制\n优点：能够捕获到很难复现的多线程问题，提供详细的报告，包括访问的内存位置、线程栈、数据竞争的上下文，帮助开发者快速定位并修复问题。 限制：会有一定的性能开销（开发阶段使用），可能不会检测所有类型的并发问题，特别是某些边缘情况或者深度依赖于硬件的并发问题。 AddressSanitizer (ASan) ASan（AddressSanitizer）是一个用于检测 内存错误 的强大工具，特别是针对 缓冲区溢出、堆栈溢出、使用后释放（use-after-free）等常见内存问题。ASan 是由 Google 开发的，作为一个 编译时检测工具，它可以在程序运行时检测出许多类型的内存错误，并提供详细的错误报告。ASan 可以用于 C 和 C++ 等语言，广泛应用于开发和测试阶段，帮助开发者发现和修复难以调试的内存错误。\nASan 的功能 ASan 的核心功能是通过 内存访问跟踪 来检测程序中的各种内存错误。它能有效检测以下几类常见的内存问题： 缓冲区溢出（Buffer Overflow）：当程序写入超出分配内存的区域时，会导致数据损坏或程序崩溃。 堆栈溢出（Stack Overflow）：当程序的栈内存超出预定范围时，可能会覆盖局部变量或函数返回地址。 使用后释放（Use-After-Free）：指在内存被释放后，程序仍然访问该内存。 内存泄漏（Memory Leak）：指程序分配了内存但没有释放，导致内存消耗不断增加。 双重释放（Double Free）：指在释放内存后再次释放该内存，可能导致程序崩溃。 未初始化内存读取（Use of Uninitialized Memory）：程序读取未初始化的内存内容，可能导致不可预测的行为。 ASan 的工作原理 ASan 通过 编译器插桩（Instrumenting Compiler）和 运行时库（Runtime Library）的配合工作来检测内存错误。 编译时插桩：在程序的源代码编译过程中，ASan 会插入额外的检查代码，这些代码会在程序运行时检查每个内存访问，确保它们在合法的内存范围内。 内存分配替换：ASan 会替换程序的 内存分配函数（如 malloc、free、new、delete）来监控内存的分配和释放操作。 内存红区（Redzones）：ASan 在每个内存块的前后插入一些特殊的 \u0026ldquo;红区\u0026rdquo;（Redzones），这些区域用于检测 越界访问。如果程序试图访问红区，ASan 会报告错误。 运行时检测：当程序访问非法内存时，ASan 会触发 运行时错误检测，并输出详细的错误信息（如错误的内存地址、堆栈信息等）。 如何使用 ASan 要启用 AddressSanitizer，您需要在编译时添加 -fsanitize=address 选项，并启用调试信息 -g（以便于调试）。 1 g++ -fsanitize=address -g -o test test.cpp ASan 错误报告 当 ASan 检测到内存错误时，它会生成详细的错误报告。该报告通常包含以下信息： 错误类型：如 use-after-free、buffer overflow、stack overflow 等。 错误位置：报告发生错误的内存地址，指出程序在哪里进行非法内存访问。 调用栈：ASan 会提供程序的调用栈信息，帮助开发者快速定位问题。 内存布局：显示内存分配情况，包括程序访问的内存区域和红区位置。 ASan 检测的内存问题 ASan 可以检测的内存问题包括： 堆栈溢出（Stack Overflow）：当局部变量超出栈边界时，ASan 会报告堆栈溢出。 缓冲区溢出（Buffer Overflow）：当访问超出数组或缓冲区的范围时，ASan 会检测到缓冲区溢出。 使用后释放（Use-After-Free）：在内存被释放后，如果程序继续使用该内存，ASan 会报告此错误。 内存泄漏（Memory Leak）：ASan 可以检测到程序中未释放的内存（通过启用 -fsanitize=address 和使用 ASAN_OPTIONS=detect_leaks=1）。 双重释放（Double Free）：当程序尝试两次释放同一块内存时，ASan 会报告此问题。 未初始化内存访问（Use of Uninitialized Memory）：当程序访问未初始化的内存时，ASan 会报告此错误。 ASan 的优点和缺点 优点：高效的内存错误检测、易于使用、详细的错误报告、广泛支持。 缺点：性能开销、仅支持动态检测、依赖编译器。 valgrind 平台：Linux / macOS / Windows（通过 Cygwin） 用途：Valgrind 是一款开源的动态分析工具，广泛用于 内存分析，如查找内存泄漏、内存越界等问题。 功能： Memcheck：用于检测内存泄漏、越界访问和未初始化的内存读取。 Cachegrind：用于缓存行为分析，评估 CPU 缓存的命中率。 Callgrind：支持函数级别的性能分析，提供详细的 CPU 性能数据。 Helgrind：用于检测并发程序中的数据竞争。 适用场景：适用于内存优化、程序调试和多线程程序的性能分析。 使用方式：通过命令行运行程序时加上 valgrind，比如 valgrind \u0026ndash;leak-check=full ./my_program。 优点：强大的内存分析功能，能够检测很多潜在的错误。 缺点：运行时开销较大，程序执行速度可能会减慢。 eBPF (Extended Berkeley Packet Filter) 类型：内核性能分析工具 功能：eBPF 可以用于监控系统的 CPU 使用情况、内存分配、I/O 性能、网络流量 等。 使用场景：eBPF 适用于 Linux 系统的全栈性能分析，特别是在容器化环境中（如 Kubernetes、Docker）。 优点：能够高效且低开销地进行性能分析，实时提供系统各个层次的性能数据。 缺点：需要一定的学习成本，并且工具的设置可能比较复杂。 在现代计算中，性能优化是提高程序效率、响应时间、资源利用率等重要方面的核心工作。不同类型的性能瓶颈可以通过不同的优化策略来解决，常见的优化策略包括并行度优化、数据传输优化、存储器访问优化、向量化优化、负载均衡优化和多线程扩展性优化。下面将详细介绍每个优化策略。\n性能优化策略 1. 并行度优化（Parallelism Optimization） 并行度优化是指将计算任务拆分成多个独立的子任务，利用多核处理器或多台机器的计算能力来加速计算过程。并行度优化主要关注如何高效地将任务分解并利用多个计算资源。\n核心策略： 任务划分：将计算任务划分为多个相对独立的子任务，确保每个子任务都能并行执行。划分可以基于数据分割或功能划分。 并行模型选择：选择合适的并行编程模型，如多线程、分布式计算、SIMD（单指令多数据）等，依赖于硬件架构和应用的需求。 粒度控制：控制任务的划分粒度，避免过多的细小任务带来的上下文切换开销。任务太小可能引发更多的线程启动和调度开销，反而会降低性能。 避免线程同步问题：在并行化时，尽量减少线程间的同步需求，如锁的竞争等，因为锁竞争会增加线程等待时间，影响性能。 实例： 多核处理器利用：将计算密集型任务分配给不同的 CPU 核心。 GPU 加速：使用图形处理单元（GPU）进行并行计算，例如深度学习中广泛使用的并行训练。 2. 数据传输优化（Data Transfer Optimization） 数据传输优化关注的是如何减少计算过程中数据的传输开销，尤其是在多核、多节点或大规模并行计算环境中，数据传输的延迟和带宽限制可能成为性能瓶颈。\n核心策略： 减少数据传输量：尽量减少进程之间、计算节点之间的通信量。可以通过局部计算、减少数据的复制或压缩数据传输来减少带宽消耗。 数据预取：根据访问模式预测数据的需求，提前加载数据到缓存中，从而减少等待时间。 内存映射与共享内存：使用共享内存或内存映射文件来避免频繁的进程间通信，特别是在多进程或多线程的应用程序中。 局部性优化：将数据分配到物理内存的本地区域，减少跨节点或跨芯片的数据传输。 实例： 在多节点集群中，避免每次计算都从主存储器加载大量数据，而是通过缓存和局部数据共享来减少传输。 在 GPU 和 CPU 之间，使用较小的批次数据传输，以减少 GPU 与主机之间的通信开销。 3. 存储器访问优化（Memory Access Optimization） 存储器访问优化主要目的是减少内存访问延迟，提高内存带宽的利用率。内存访问模式的不合理会造成严重的性能瓶颈，尤其是对于大规模数据的计算密集型任务。\n核心策略： 数据局部性优化：通过优化数据访问模式，提高数据在缓存中的命中率。可分为时间局部性（重复访问相同数据）和空间局部性（访问数据时的空间邻近性）。 缓存优化：优化程序数据结构，使数据在缓存中更容易命中，从而减少访问主内存的次数。使用预取技术和合理的缓存对齐可以显著提高缓存命中率。 避免不必要的内存访问：减少冗余的内存访问，如不必要的内存复制或多次访问相同的数据。 非一致性存储模型优化：在多处理器系统中，保持各个缓存一致性可能导致额外的开销，优化缓存一致性协议和访问策略可以提升性能。 实例： 优化矩阵运算时，按行或按列的顺序访问数据，以提高缓存命中率。 使用 NUMA（Non-Uniform Memory Access）架构时，避免频繁地访问远程内存，尽量保持计算和数据存储在同一个节点的本地内存中。 4. 向量化优化（Vectorization Optimization） 向量化是指将标量操作转换为向量操作，在单条指令中处理多个数据元素。现代处理器，尤其是具有SIMD（单指令多数据）指令集的处理器，能够通过向量化提升计算效率。\n核心策略： 利用SIMD指令：使用 SIMD 指令集（如 AVX、SSE、NEON 等）对数据进行向量化操作，在单个指令周期内处理多个数据元素。 编译器自动向量化：现代编译器（如 GCC、Clang、Intel Compiler）能够自动识别可以向量化的循环，并进行相应优化。 手动优化：在一些复杂的场景中，可以手动编写 SIMD 代码，通过内联汇编或编写特定的 SIMD 库来实现向量化优化。 数据对齐：确保数据存储在合适的内存地址对齐，以便在向量化时避免额外的开销。 实例： 在图像处理、科学计算等应用中，使用 SIMD 向量化技术对每个像素或数据点执行并行操作（如加法、乘法等）。 5. 负载均衡优化（Load Balancing Optimization） 负载均衡优化是指在多处理器、多核心或分布式系统中，合理分配计算任务，以避免某些处理器过载或闲置，从而提高计算资源的利用率。\n核心策略： 任务划分：合理划分任务，将计算负载均匀地分配给不同的计算单元。划分粒度要合适，避免过细的划分导致调度开销。 动态负载均衡：在运行时动态调整任务的分配，以应对负载变化和计算资源的不均衡。例如，在多核环境中，动态地将任务从负载较重的核心转移到空闲的核心上。 数据局部性和负载均衡的结合：在多核或多节点环境中，除了考虑负载均衡，还要考虑任务和数据的局部性，避免数据传输引发的性能瓶颈。 实例： 在分布式计算中使用负载均衡策略，避免某些计算节点过载，其他节点空闲。 在多核处理器上，使用调度算法动态调整任务负载。 6. 多线程扩展性优化（Multithreading Scalability Optimization） 多线程扩展性优化关注的是如何使程序在多核或多处理器系统上运行时能够保持良好的性能提升，尤其是在线程数增加时，如何避免性能的下降。\n核心策略： 避免线程竞争：合理设计程序，减少线程间的资源竞争。过多的锁、临界区和线程同步会导致线程间的阻塞，从而影响程序的扩展性。 线程数的调优：选择合适的线程数，避免过多线程带来的上下文切换开销。通常，线程数不应超过处理器核心数。 工作窃取（Work Stealing）：在多线程应用中，可以使用工作窃取算法，通过让空闲线程从负载较重的线程中窃取任务，平衡负载，提升扩展性。 任务划分粒度：避免过小或过大的任务粒度，过小的任务会增加线程调度开销，过大的任务则可能导致资源利用率不足。 实例： 在多核机器上，动态调整线程数，以适应任务的计算需求和机器的硬件能力。 在并行计算中，使用线程池和任务队列来有效管理线程的创建和销毁。 环境模拟 docker qemu docs\nqemu安装ARM QEMU启动ARM64 Linux内核 大致思路是： 安装qemu-system-aarch64（ARM-64bit）模拟器； 安装aarch64-linux-gnu（ARM-64bit）交叉编译器； 交叉编译linux源码，得到ARM64 Linux内核镜像； 交叉编译busybox源码，使用busybox制作initramfs； 最后使用qemu-system-aarch64启用ARM64 Linux内核； 环境管理/运维 微服务 k8s python conda 其他 内容比较compare Beyond Compare 代码调用关系 cflow：静态分析工具，用来生成 C/C++ 程序的调用图。 Callgrind：动态函数分析 ","date":"2025-01-05T00:00:00Z","permalink":"https://loveleaves.github.io/p/tool/","title":"【Tool】 记录各种用到的工具"},{"content":"介绍 CUDA（Compute Unified Device Architecture，统⼀计算架构）是由 NVIDIA 开发的并行计算平台和编程模型，旨在利用 NVIDIA GPU（图形处理单元）强大的并行计算能力来加速计算密集型任务。CUDA 提供了一种编程接口，让程序员能够直接访问 GPU 上的计算资源。通过并行化计算任务，可以显著提升执行效率。GPU 相较于 CPU，在处理大量并行任务时具有显著的优势，通常拥有成百上千的处理核心（CUDA 核心），能够同时执行大量的操作。\n核心指标：核心数、GPU显存容量、GPU计算峰值、显存带宽 GPU不能单独计算，CPU+GPU组成异构计算架构：CPU起到控制作用，一般称为主机（Host）；GPU可以看作CPU的协处理器，一般称为设备（Device）；主机和设备之间内存访问一般通过PCIe总线链接。 CUDA 提供两层API接口：CUDA驱动(driver)API和CUDA运行时(runtime)API CUDA驱动(driver)API cuda driver使用方式：libcuda.so和cuda.h，cuda-driver-api context管理机制：方便管理device 手动管理的context,cuCtxCreate(手动管理，以堆栈方式push/pop) 自动管理的context,cuDevicePrimaryCtxRetain(自动管理，runtime api以此为基础) 首先需要调用culnit初始化驱动API CUDA运行时(runtime)API cuda runtime使用方式：libcudart.so和cuda_runtime.h。runtime API随cuda toolkit发布 主要内容：核函数的使用、线程束布局、内存模型、流的使用 主要实现：归约求和、仿射变换、矩阵乘法、模型后处理 References 《CUDA 并行程序设计-GPU 编程指南》 第5、6、9章 https://github.com/loveleaves/ML_CPP/tree/main/ParallelFramework/CUDA cuda docs、programming-guide、best-practices-guide CIS 5650-GPU Programming and Architecture CUDA笔记 CUDALibrarySamples CUDA框架 基础编程框架 单文件example.cu编程框架\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 header inclusion const or macro definition declarations of C++ functions and CUDA kernels int main() { allocate host and device memory initialize data in host memory transfer data from host to device launch (call) kernel to do calculations in the device transfer data from device to host free host and device memory } definitions of C++ functions and CUDA kernels 编译指令\n1 nvcc -arch=sm_XY -code=compute_XY -o example example.cu nvcc编译工作原理 host code（standard C/C++ compiler）、device code（compiled into PTX/cubin） CUDA程序兼容性考虑：在将源代码编译为 PTX 代码时，需要用选项-arch=compute_XY指定一个虚拟架构的计算能力，用以确定代码中能够使用的CUDA功能。在将PTX代码编译为cubin代码时，需要用选项-code=sm_ZW指定一个真实架构的计算能力，用以确定可执行文件能够使用的GPU。 https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html Deep Dive into Triton Internals GPU设备设置 1、获取GPU设备数量 1 2 int iDeviceCount = 0; cudaGetDeviceCount(\u0026amp;iDeviceCount); 2、设置GPU执行时使用的设备 1 2 int iDev = 0; cudaSetDevice(iDev) 内存管理 主设内存管理 Note：GPU内存管理runtime接口传入的是双重指针。\n内存分配：malloc、cudaMalloc 数据传递：memcpy、cudaMemcpy 内存初始化：memset、cudaMemset 内存释放：free、cudaFree 主设内存传递\n1 cudaError_t cudaMemcpy(dst, src, count, kind); 枚举类型kind可取值：\ncudaMemcpyHostToHost，表示从主机复制到主机。 cudaMemcpyHostToDevice，表示从主机复制到设备。 cudaMemcpyDeviceToHost，表示从设备复制到主机。 cudaMemcpyDeviceToDevice，表示从设备复制到设备。 cudaMemcpyDefault，表示根据指针dst和src所指地址自动判断数据传输的方向。这要求系统具有统一虚拟寻址（unifiedvirtualaddressing）的功能（要求64位的主机）。 数据同步Synchronize 调用输出函数时，输出流是先存放在缓冲区的，而缓冲区不会自动刷新。只有程序遇到某种同步操作时缓冲区才会刷新。所以当要打印某个数据时，要先使用函数cudaDeviceSynchronize显式地同步主机与设备，促使缓冲区刷新。 核函数（Kernel function） 1、核函数在GPU上进行并行执行 2、注意： （1）限定词__global__ 修饰（可在void前后） （2）返回值必须是void （3）对于N是非blockSize整数倍时，必要时添加if，即使导致条件分支 注意事项：\n1、核函数只能访问GPU内存 2、核函数不能使用变长参数 3、核函数不能使用静态变量 4、核函数不能使用函数指针 5、核函数具有异步性 6、其他：核函数不支持C++的iostream 自定义设备函数 用__global__修饰的函数称为核函数，一般由主机调用，在设备中执行。如果使用动态并行，则也可以在核函数中调用自己或其他核函数。 用__device__修饰的函数叫称为设备函数，只能被核函数或其他设备函数调用，在设备中执行。 用__host__修饰的函数就是主机端的普通C++函数，在主机中被调用，在主机中执行。对于主机端的函数，该修饰符可省略。之所以提供这样一个修饰符，是因为有时可以用__host__和__device__同时修饰一个函数，使得该函数既是一个C++中的普通函数，又是一个设备函数。这样做可以减少冗余代码。编译器将针对主机和设备分别编译该函数。 不能同时用__device__和__global__修饰一个函数，即不能将一个函数同时定义为设备函数和核函数。 也不能同时用__host__和__global__修饰一个函数，即不能将一个函数同时定义为主机函数和核函数。 线程模型 线程的组织结构是由执行配置（executionconfiguration）\u0026laquo;\u0026lt;grid_size,block_size\u0026raquo;\u0026gt;决定的。这里的grid_size（网格大小）和block_size（线程块大小），对应核函数内部的内建变量 gridDim、blockDim、blockIdx、threadIdx 注意GPU系列对应框架最大允许的线程块大小，如1024 线程束：线程调度、管理 CUDA错误检查 运行时错误检测 所有CUDA运行时API函数都是以cuda为前缀的，而且都有一个类型为cudaError_t的返回值，代表了一种错误信息。只有返回值为cudaSuccess时才代表成功地调用了API函数。\n功能正确性检查 内存检查、越界访问、异常检查等 checktool 获得GPU加速的关键 CUDA事件计时 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 cudaEvent_t start, stop; CHECK(cudaEventCreate(\u0026amp;start)); CHECK(cudaEventCreate(\u0026amp;stop)); CHECK(cudaEventRecord(start)); cudaEventQuery(start); // 此处不能用 CHECK 宏函数，对处于TCC 驱动模式的 GPU 来说可以省略，但对处于 WDDM 驱动模式的GPU来说必须保留。 需要计时的代码块 CHECK(cudaEventRecord(stop)); CHECK(cudaEventSynchronize(stop)); float elapsed_time; CHECK(cudaEventElapsedTime(\u0026amp;elapsed_time, start, stop)); printf(\u0026#34;Time = %g ms.\\n\u0026#34;, elapsed_time); CHECK(cudaEventDestroy(start)); CHECK(cudaEventDestroy(stop)); 程序性能分析 Nsight Compute，详见tools\n影响GPU加速的关键因素 数据传输的比例：主设数据传输 算术强度（arithmeticintensity）：计算相比于数据传输耗时的占比 并行规模：数据规模要尽量匹配SM等计算资源 因此, 在编写与优化CUDA程序时，一定要想方设法（主要是指仔细设计算法）做到以下几点\n减少主机与设备之间的数据传输。 提高核函数的算术强度。 增大核函数的并行规模。 CUDA中的数学函数库 https://docs.nvidia.com/cuda/cuda-math-api/\n单精度浮点数内建函数和数学函数（singleprecisionintrinsics and math functions）。使用该类函数时不需要包含任何额外的头文件。 双精度浮点数内建函数和数学函数（doubleprecisionintrinsicsandmathfunctions）。使用该类函数时不需要包含任何额外的头文件。 半精度浮点数内建函数和数学函数（halfprecisionintrinsicsandmathfunctions）。使用该类函数时需要包含头文件\u0026lt;cuda_fp16.h\u0026gt;。 整数类型的内建函数（integerintrinsics）。使用该类函数时不需要包含任何额外的头文件。 类型转换内建函数（typecasting intrinsics）。使用该类函数时不需要包含任何额外的头文件。 单指令-多数据内建函数（SIMDintrinsics）。使用该类函数时不需要包含任何额外的头文件。 内存组织 分层思想，平衡成本和效率（在编码中体现为高内聚、低耦合） https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-variable-specifier 不同硬件架构的内存编排不一定相同 全局内存（global memory） 核函数中的所有线程都能够访问其中的数据，容量是所有设备内存中最大的。基本上就是显存容量。 主要为核函数提供数据，并在主机与设备及设备与设备之间传递数据。 host端访问数据：使用runtime接口cudaGetSymbolAddress() / cudaGetSymbolSize() / cudaMemcpyToSymbol() / cudaMemcpyFromSymbol() 同步函数__syncthreads()：只是针对同一个线程块中的线程的，不同线程块中线程的执行次序依然是不确定的（不同线程块数据要保证不依赖）。 在CUDA中还有一种内部构造对用户不透明的（nottransparent）全局内存，称为CUDAArray。CUDAArray使用英伟达公司不对用户公开的数据排列方式，专为纹理拾取服务。 动态全局内存\n生命周期（lifetime）不是由核函数决定的，而是由主机端决定的（cudaMalloc、cudaFree） 静态全局内存\n静态全局内存变量由以下方式在任何函数外部定义： 1 2 __device__ T x; // 单个变量 __device__ T y[N]; // 固定长度的数组 在核函数中，可直接对静态全局内存变量进行访问，并不需要将它们以参数的形式传给核函数。 常量内存（constant memory） 有常量缓存的全局内存，一共仅有64KB，位于常量内存空间，核函数外部用__constant__定义。 它的可见范围和生命周期与全局内存一样，host端访问数据与全局内存一样。 由于有缓存，常量内存的访问速度比全局内存高，但得到高访问速度的前提是一个线程束中的线程（一个线程块中相邻的32个线程）要读取相同的常量内存数据。 纹理内存（texture memory）和表面内存（surface memory） 类似于常量内存，也是一种具有缓存的全局内存，有相同的可见范围和生命周期，而且一般仅可读（表面内存也可写）。不同的是，纹理内存和表面内存容量更大，而且使用方式和常量内存也不一样。 对于计算能力5.0以上的GPU来说，将某些只读全局内存数据用__ldg()函数通过只读数据缓存（read-onlydatacache）读取，既可达到使用纹理内存的加速效果，又可使代码简洁。对帕斯卡架构和更高的架构来说，全局内存的读取在默认情况下就利用了__ldg()函数，所以不需要明显地使用它。 寄存器（register）和 局部内存（local memory） 存储函数入参、内建变量和临时变量等，32位。 计算能力5.0~9.0的GPU，每个中都是64K的寄存器数量，Fermi架构只有32K； 考虑：每个线程块使用的最大数量、每个线程的最大寄存器数量 局部内存是全局内存的一部分，寄存器溢出是保存在局部内存中。 共享内存（shared memory） 和寄存器类似，存在于芯片上，具有仅次于寄存器的读写速度，extern __shared__ float shared[]定义，数组大小在运行时确定,或__shared__ float shared[100]。 共享内存对整个线程块可见，其生命周期也与整个线程块一致。 一个线程块中的所有线程都可以访问该线程块的共享内存变量副本，但是不能访问其他线程块的共享内存变量副本。 注意避免n路bank冲突（n很大场景，类似TLB组相联）：共享内存在物理上被分为32个（刚好等于一个线程束中的线程数目，即内建变量warpSize的值）同样宽度的、能被同时访问的内存bank。在所有其他架构中，每个bank的宽度为4字节。当同一线程束内的多个线程不同时访问同一个bank中不同层的数据，该线程束对共享内存的访问就只需要一次内存事务（memory transaction）,就会发生bank冲突。 L1 和 L2 缓存 从费米架构开始，有了SM层次的L1缓存和设备（一个设备有多个SM）层次的L2缓存 SM及其占有率 SM（Streaming MultiProcessor）构成\n一个GPU是由多个SM构成的。一个SM包含如下资源（不同架构不一定相同）：\n一定数量的寄存器。 一定数量的共享内存。 常量内存的缓存。 纹理和表面内存的缓存。 L1缓存。 两个（计算能力6.0）或4个（其他计算能力）线程束调度器（warpscheduler），用于在不同线程的上下文之间迅速地切换，以及为准备就绪的线程束发出执行指令。 执行核心，包括： 若干整型数运算的核心（INT32）。 若干单精度浮点数运算的核心（FP32）。 若干双精度浮点数运算的核心（FP64）。 若干单精度浮点数超越函数（transcendentalfunctions）的特殊函数单元（Special Function Units，SFUs）。 若干混合精度的张量核心（tensorcores，由伏特架构引入，适用于机器学习中的低精度矩阵计算）。 SM管理\nGPU中每个SM都可以支持数百个线程并发执行 以线程块block为单位，向SM分配线程块，多个线程块可被同时分配到一个可用的SM上 当一个线程块被分配好后，就不可以在分配到其他上了 线程束（warp）\nCUDA 采用单指令多线程架构管理执行线程，每32个为一组，构成一个线程束。同一个线程块中相邻的 32个线程构成一个线程束 每个线程束中只能包含同一线程块中的线程 线程束是GPU硬件上真正的做到了并行 ** SM 的占有率**\n一般来说，要尽量让SM的占有率不小于某个值，比如%，才有可能获得较高的性能。 SM的理论占有率（theoreticaloccupancy）的两个指标: 一个SM中最多能拥有的线程块个数 一个SM中最多能拥有的线程个数 根据寄存器、共享内存等具体架构具体分析 高效正确地并发并行 原子函数（atomic function） cuda提供原子函数来进行控制数据一致性读写。其中atomicCAS函数是比较特殊的，所有其他原子函数都可以用它实现（指定架构不支持时，但性能可能较差）。\nAtomic APIs with _system suffix (example: atomicAdd_system) are atomic at scope cuda::thread_scope_system if they meet particular conditions. compute capability must greater than 7.2. Atomic APIs without a suffix (example: atomicAdd) are atomic at scope cuda::thread_scope_device. Atomic APIs with _block suffix (example: atomicAdd_block) are atomic at scope cuda::thread_scope_block. 线程束（warp）基本函数 一个SM以32个线程为单位产生、管理、调度、执行线程。这样的32 个线程称为一个线程束。 SM执行属于单指令-多线程（single instruction, multiple thread，SIMT）的执行模式：在同一时刻，一个线程束中的线程只能执行一个共同的指令或者闲置。 在伏特架构之前，一个线程束中的线程拥有同一个程序计数器（programcounter），但各自有不同的寄存器状态（registerstate），从而可以根据程序的逻辑判断选择不同的分支。因此当同一个线程束（不同的不会）中的线程顺序地执行判断语句中的不同分支时，会发生分支发散（branch divergence）。 从伏特架构开始，引入了独立线程调度（independentthreadscheduling）机制。每个线程有自己的程序计数器。这使得伏特架构有了一些以前的架构所没有的新的线程束内同步与通信的模式，但导致： 增加了寄存器负担：单个线程的程序计数器一般需要使用两个寄存器。 独立线程调度机制使得假设了线程束同步（warpsynchronous）的代码变得不再安全：必须显式同步。 线程束内的线程同步函数：都在一个线程束内时，可以将线程块同步函数__syncthreads 换成一个更加廉价的线程束同步函数__syncwarp。 其他基本函数： 线程束表决函数（warpvotefunctions） 线程束匹配函数（warpmatchfunctions） 线程束洗牌函数（warp shuffle functions） 线程束矩阵函数（warp matrix functions） 协作组（cooperativegroups） 类似线程块和线程束同步机制的推广，它提供了更为灵活的线程协作方式，包括线程块内部的同步与协作、线程块之间的（网格级的）同步与协作及设备之间的同步与协作。 https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#introduction-cg CUDA流（CUDA stream） CUDA流介绍 主要用cuda流解决核函数外部的并行：\n核函数计算与数据传输之间的并行。 主机计算与数据传输之间的并行。 不同的数据传输（回顾一下cudaMemcpy函数中的第4个参数）之间的并行。 核函数计算与主机计算之间的并行。 不同核函数之间的并行。 任何CUDA操作都存在于某个CUDA流中，要么是默认流（default stream），也称为空流（null stream），要么是明确指定的非空流。\n在主机端产生与销毁。一个CUDA流由类型为cudaStream_t 的变量表示，cudaStreamCreate和cudaStreamDestroy创建和销毁。 为了实现不同CUDA流之间的并发，主机在向某个CUDA流中发布一系列命令之后必须马上获得程序的控制权，不用等待该CUDA流中的命令在设备中执行完毕。这样，就可以通过主机产生多个相互独立的CUDA流。 检查一个CUDA流中的所有操作是否都在设备中执行完毕：cudaStreamSynchronize同步、cudaStreamQuery查询 默认流（default stream）/ 为空流（null stream） 1 2 3 两种调用方式： my_kernel\u0026lt;\u0026lt;\u0026lt;N_grid, N_block\u0026gt;\u0026gt;\u0026gt;(函数参数); my_kernel\u0026lt;\u0026lt;\u0026lt;N_grid, N_block, N_shared\u0026gt;\u0026gt;\u0026gt;(函数参数); 核函数的启动是异步的（asynchronous），或者说是非阻塞的（non-blocking），所以会host会立即执行下一条语句。该命令如果是CUDA操作不会被device立即执行，因为这是默认流中的CUDA操作，必须等待前一个CUDA操作（即核函数的调用）执行完毕才会开始执行。 可以在核函数启动后放置host操作，利用前面CUDA操作完成时间。 非默认流/非空流 1 2 3 4 调用方式： my_kernel\u0026lt;\u0026lt;\u0026lt;N_grid, N_block, N_shared, stream_id\u0026gt;\u0026gt;\u0026gt;(函数参数); my_kernel\u0026lt;\u0026lt;\u0026lt;N_grid,N_block, 0 ,stream_id\u0026gt;\u0026gt;\u0026gt;(函数参数); // 不使用动态共享内存 # stream_id是CUDA流的编号，N_shared是核函数中使用的动态共享内存的字节数。 用非默认CUDA流重叠核函数的执行与数据传递\n要实现核函数执行与数据传输的并发（重叠），必须让这两个操作处于不同的非默认流，而且数据传输必须使用cudaMemcpy函数的异步版本，即cudaMemcpyAsync函数。异步传输由GPU中的DMA（directmemoryaccess）直接实现，不需要主机参与。 在使用异步的数据传输函数时，需要将主机内存定义为不可分页内存（non-pageable memory）或者固定内存（pinned memory），在程序运行期间，其物理地址将保持不变，由cudaMallocHost和cudaFreeHost申请和释放。 统一内存（unifiedmemory）编程 介绍 统一内存是一种逻辑上的概念，一种系统中的任何处理器（CPU或GPU）都可以访问，并能保证一致性的虚拟存储器。这种虚拟存储器是通过CPU和GPU各自内部集成的内存管理单元（memorymanagementunit）实现的。 使用统一内存对硬件有较高的要求：不低于开普勒架构等。 好处：不用手动内存传输管理；相比手动内存操作可能会有更好的性能；超量分配，类似虚拟内存策略。 基本使用 统一内存在设备中是当作全局内存使用的，而且必须在主机端定义或分配内存，而不能在设备端（核函数和__device__函数）定义或分配内存。 动态申请：cudaMallocManaged 静态申请： __device____managed__int ret[1000]; 数据预取：cudaMemPrefetchAsync 多GPU编程 CUDA标准库 cuda所以接口及库详见官网：cuda docs、cuda developer\nThrust 类似于C++的标准模板库（standard template library）\nthrust、NCCL 数据结构：容器thrust::host_vector\u0026lt;typename\u0026gt;和thrust::device_vector\u0026lt;typename\u0026gt; 算法： 变换（transformation）。本书多次讨论的数组求和计算就是一种变换操作。 规约（reduction）。这是本书重点讨论过的算法。 前缀和（prefixsum）。下一节将详细讨论该算法。 排序（sorting）与搜索（searching）。 选择性复制、替换、移除、分区等重排（reordering）操作。 cuBLAS（basic linear algebra subprograms） 基本线性代数子程序，矩阵在内存中的存储是列主序（column-major order）的Fortran 风格，而不是像C语言中是行主序（row-majororder）的。\ncublas、blas cuBLAS 库包含3个API： cuBLAS API：相关数据必须在设备。 cuBLASXTAPI：相关数据必须在主机。 cuBLASLt API：一个专门处理矩阵乘法的API。 cuFFT 快速傅里叶变换（fast Fourier transforms）\ncufft cuSPARSE 稀疏（sparse）矩阵\ncusparse cusparse提供了一些稀疏矩阵、向量和稠密矩阵、向量的运算函数。 cuSolver 稠密（dense）矩阵和稀疏（sparse）矩阵计算库\ncuSolver 专注于一些比较高级的线性代数方面的计算，如矩阵求逆和矩阵对角化，类似LAPACK库。基于cuBLAS和cuSPARSE两个更基础的库实现。 cusolver、lapack cuSolver 库由以下3个相互独立的子库组成： cuSolverDN（DeNse, DN）：一个处理稠密矩阵线性代数计算的库。 cuSolverSP（SParse, SP）：一个处理稀疏矩阵的线性代数计算的库。 cuSolverRF（ReFactorization, RF）：一个特殊的处理稀疏矩阵分解的库。 cuSolver 库函数倾向于使用异步执行。为了保证一个cuSolver函数的工作已经完成，可以使用cudaDeviceSynchronize() 函数进行同步。 cuRAND 与随机数生成有关的库,包含伪随机数（pseudorandom numbers）和准随机数（quasirandom numbers）。\ncurand cuRand是后向兼容（backward compatible）的，注意cuRAND 和 the CUDA runtime的版本对应 提供了两种API：主机API和设备API。 cuDNN 深度神经网络（deep neural networks）\n是一个用于深度神经网络的 GPU 加速基元库。cuDNN 为标准例程（如前向和后向卷积、注意力、matmul、池化和规范化）提供高度优化的实现。 cudnn docs、cudnn developer ","date":"2025-01-03T00:00:00Z","permalink":"https://loveleaves.github.io/p/gpu/","title":"【GPU】 GPU架构及使用介绍"},{"content":"References openmlsys https://www.zhihu.com/question/26754848 ML Architecture 编程接口 pybind ctypes Computational Graph (计算图) 计算图由基本数据结构张量（Tensor）和基本运算单元算子（operator）构成。在计算图中通常使用节点来表示算子，节点间的有向边（Directed Edge）来表示张量状态，同时也描述了计算间的依赖关系，通常为有向无环图DAG。\n计算图是机器学习框架的核心理念之一，了解主流机器学习框架的设计思想，有助于深入掌握这一概念，建议阅读 TensorFlow 设计白皮书、 PyTorch计算框架设计论文。\n图外控制流直接使用前端语言控制流，熟悉编程语言即可掌握这一方法，而图内控制流则相对较为复杂，建议阅读TensorFlow控制流论文。\n动态图和静态图设计理念与实践，建议阅读TensorFlow Eager 论文、TensorFlow Eager Execution示例、TensorFlow Graph理念与实践、MindSpore动静态图概念。\nTorch JIT\nhttps://github.com/louis-she/torchscript-demos https://zhuanlan.zhihu.com/p/370455320 pytorch2.0新特性：https://www.bilibili.com/video/BV1p84y1675B pytorch计算图例子 How Computational Graphs are Constructed in PyTorch AI编译器和前端技术 编译器前端基础结构\n传统编译器：video AI编译器: summary、video AI System 机器学习编译 主流编译器LLVM Getting Started with LLVM Core Libraries 用LLVM开发新语言 LLVM写个简易编译器1、LLVM写个简易编译器2 编译器后端和运行时 编译器后端总体架构简图\n硬件加速器 算子编译器 当前业界的算子编译器/编译框架主要有\nTVM/Ansor [Zheng et al., 2020] MLIR [Lattner et al., 2020] 华为昇腾芯片上的TBE/AKG [Zhao et al., 2021] 加速器实践 数据处理框架 数据模块的核心组件\n模块设计重点\n易用性 高效性 保序性 AI 推理框架 移动端 腾讯ncnn 模型部署 分布式训练 进阶： 华盛顿大学 Deep Learning Systems，机器学习程序的编译过程,Apache TVM深度学习编译器 AI Systems 微软亚洲研究院 ","date":"2024-12-10T00:00:00Z","permalink":"https://loveleaves.github.io/p/mlsys/","title":"【MLsys】机器学习系统介绍"},{"content":"多线程并行/并发 同一进程各个线程之间共享内存，可用多个线程并行执行，每个线程处理数据或操作的一部分，类似OpenMP。\n线程 操作系统调度的最小单位 每个进程可以有多个线程，线程之间共享进程的内存空间，但有自己的栈、寄存器等 线程切换涉及保存和恢复上下文，内存分配，线程同步等，具有一定的系统开销（注意衡量数据规模对应的线程数量）。 在多核处理器上，线程可以实现真正的并行计算。 在 C 语言中，线程通常通过 POSIX 线程（pthread）库来实现 使用 用多线程优化下面算子：\n1 2 3 4 5 6 7 int sum_array(int *arr, int len) { int sum = 0; for(int i = 0; i \u0026lt; len; ++i) { sum += arr[i]; } return sum; } C例子 优化完成程序：(在链接时指定 -pthread，告诉 GCC 在编译和链接时启用 POSIX 线程支持)\n仅使用多线程 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 #include \u0026lt;pthread.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; typedef struct { int *arr; int start; int end; int partial_sum; } ThreadData; void *sum_partial(void *arg) { ThreadData *data = (ThreadData *)arg; data-\u0026gt;partial_sum = 0; for (int i = data-\u0026gt;start; i \u0026lt; data-\u0026gt;end; ++i) { data-\u0026gt;partial_sum += data-\u0026gt;arr[i]; } return NULL; } int sum_array(int *arr, int len, int num_threads) { pthread_t *threads = malloc(sizeof(pthread_t) * num_threads); ThreadData *thread_data = malloc(sizeof(ThreadData) * num_threads); int chunk_size = len / num_threads; int remainder = len % num_threads; // Create threads to compute partial sums for (int i = 0; i \u0026lt; num_threads; ++i) { thread_data[i].arr = arr; thread_data[i].start = i * chunk_size; thread_data[i].end = (i == num_threads - 1) ? len : (i + 1) * chunk_size; // Handle remainder elements (if any) if (i == num_threads - 1 \u0026amp;\u0026amp; remainder != 0) { thread_data[i].end += remainder; } pthread_create(\u0026amp;threads[i], NULL, sum_partial, \u0026amp;thread_data[i]); } // Wait for all threads to finish and calculate the total sum int total_sum = 0; for (int i = 0; i \u0026lt; num_threads; ++i) { pthread_join(threads[i], NULL); total_sum += thread_data[i].partial_sum; } free(threads); free(thread_data); return total_sum; } int main() { int arr[] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}; int len = sizeof(arr) int num_threads = 4; int result = sum_array(arr, len, num_threads); printf(\u0026#34;Total sum: %d\\n\u0026#34;, result); return 0; } 使用多线程+互斥锁 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;pthread.h\u0026gt; #define NUM_THREADS 4 // 使用4个线程 // 线程参数结构体 typedef struct { int *arr; int start; int end; int *sum; pthread_mutex_t *mutex; } ThreadData; void* thread_sum(void *arg) { ThreadData *data = (ThreadData *)arg; int partial_sum = 0; for (int i = data-\u0026gt;start; i \u0026lt; data-\u0026gt;end; ++i) { partial_sum += data-\u0026gt;arr[i]; } // 使用互斥锁保护共享资源sum pthread_mutex_lock(data-\u0026gt;mutex); *data-\u0026gt;sum += partial_sum; pthread_mutex_unlock(data-\u0026gt;mutex); return NULL; } int sum_array(int *arr, int len, int *sum, pthread_mutex_t *sum_mutex) { pthread_t threads[NUM_THREADS]; ThreadData thread_data[NUM_THREADS]; int chunk_size = len / NUM_THREADS; // 创建线程 for (int i = 0; i \u0026lt; NUM_THREADS; ++i) { thread_data[i].arr = arr; thread_data[i].start = i * chunk_size; thread_data[i].end = (i == NUM_THREADS - 1) ? len : (i + 1) * chunk_size; // 最后一个线程处理剩余部分 thread_data[i].sum = sum; thread_data[i].mutex = sum_mutex; pthread_create(\u0026amp;threads[i], NULL, thread_sum, (void*)\u0026amp;thread_data[i]); } // 等待所有线程完成 for (int i = 0; i \u0026lt; NUM_THREADS; ++i) { pthread_join(threads[i], NULL); } return *sum; } int main() { int arr[] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}; // 示例数组 int len = sizeof(arr) / sizeof(arr[0]); int sum = 0; // 用于存储总和 pthread_mutex_t sum_mutex; // 互斥锁 pthread_mutex_init(\u0026amp;sum_mutex, NULL); // 调用sum_array进行求和 int result = sum_array(arr, len, \u0026amp;sum, \u0026amp;sum_mutex); pthread_mutex_destroy(\u0026amp;sum_mutex); printf(\u0026#34;Sum of array: %d\\n\u0026#34;, result); return 0; } C++ 例子 优化完成程序：(在链接时指定 -pthread，告诉 GCC 在编译和链接时启用 POSIX 线程支持)\n仅使用多线程 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;thread\u0026gt; int sum_array(int *arr, int len) { const int num_threads = 4; int chunk_size = len / num_threads; std::vector\u0026lt;std::thread\u0026gt; threads(num_threads); std::vector\u0026lt;int\u0026gt; partial_sums(num_threads, 0); // 定义一个线程函数，用于计算某个范围内的和 auto sum_part = [\u0026amp;](int thread_id) { int start = thread_id * chunk_size; int end = (thread_id == num_threads - 1) ? len : (thread_id + 1) * chunk_size; for (int i = start; i \u0026lt; end; ++i) { partial_sums[thread_id] += arr[i]; } }; for (int i = 0; i \u0026lt; num_threads; ++i) { threads[i] = std::thread(sum_part, i); } for (int i = 0; i \u0026lt; num_threads; ++i) { threads[i].join(); } int total_sum = 0; for (int i = 0; i \u0026lt; num_threads; ++i) { total_sum += partial_sums[i]; } return total_sum; } int main() { // 示例数组 int arr[] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}; int len = sizeof(arr) / sizeof(arr[0]); int result = sum_array(arr, len); std::cout \u0026lt;\u0026lt; \u0026#34;Sum of array: \u0026#34; \u0026lt;\u0026lt; result \u0026lt;\u0026lt; std::endl; return 0; } 使用多线程+原子操作 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;thread\u0026gt; #include \u0026lt;atomic\u0026gt; #include \u0026lt;numeric\u0026gt; // std::accumulate void partial_sum(const std::vector\u0026lt;int\u0026gt;\u0026amp; data, size_t start, size_t end, std::atomic\u0026lt;long long\u0026gt;\u0026amp; result) { long long partial_result = 0; for (size_t i = start; i \u0026lt; end; ++i) { partial_result += data[i]; } result += partial_result; // result += std::accumulate(data.data()+start, data.data()+end, 0); } int main() { const size_t data_size = 10000000; std::vector\u0026lt;int\u0026gt; data(data_size, 1); const size_t num_threads = 4; // std::thread::hardware_concurrency(); 获取硬件支持的线程数 std::vector\u0026lt;std::thread\u0026gt; threads; std::atomic\u0026lt;long long\u0026gt; result(0); size_t chunk_size = data_size / num_threads; for (size_t i = 0; i \u0026lt; num_threads; ++i) { size_t start = i * chunk_size; size_t end = (i == num_threads - 1) ? data_size : (i + 1) * chunk_size; // 最后一个线程处理剩余部分 threads.emplace_back(partial_sum, std::cref(data), start, end, std::ref(result)); } for (auto\u0026amp; t : threads) { t.join(); } std::cout \u0026lt;\u0026lt; \u0026#34;Total sum: \u0026#34; \u0026lt;\u0026lt; result.load() \u0026lt;\u0026lt; std::endl; return 0; } 使用多线程+互斥锁 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;thread\u0026gt; #include \u0026lt;mutex\u0026gt; void partial_sum(const std::vector\u0026lt;int\u0026gt;\u0026amp; data, size_t start, size_t end, long long\u0026amp; result, std::mutex\u0026amp; mtx) { long long local_sum = 0; for (size_t i = start; i \u0026lt; end; ++i) { local_sum += data[i]; } mtx.lock(); // std::lock_guard\u0026lt;std::mutex\u0026gt; lock(mtx); result += local_sum; mtx.unlock(); } int main() { const size_t data_size = 1\u0026#39;000\u0026#39;000; const size_t thread_count = 4; std::vector\u0026lt;int\u0026gt; data(data_size, 1); long long result = 0; std::mutex mtx; std::vector\u0026lt;std::thread\u0026gt; threads; size_t chunk_size = data_size / thread_count; for (size_t i = 0; i \u0026lt; thread_count; ++i) { size_t start = i * chunk_size; size_t end = (i == thread_count - 1) ? data_size : start + chunk_size; threads.emplace_back(partial_sum, std::cref(data), start, end, std::ref(result), std::ref(mtx)); } for (auto\u0026amp; t : threads) { t.join(); } std::cout \u0026lt;\u0026lt; \u0026#34;Total sum: \u0026#34; \u0026lt;\u0026lt; result \u0026lt;\u0026lt; std::endl; return 0; } ","date":"2024-10-22T00:00:00Z","permalink":"https://loveleaves.github.io/p/multi_threads/","title":"【HPC】 C/C++多线程并行/并发"},{"content":"\rUnlock to view this content.\r# NLP基础 ## 导论 ### 定义 ### 常见任务 ### 技术演进历史 ## 文本表示（Representation） ### 概述 文本表示是将自然语言转化为计算机能够理解的数值形式，是绝大多数自然语言处理（NLP）任务的基础步骤。 早期的文本表示方法（如词袋模型）通常将整段文本编码为一个向量。这类方法实现简单、计算高效，但存在明显的局限性——表达语序和上下文语义的能力较弱。因此，现代 NLP 技术逐渐引入更加精细和表达力更强的文本表示方法，以更有效地建模语言的结构和含义。文本表示的第一步通常是分词和词表构建。 1. 分词（Tokenization）是将原始文本切分为若干具有独立语义的最小单元（即token）的过程，是所有 NLP 任务的起点。 2. 词表（Vocabulary）是由语料库构建出的、包含模型可识别 token 的集合。词表中每个token都分配有唯一的 ID，并支持 token 与 ID 之间的双向映射。 在后续训练或预测过程中，模型会首先对输入文本进行分词，再通过词表将每个 token 映射为其对应的 ID。接着，这些 ID 会被输入嵌入层（Embedding Layer），转换为低维稠密的向量表示（即词向量） ### 分词（Tokenization） **分词粒度** - 词级（Word-Level）分词 - 字符级（Character-Level）分词 - 子词级（Subword‑Level）分词：目前主流方法 以下是 tokenization 过程的高度概括： ![tokenization](/imgs/nlp/tokenization.png) 在分词（根据其模型）之前，tokenizer 需要进行两个步骤： **标准化（normalization）** 和 **预分词（pre-tokenization）** 。 **标准化**步骤涉及一些常规清理，例如删除不必要的空格、小写和“/”或删除重音符号。 tokenizer 一般不会在原始文本上进行训练。因此，我们首先需要将文本拆分为更小的实体，例如单词。这就是**预分词**步骤的作用。基于单词的 tokenizer 可以简单地根据空格和标点符号将原始文本拆分为单词。这些词将是 tokenizer 在训练期间可以学习的子词的边界。 三种主要的子词 tokenization 算法：BPE（由 GPT-2 等使用）、WordPiece（由 BERT 使用）和 Unigram（由 T5 等使用）。 模型 | BPE | WordPiece | Unigram :----:|:---:|:---------:|:------: 训练 | 从小型词汇表开始，学习合并 token 的规则 | 从小型词汇表开始，学习合并 token 的规则 | 从大型词汇表开始，学习删除 token 的规则 训练步骤 | 合并对应最常见的 token 对 | 合并对应得分最高的 token 对，优先考虑每个独立 token 出现频率较低的对 | 删除会在整个语料库上最小化损失的词汇表中的所有 token 学习 | 合并规则和词汇表 | 仅词汇表 | 含有每个 token 分数的词汇表 编码 | 将一个单词分割成字符并使用在训练过程中学到的合并 | 从开始处找到词汇表中的最长子词，然后对其余部分做同样的事 | 使用在训练中学到找到最可能的 token 分割方式 #### BPE（Byte Pair Encoding）/BBPE（Byte Level Byte Pair Encoding） 字节对编码（BPE）最初被开发为一种压缩文本的算法，然后在预训练 GPT 模型时被 OpenAI 用于 tokenization。许多 Transformer 模型都使用它，包括 GPT、GPT-2、RoBERTa、BART 和 DeBERTa。 BPE 训练首先计算语料库中使用的唯一单词集合（在完成标准化和预分词步骤之后），然后取出用来编写这些词的所有符号来构建词汇表。举一个非常简单的例子，假设我们的语料库使用了这五个词： ``` python \"hug\", \"pug\", \"pun\", \"bun\", \"hugs\" ``` 基础单词集合将是 [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\"] 。在实际应用中，基本词汇表将至少包含所有 ASCII 字符，可能还包含一些 Unicode 字符。如果你正在 tokenization 不在训练语料库中的字符，则该字符将转换为未知 tokens，这就是为什么许多 NLP 模型在分析带有表情符号的内容的结果非常糟糕的原因之一。 \u003e GPT-2 和 RoBERTa （这两者非常相似）的 tokenizer 有一个巧妙的方法来处理这个问题：他们不把单词看成是用 Unicode 字符编写的，而是用字节编写的。这样，基本词汇表的大小很小（256），但是能包含几乎所有你能想象的字符，而不会最终转换为未知 tokens 这个技巧被称为 字节级（byte-level） BPE 。 获得这个基础单词集合后，我们通过学习 **合并（merges）** 来添加新的 tokens 直到达到期望的词汇表大小。合并是将现有词汇表中的两个元素合并为一个新元素的规则。所以，一开始会创建出含有两个字符的 tokens 然后，随着训练的进展，会产生更长的子词。 在分词器训练期间的任何一步，BPE 算法都会搜索最常见的现有 tokens 对 （在这里，“对”是指一个词中的两个连续 tokens ）。最常见的这一对会被合并，然后我们重复这个过程。 **步骤** 1. 标准化 2. 预分词 3. 将单词拆分为单个字符 4. 根据学习的合并规则，按顺序合并拆分的字符 **代码实现** ``` python # 字母表 alphabet = [] for word in word_freqs.keys(): for letter in word: if letter not in alphabet: alphabet.append(letter) alphabet.sort() vocab = [\"\u003c|endoftext|\u003e\"] + alphabet.copy() # GPT-2 模型的特殊 tokens # 合并规则，计算出现频率，之后可以按相邻频率最高的pair对合并 def compute_pair_freqs(splits): pair_freqs = defaultdict(int) for word, freq in word_freqs.items(): split = splits[word] if len(split) == 1: continue for i in range(len(split) - 1): pair = (split[i], split[i + 1]) pair_freqs[pair] += freq return pair_freqs # 设定词表上限，得到训练词表 vocab 及合并规则 merges vocab_size = 50 while len(vocab) \u003c vocab_size: pair_freqs = compute_pair_freqs(splits) best_pair = \"\" max_freq = None for pair, freq in pair_freqs.items(): if max_freq is None or max_freq \u003c freq: best_pair = pair max_freq = freq if not best_pair: # reach max size break splits = merge_pair(*best_pair, splits) merges[best_pair] = best_pair[0] + best_pair[1] vocab.append(best_pair[0] + best_pair[1]) # 通过vocab、merges def tokenize(word): splits = [[l for l in word]] for pair, merge in merges.items(): for idx, split in enumerate(splits): i = 0 while i \u003c len(split) - 1: if split[i] == pair[0] and split[i + 1] == pair[1]: split = split[:i] + [merge] + split[i + 2 :] else: i += 1 splits[idx] = split return sum(splits, []) ``` **总结** - 使用频率进行合并 #### WordPiece \u003e [WordPiece tokenization 算法](https://hf-mirror.com/learn/llm-course/zh-CN/chapter6/6) WordPiece 是 Google 开发的用于 BERT 预训练的分词算法。自此之后，很多基于 BERT 的 Transformer 模型都复用了这种方法，比如 DistilBERT，MobileBERT，Funnel Transformers 和 MPNET。它在训练方面与 BPE 非常类似，但实际的分词方法有所不同。 与BPE 一样，WordPiece 也是从包含模型使用的特殊 tokens 和初始字母表的小词汇表开始的。由于它是通过添加前缀（如 BERT 中的 ## ）来识别子词的，每个词最初都会通过在词内部所有字符前添加该前缀进行分割。因此，例如 \"word\" 将被这样分割： ``` python w ##o ##r ##d ``` 因此，初始字母表包含所有出现在单词第一个位置的字符，以及出现在单词内部并带有 WordPiece 前缀的字符。 然后，同样像 BPE 一样，WordPiece 会学习合并规则。主要的不同之处在于合并对的选择方式。WordPiece 不是选择频率最高的对，而是对每对计算一个得分，使用以下公式： score=(freq_of_pair)/(freq_of_first_element×freq_of_second_element) **代码实现** ``` python # 字母表 alphabet = [] for word in word_freqs.keys(): if word[0] not in alphabet: alphabet.append(word[0]) for letter in word[1:]: if f\"##{letter}\" not in alphabet: alphabet.append(f\"##{letter}\") vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy() # 合并规则，计算pair分数 def compute_pair_scores(splits): letter_freqs = defaultdict(int) pair_freqs = defaultdict(int) for word, freq in word_freqs.items(): split = splits[word] if len(split) == 1: letter_freqs[split[0]] += freq continue for i in range(len(split) - 1): pair = (split[i], split[i + 1]) letter_freqs[split[i]] += freq pair_freqs[pair] += freq letter_freqs[split[-1]] += freq scores = { pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]]) for pair, freq in pair_freqs.items() } return scores # 最大子词查找 def encode_word(word): tokens = [] while len(word) \u003e 0: i = len(word) while i \u003e 0 and word[:i] not in vocab: i -= 1 if i == 0: return [\"[UNK]\"] tokens.append(word[:i]) word = word[i:] if len(word) \u003e 0: word = f\"##{word}\" return tokens ``` **总结** - 通过`##`识别子词 - WordPiece 和 BPE 的分词方式有所不同，WordPiece 只保存最终词汇表，而不保存学习到的合并规则。WordPiece 从待分词的词开始，找到词汇表中最长的子词，然后在其处分割。 - 当分词过程中无法在词汇库中找到该子词时，**整个词**会被标记为 unknown（未知），BPE 只会将不在词汇库中的**单个字符**标记为 unknown。 #### Unigram \u003e [Unigram tokenization 算法](https://hf-mirror.com/learn/llm-course/zh-CN/chapter6/7) Unigram 算法常用于 SentencePiece 中，该切分算法被 AlBERT，T5，mBART，Big Bird 和 XLNet 等模型广泛采用。 **训练** 与BPE 和 WordPiece 相比，Unigram 的工作方式正好相反：它从一个**大词汇库**开始，然后逐步删除词汇，直到达到目标词汇库大小。构建基础词汇库有多种方法：例如，我们可以选取预切分词汇中最常见的子串，或者在具有大词汇量的初始语料库上进行 BPE 得到一个初始词库。 在训练的每一步，Unigram 算法都会在给定当前词汇的情况下计算语料库的损失。然后，对于词汇表中的每个符号，算法计算如果删除该符号，整体损失会增加多少，并寻找删除后损失增加最少的符号。这些符号对语料库的整体损失影响较小，因此从某种意义上说，它们“相对不必要”并且是移除的最佳候选者。 这个过程非常消耗计算资源，因此我们不只是删除与最低损失增长相关的单个符号，而是删除与最低损失增长相关的百分之/p （p 是一个可以控制的超参数，通常是 10 或 20）的符号。然后重复此过程，直到词汇库达到所需大小。 注意，我们永远不会删除基础的单个字符，以确保任何词都能被切分。 然而，这仍然有些模糊：算法的主要部分是在词汇库中计算语料库的损失并观察当我们从词汇库中移除一些符号时损失如何变化，但我们尚未解释如何做到这一点。这一步依赖于 Unigram 模型的切分算法。 #### 常用工具 按照实现方式大致可以分为如下两类： - 一类是基于词典或模型的传统方法，主要以“词”为单位进行切分； - 另一类是基于子词建模算法（如BPE）的方式，从数据中自动学习高频字组合，构建子词词表。 前者的代表工具包括 jieba、HanLP等，这些工具广泛应用于传统 NLP 任务中。 后者的代表工具包括 Hugging Face Tokenizer、SentencePiece、tiktoken等，常用于大规模预训练语言模型中。 ##### SentencePiece \u003e [官网](https://github.com/google/sentencepiece) **主要特性** - 多分词粒度：支持BPE、ULM子词算法，也支持char, word分词； - 多语言：以unicode方式编码字符，将所有的输入（英文、中文等不同语言）都转化为unicode字符，解决了多语言编码方式不同的问题； - 编解码的可逆性：之前几种分词算法对空格的处理略显粗暴，有时是无法还原的。Sentencepiece显式地将空白作为基本标记来处理，用一个元符号 “▁”（ U+2581 ）转义空白，这样就可以实现简单且可逆的编解码； - 无须Pre-tokenization：Sentencepiece可以直接从raw text/setences进行训练，无须Pre-tokenization - Fast and lightweight； ##### tokenizers库 \u003e [官网](https://hugging-face.cn/docs/tokenizers/index) tokenizers是transformers的兄弟库，实现了当前最常用的分词器。 主要特点： - 使用当今最常用的分词器来训练新词汇表和进行分词。 - 得益于 Rust 实现，速度极快（包括训练和分词）。在服务器 CPU 上，对 1GB 的文本进行分词耗时不到 20 秒。 - 易于使用，同时也极其通用。 - 专为研究和生产而设计。 - 完整的对齐跟踪。即使进行了破坏性的规范化，也始终可以获取到与任意词元对应的原始句子部分。 - 完成所有预处理：截断、填充、添加模型所需的特殊词元。 分词流程 - 归一化 - 预分词 - 模型 - 后处理 #### LLM中的分词器 ##### BERT的分词器 \u003e [代码](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/tokenization_bert.py) \u003e \u003e [文档](https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/bert#transformers.BertTokenizer) BERT的分词器由两个部分组成： - BasicTokenizer： - 转成 unicode：Python3，输入为str时，可以省略这一步 - _clean_text：去除各种奇怪字符 - _tokenize_chinese_chars：中文按字拆开 - whitespace_tokenize：空格分词 - _run_strip_accents：去掉变音符号 - _run_split_on_punc：标点分词 - 再次空格分词：whitespace_tokenize(\" \".join(split_tokens))，先用空格join再按空白分词，可以去掉连续空格 - WordpieceTokenizer： - 贪心最大匹配：用双指针实现； ### 词表示（word representation） \u003e 详见：`尚硅谷大模型技术之NLP1.0.3`的3.3 词表示的发展经历了从稀疏的one-hot编码，到稠密的语义化词向量，再到近年来的上下文相关的词表示。不同的词表示方法在表达能力、语义建模、上下文适应性等方面存在显著差异。 #### One-hot编码 缺点： - 随着词表规模的扩大，向量维度会迅速膨胀，导致计算效率低下 - 无法体现词与词之间的语义关系（如相关词向量正交为0） #### 语义化词向量 Word2Vec 它通过对大规模语料的学习，为每个词生成一个具有语义意义的稠密向量表示。这些向量能够在连续空间中表达词与词之间的关系，使得“意思相近”的词在空间中距离更近。 ![CBOW](/imgs/nlp/cbow.png) CBOW ![Skip-Gram](/imgs/nlp/skip_gram.png) Skip-Gram #### 上下文相关词表示（Contextual Word Representations） 虽然像Word2Vec这样的模型已经能够为词语提供具有语义的向量表示，但是它只为每个词分配一个固定的向量表示，不论它在句中出现的语境如何。这种表示被称为静态词向量（static embeddings）。 然而，语言的表达极其灵活，一个词在不同上下文中可能有完全不同的含义。如吃的苹果和苹果手机。 上下文相关词表示（Contextual Word Representations），是指词语的向量表示会根据它所在的句子上下文动态变化，从而更好地捕捉其语义。一个具有代表性的模型是——ELMo。该模型全称为 Embeddings from Language Models，发表于2018年2月。其基于LSTM 语言模型，使用上下文动态生成每个词的表示，每个词的向量由其前文和后文共同决定，是第一个被广泛应用于下游任务的上下文词向量模型。 ## 传统序列模型 ### RNN（Recurrent Neural Network，循环神经网络） 在自然语言中，词语的顺序对于理解句子的含义至关重要。虽然词向量能够表示词语的语义，但它本身并不包含词语之间的顺序信息。 为了解决这一问题，研究者提出RNN（Recurrent Neural Network，循环神经网络）。 RNN 会逐个读取句子中的词语，并在每一步结合当前词和前面的上下文信息，不断更新对句子的理解。通过这种机制，RNN 能够持续建模上下文，从而更准确地把握句子的整体语义。因此RNN曾是序列建模领域的主流模型，被广泛应用于各类NLP任务。 说明： 随着技术的发展，RNN已经逐渐被结构更灵活、计算效率更高的Transformer 模型所取代，后者已经成为当前自然语言处理的主流方法。 尽管如此，RNN 仍然具有重要的学习价值。它所体现的“循环建模上下文”的思想，不仅为 LSTM 和 GRU 等改进模型奠定了基础，也有助于我们更好地理解 Transformer 等更复杂的架构。 #### 网络结构 其中隐藏层的计算公式为： $h_t = \\tanh(x_t*W_x+h_{t-1}*W_h+b)$ ![RNN计算图](/imgs/nlp/RNN计算图.svg) RNN计算图 ![RNN](/imgs/nlp/RNN.png) 多层+双向 RNN Pytorch API： [torch.nn.RNN 模块](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) ``` python torch.nn.RNN( input_size, # 每个时间步输入特征的维度（词向量维度） hidden_size, # 隐藏状态的维度 num_layers=1, # RNN 层数，默认为 1 nonlinearity=\"tanh\", # 激活函数，'tanh'（默认）或 'relu' bias=True, # 是否使用偏置项，默认 True batch_first=False, # 输入张量是否是 (batch, seq, feature)，默认 False 表示 (seq, batch, feature) dropout=0.0, # 除最后一层外，其余层之间的 dropout 概率 bidirectional=False, # 是否为双向 RNN，默认 False device=None, dtype=None, ) rnn = torch.nn.RNN(...) output, h_n = rnn(input, h_0) # 注意 序列padding时，输入RNN前需进行pack处理，详见LSTM部分 ``` **输入输出形状：** - 输入 - input：输入序列，形状为(seq_len, batch_size, input_size)，如果 batch_first=True，则为 (batch_size, seq_len, input_size) - h_0：可选，初始隐藏状态，形状为 (num_layers × num_directions, batch_size, hidden_size) - 输出： - output：RNN层的输出，包含最后一层每个时间步的隐藏状态，形状为 (seq_len, batch_size, num_directions × hidden_size )，如果如果 batch_first=True，则为(batch_size, seq_len, num_directions × hidden_size ) - h_n：最后一个时间步的隐藏状态，包含每一层的每个方向，形状为 (num_layers × num_directions, batch_size, hidden_size) ![RNN](/imgs/nlp/RNN_shape.png) 多层+双向 RNN 输入输出形状 #### 存在问题 尽管循环神经网络（RNN）在处理序列数据方面具有天然优势，但它在实际应用中面临一个非常严重的问题：长期依赖建模困难。这指的是：在训练过程中，当输入序列很长时，模型难以有效学习早期输入对最终输出的影响。 上述问题的根本原因在于训练过程中存在的梯度消失或梯度爆炸问题。 在训练RNN时，采用的是时间反向传播（Backpropagation Through Time, BPTT）方法，在反向传播过程中，梯度需要在每个时间步上不断链式传递。 ### 长短期记忆网络（Long Short-Term Memory, LSTM） 为了缓解RNN梯度消失或者梯度爆炸的问题，Hochreiter 和 Schmidhuber 于 1997 年提出了长短期记忆网络（Long Short-Term Memory, LSTM）。 #### 网络结构 LSTM 通过引入特殊的记忆单元（Memory Cell，图中的），有效提升了模型对长序列依赖关系的建模能力。 其中隐藏层的计算公式为： $$ i_t​=\\sigma(W_{ii​}x_{t}​+b_{ii}​+W_{hi}​h_{t−1}​+b_{hi}​)\\\\\\\\ f_t​=\\sigma(W_{if}​x_t​+b_{if}​+W_{hf}​h_{t−1}​+b_{hf}​)\\\\\\\\ g_t​=\\tanh(W_{ig}​x_t​+b_{ig}​+W_{hg}​h_{t−1}​+b_{hg}​)\\\\\\\\ o_t​=\\sigma(W_{io}​x_t​+b_{io}​+W_{ho}​h_{t−1}​+b_{ho}​)\\\\\\\\ c_t​=f_t​ \\odot c_{t−1}​+i_t​ \\odot g_t\\\\\\\\ ​h_t​=o_t \\odot \\tanh(c_t​)​ $$ 注： - σ：sigmoid function，(0,1) - ⊙：Hadamard product，各元素乘积 其沿时间步展开后的内部结构如下图所示，核心结构是三个“门”，分别是： - 遗忘门（f）：决定当前时间步要忘记多少过去的记忆。 - 输入门（i）：控制要从当前时间步的输入向记忆单元存入多少新的信息。 - 输出门（o）：控制从记忆单元中读取多少信息作为当前时间步的隐藏状态进行输出。 ![LSTM 结构](/imgs/nlp/lstm_structure1.png) LSTM 结构 ![LSTM 展开结构](/imgs/nlp/lstm_structure.png) LSTM 展开结构 ![LSTM](/imgs/nlp/lstm.png) 多层+双向 LSTM Pytorch API： [torch.nn.LSTM 模块](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) ``` python torch.nn.LSTM( input_size, # 每个时间步输入特征的维度（词向量维度） hidden_size, # 隐藏状态的维度 num_layers=1, # LSTM 层数，默认为 1 nonlinearity=\"tanh\", # 激活函数，'tanh'（默认）或 'relu' bias=True, # 是否使用偏置项，默认 True batch_first=False, # 输入张量是否是 (batch, seq, feature)，默认 False 表示 (seq, batch, feature) dropout=0.0, # 除最后一层外，其余层之间的 dropout 概率 bidirectional=False, # 是否为双向 LSTM，默认 False proj_size=0, # 隐藏状态的投影输出维度；若为 0，则不使用 projection。（详见官方文档，用于调整） device=None, dtype=None, ) lstm = torch.nn.LSTM() output, (h_n, c_n) = lstm(input, (h_0, c_0)) ``` **注意：** ``` python # 对于序列模型中有padding操作时，注意把padding去掉再丢入LSTM计算 # 双向多层LSTM模型示例 class ReviewAnalyzeModel(nn.Module): def __init__(self, vocab_size, padding_idx=0): super(ReviewAnalyzeModel, self).__init__() self.padding_idx = padding_idx self.embedding = nn.Embedding( num_embeddings=vocab_size, embedding_dim=config.EMBEDDING_DIM, padding_idx=padding_idx ) self.lstm = nn.LSTM( input_size=config.EMBEDDING_DIM, hidden_size=config.HIDDEN_SIZE, num_layers=config.NUM_LAYERS, batch_first=True, bidirectional=True ) self.linear = nn.Linear(2 * config.HIDDEN_SIZE, 1) def forward(self, x): # x.shape [batch_size, seq_len]，尾部padding # ① 自动计算每条样本实际长度（忽略 padding） lengths = (x != self.padding_idx).sum(dim=1) # ② embedding embedding = self.embedding(x) # ③ pack（让 LSTM 忽略 padding） packed = nn.utils.rnn.pack_padded_sequence( embedding, lengths.cpu(), batch_first=True, enforce_sorted=False ) # ④ LSTM _, (h_n, _) = self.lstm(packed) # h_n: [num_layers * num_directions, batch, hidden_dim] # 由于是双向，最后一层包括 forward 与 backward # forward: index = (num_layers - 1) * 2 # backward: index = forward + 1 forward_idx = (config.NUM_LAYERS - 1) * 2 backward_idx = forward_idx + 1 last_hidden_forward = h_n[forward_idx] # [batch, hidden] last_hidden_backward = h_n[backward_idx] # [batch, hidden] # ⑤ 拼接双向 hidden last_hidden = torch.cat((last_hidden_forward, last_hidden_backward), dim=-1) # ⑥ 线性分类 out = self.linear(last_hidden) return out.squeeze(1) ``` **输入输出形状：** 说明： 如果proj_size设置为0，那么输入、输出除了`c_0`与`c_n`，其他与RNN完全一致。 - 输入（假定proj_size设置为0） - input：输入序列，形状为(seq_len, batch_size, input_size)，如果 batch_first=True，则为 (batch_size, seq_len, input_size) - h_0：可选，初始隐藏状态，形状为 (num_layers × num_directions, batch_size, hidden_size) - c_0：可选，初始细胞状态，形状为 (num_layers × num_directions, batch_size, hidden_size) - 输出： - output：LSTM层的输出，包含最后一层每个时间步的隐藏状态，形状为 (seq_len, batch_size, num_directions × hidden_size )，如果如果 batch_first=True，则为(batch_size, seq_len, num_directions × hidden_size ) - h_n：最后一个时间步的隐藏状态，包含每一层的每个方向，形状为 (num_layers × num_directions, batch_size, hidden_size) - c_n：最后一个时间步的细胞状态，包含每一层的每个方向，形状为 (num_layers × num_directions, batch_size, hidden_size) ![LSTM](/imgs/nlp/LSTM_shape.png) 多层+双向 LSTM 输入输出形状 #### LSTM为何能缓解梯度消失和梯度爆炸？ LSTM通过引入记忆单元（Memory Cell），在时间步之间提供了一条稳定的梯度传播路径。 记忆单元的更新公式为 $$c_t​=f_t​⊙c_{t−1}​+i_t​⊙g_t$$ 所以$\\frac{\\partial C_t}{\\partial C_{t-1}}$（简单起见，按照标量推导） 在反向传播时，沿记忆单元路径，梯度传播实际上是多个$f_t$连乘的结果。虽然每个$f_t$的取值小于1，但通常较接近于1。这是因为$f_t$由遗忘门生成，在一般任务中，遗忘门倾向于“记得多、忘得少”，因此$f_t$的值通常较大。 由于乘积中的每一项$f_t$较接近1，整体衰减速度远小于传统RNN中隐藏状态链式传播时的指数衰减。这使得早期时间步的输入，能够通过记忆单元路径稳定地影响到最终的总梯度，从而有效参与参数的更新，保证了模型对长序列依赖的学习能力。 #### 存在问题 尽管 LSTM 相较传统 RNN 解决了长期依赖问题，性能大幅提升，但在实际应用中，仍存在一些明显的局限性和问题，主要包括： - 难以并行计算 - LSTM 的时间步之间具有强依赖性（后一个时间步的输入依赖前一个时间步的输出），导致无法进行大规模并行加速，训练和推理速度受限。 - 参数量大，计算开销高： - 每个 LSTM 单元内部包含多个门控机制（输入门、遗忘门、输出门），每个门都需要独立计算，导致参数数量和计算量远大于普通 RNN。 - 在资源受限的场景下（如移动端、嵌入式设备），部署 LSTM 会面临挑战。 - 长期依赖建模仍然有限 - 虽然 LSTM 延缓了梯度消失问题，但并不能完全消除。当序列极长时，模型依然难以有效捕捉非常远距离的依赖关系。 ### 门控循环单元（Gated Recurrent Unit，GRU） Gated Recurrent Unit（GRU）是为了进一步简化 LSTM 结构、降低计算成本而提出的一种变体。GRU 保留了门控机制的核心思想，但相比 LSTM，结构更为简洁，参数更少，训练效率更高。 在许多实际任务中，GRU 能在保持类似性能的同时，显著减少训练时间。 #### 网络结构 与LSTM相比，GRU做出了以下改进： - 取消了LSTM中独立的记忆单元，只保留隐藏状态。 - 通过两个门控结构控制信息流动： - 重置门（Reset Gate）：用于控制遗忘多少旧信息 - 更新门（Update Gate）：用于控制保留多少旧信息，以及引入多少新信息 其中计算公式为： $$ r_t = \\sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr})\\\\\\\\ z_t = \\sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\\\\\\\ n_t = \\tanh(W_{in} x_t + b_{in} + r_t \\odot (W_{hn} h_{(t-1)}+ b_{hn}))\\\\\\\\ h_t = (1 - z_t) \\odot n_t + z_t \\odot h_{(t-1)} $$ ![GRU结构图](/imgs/nlp/gru.png) GRU结构图 ![GRU展开结构图](/imgs/nlp/gru_structure.png) GRU展开结构图 Pytorch API： [torch.nn.GRU 模块](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html) ``` python torch.nn.GRU( input_size, # 每个时间步输入特征的维度（词向量维度） hidden_size, # 隐藏状态的维度 num_layers=1, # GRU 层数，默认为 1 nonlinearity=\"tanh\", # 激活函数，'tanh'（默认）或 'relu' bias=True, # 是否使用偏置项，默认 True batch_first=False, # 输入张量是否是 (batch, seq, feature)，默认 False 表示 (seq, batch, feature) dropout=0.0, # 除最后一层外，其余层之间的 dropout 概率 bidirectional=False, # 是否为双向 GRU，默认 False device=None, dtype=None, ) gru= torch.nn.GRU() output, h_n = gru(input, h_0) ``` **输入输出形状：** - 同RNN ## Seq2Seq模型 传统的自然语言处理任务（如文本分类、序列标注）以​**​静态输出**​​为主，其目标是预测固定类别或标签。然而，现实中的许多应用需要模型**​​动态生成新的序列**​​，例如： - ​​机器翻译​​：输入中文句子，输出对应的英文翻译。 - ​​文本摘要​​：输入长篇文章，生成简短的摘要。 - ​问答系统​​：输入用户问题，生成自然语言回答。 - ​对话系统​​：输入对话历史，生成连贯的下一条回复。 这些任务具有两个关键共同点： - ​​输入和输出均为序列​​（如词、字符或子词序列）。 - ​输入与输出序列长度动态可变​​（例如翻译任务中，中英文句子长度可能不同）。 为了解决这类问题，研究者提出了**Seq2Seq（Sequence to Sequence，序列到序列）**模型。 ### 模型结构 Seq2Seq 模型由一个编码器（Encoder）和一个解码器（Decoder）构成。 - 编码器（Encoder）：负责提取输入序列的语义信息，并将其压缩为一个固定长度的上下文向量（Context Vector） - 解码器（Decoder）：基于该上下文向量，逐步生成目标序列。 ![Seq2Seq结构图](/imgs/nlp/seq2seq.png) Seq2Seq结构图 #### 编码器（Encoder） 编码器主要由一个循环神经网络（RNN/LSTM/GRU）构成，其任务是将输入序列的语义信息提取并压缩为一个上下文向量。 在模型处理输入序列时，循环神经网络会依次接收每个token的输入，并在每个时间步步更新隐藏状态。每个隐藏状态都携带了截止到当前位置为止的信息。随着序列推进，信息不断累积，最终会在最后一个时间步形成一个包含整句信息的隐藏状态。 这个最后的隐藏状态就会作为上下文向量（context vector），传递给解码器，用于指导后续的序列生成。 ![Seq2Seq encoder](/imgs/nlp/seq2seq_encoder.png) Seq2Seq RNN encoder 为增强编码器的理解能力，循环网络也可以采用双向结构（结合前文与后文信息）或多层结构（提取更深的语义特征）。 #### 解码器（Decoder） 解码器主要也由一个循环神经网络（RNN / LSTM / GRU）构成，其任务是基于编码器传递的上下文向量，逐步生成目标序列。 ![Seq2Seq decoder](/imgs/nlp/seq2seq_decoder.png) Seq2Seq RNN decoder 在生成开始时，循环神经网络以上下文向量作为初始隐藏状态，并接收一个特殊的起始标记 `（start of sentence）`作为第一个时间步的输入，用于预测第一个 token。 随后，在每一个时间步，模型都会根据前一时刻的隐藏状态和上一步生成的 token，预测当前的输出。这种“将前一步的输出作为下一步输入”的方式被称为自回归生成（Autoregressive Generation），它确保了生成结果的连贯性。 生成过程会持续进行，直到模型生成了一个特殊的结束标记` （end of sentence）`，表示句子生成完成。 \u003e 说明：起始标记和结束标记会在训练数据中显式添加，模型会在训练中学会何时开始、如何续写，以及何时结束，从而掌握完整的生成流程。 ### 模型训练和推理机制 #### 模型训练 - 编码器：通过嵌入层和循环神经网络（RNN / LSTM / GRU）的逐步处理，将整句编码为上下文向量。 - 解码器：解码器使用该上下文向量初始化其隐藏状态，然后逐步生成目标序列。 - 注意：训练阶段与推理阶段的解码策略是不同的 - 1、在推理阶段，解码器采用自回归生成方式：每一步的输入是模型自己上一步的预测结果。 - 2、在训练阶段，通常使用一种称为 Teacher Forcing 的策略，即使用目标序列中真实的前一个token。 #### 模型推理 - 编码器：推理阶段的编码器处理流程与训练时完全一致。 - 解码器：生成方式采用自回归生成（Autoregressive Generation）：每一步的输出会作为下一步的输入，逐步构造完整句子。 - 1、自回归生成流程：起始标记 ``，结束标记``，上一步生成的词作为当前输入 - 2、**词选择策略**： - 贪心解码（Greedy Decoding）：每一步都选择概率最高的词，局部最优。 - 束搜索（Beam Search）：每一步保留多个候选词序列（如 beam size = 3），并在扩展后选择得分最高的完整句子。 ### 存在问题 在上述 Seq2Seq 架构中，编码器会将整个源句压缩为一个固定长度的上下文向量，并将其作为解码器生成目标序列的唯一参考。这种“压缩再解压”的方式虽然结构简洁，但在实际任务中暴露出两个核心问题： 1. 信息压缩困难，语义表达受限 对于编码器而言，用一个定长向量去表达任意复杂的句子，是一项非常困难的任务。尤其在面对长句时，信息很容易在压缩过程中丢失，导致语义表达不完整。 这种“信息瓶颈”限制了模型在处理长文本或复杂语义结构时的表现。 2. 缺乏动态感知，解码难以精准生成 解码器始终只能基于同一个上下文向量进行生成。 但在实际生成过程中，不同位置的目标词，往往依赖源句中不同的关键信息： - 生成主语时，可能更依赖源句的开头； - 生成谓语或宾语时，可能需要参考句中或句末内容。 然而在固定表示下，解码器无法“有选择地关注”输入序列的不同部分，只能一视同仁地处理所有信息，从而降低了生成的准确性与灵活性。 ## Attention机制 传统的 Seq2Seq 模型中，编码器在处理源句时，无论其长度如何，最终都只能将整句信息压缩为一个固定长度的上下文向量，用作解码器的唯一参考。这种设计存在两个显著问题： - 信息压缩困难：固定向量难以完整表达长句或复杂语义，容易丢失关键信息； - 缺乏动态感知：解码器在每一步生成中都只能依赖同一个上下文向量，难以根据不同位置的生成需要灵活提取信息。 为了解决上述问题，研究者引入了** Attention 机制**。其**核心思想**是： 解码器在生成目标序列的每一步时，不再依赖于一个静态的上下文向量，而是根据当前的解码状态，动态地从编码器各时间步的隐藏状态中选取最相关的信息，以辅助当前步的生成。 这种机制赋予模型“对齐”能力，使其能够自动判断源句中哪些位置对当前的目标词更为重要，从而有效缓解信息瓶颈问题，提升生成质量与表达能力。 ### 工作原理 注意力机制的核心思想，是解码器在生成目标序列的每一步时，动态地从编码器的各个时间步的隐藏状态中提取当前所需的信息，而不再只依赖一个固定的上下文向量。 ![attention](/imgs/nlp/attention.png) attention 这一机制通常通过以下 4 个关键步骤实现： #### 相关性计算 在目标序列生成的每一步，解码器都会计算当前时间步的隐藏状态与编码器各个时间步输出之间的相关性。这些相关性衡量了源句中每个位置对当前生成内容的重要程度，从而决定模型应将多少注意力分配给不同的源位置。 相关性的计算依赖于特定的函数，通常被称为注意力评分函数（attention scoring function）。常见的评分函数实现方式将在下一节中详细介绍。 ![attention](/imgs/nlp/attention_step1.png) attention 相关性计算 #### 注意力权重计算 得到所有源位置的注意力评分后，使用 Softmax 函数将其归一化为概率分布，作为注意力权重。得分越高的位置，其对应的权重越大，代表模型在当前生成中更关注该位置的信息。 ![attention](/imgs/nlp/attention_step2.png) attention 注意力权重计算 #### 上下文向量计算 将所有编码器输出按照注意力权重进行加权求和，得到一个上下文向量。这个向量就表示当前时间步，模型从源句中提取出的关键信息。 ![attention](/imgs/nlp/attention_step3.png) attention 上下文向量计算 #### 解码信息融合 在得到上下文向量后，解码器将其与当前时间步的隐藏状态进行拼接，以融合两者信息，最终通过线性变换和 Softmax，生成当前时间步目标词的概率分布。 ![attention](/imgs/nlp/attention_step4.png) attention 解码信息融合 ### 注意力评分函数 注意力评分函数有多种实现方式。本节将介绍三种常见的计算方法：点积评分（Dot）、通用点积评分（General）和拼接评分（Concat）。它们虽然在结构上各有差异，但本质上都是用于衡量解码器当前隐藏状态与编码器各时间步隐藏状态之间的相关性，并据此分配注意力权重。 #### 点积评分（Dot） 点积评分是注意力机制中最简单、最直接的一种相关性评分方法。它通过计算解码器当前时间步的隐藏状态与编码器每个时间步的隐藏状态的点积，来衡量二者之间的相关性： ![attention_score_function_dot](/imgs/nlp/attention_score_function_dot.png) 其含义可以理解为：如果两个向量方向越一致（即越接近），它们的点积就越大，表示相关性越强，模型应当给予更多注意力。 #### 通用点积评分（General） 通用点积评分在点积的基础上引入了一个可学习的权重矩阵W,用于先对编码器隐藏状态进行线性变换，再与解码器隐藏状态进行点积： ![attention_score_function_general](/imgs/nlp/attention_score_function_general.png) 该方法的设计动机主要是为了解决编码器和解码器隐藏状态维度不一致的问题。通过引入权重矩阵W，不仅实现了维度对齐，也增强了模型对编码器输出的适应能力，从而提升了注意力机制的表达能力。 #### 拼接评分（Concat） 拼接评分是一种表达能力更强的相关性评分方法。它的核心思想是：将解码器当前隐藏状态与编码器每个时间步的隐藏状态拼接为一个长向量，经过线性变换和非线性激活，最后用一个向量进行投影，得到最终打分值： ![attention_score_function_concat](/imgs/nlp/attention_score_function_concat.png) 相比前两种方法，Concat 评分方式在建模能力上更强。它不仅考虑了两个状态的数值关系，还引入非线性变换，能够捕捉更复杂的交互模式，更适合处理对齐关系复杂的任务场景。 ### 存在问题 尽管注意力机制极大地增强了 Seq2Seq 模型的建模能力，但由于其核心依然依赖于 RNN 结构，仍面临两个根本性问题： - 计算过程无法并行 - RNN 的时间步之间存在强依赖，必须顺序执行，限制了训练效率和硬件资源的利用率。 - 长期依赖问题仍未根除 - 模型需要跨多个时间步传递信息，对于超长序列，训练过程中容易出现梯度消失，难以有效建模长距离依赖关系。 ## Transformer模型 此前的Seq2Seq模型通过注意力机制取得了一定提升，但由于整体结构仍依赖 RNN，依然存在计算效率低、难以建模长距离依赖等结构性限制。 为了解决这些问题，Google在2017 年发表一篇论文《[Attention Is All You Need](https://arxiv.org/pdf/1706.03762)》，提出了一种全新的模型架构——Transformer。该模型完全摒弃了 RNN 结构，转而使用注意力机制直接建模序列中各位置之间的关系。通过这种方式，Transformer不仅显著提升了训练效率，也增强了模型对长距离依赖的建模能力。 Transformer 的提出对自然语言处理产生了深远影响。在机器翻译任务中，它首次超越了 RNN 模型的表现，并成为后续各类预训练语言模型的基础框架，如 BERT、GPT 等。这些模型推动 NLP 进入了“预训练 + 微调”的新时代，极大地提升了模型在多种任务上的通用性与性能。如今，Transformer 架构不仅广泛应用于 NLP，还扩展至语音识别、图像处理、代码生成等多个领域，成为现代深度学习中最具代表性的通用模型之一。 ### 模型结构 #### 核心思想 在seq2seq基础上，通过Attention等替代RNN进行位置关系建模，便于并行计算，更适合捕捉长距离依赖。 ![transformer_demo](/imgs/nlp/transformer_demo.png) #### 整体结构 标准的 Transformer 模型通常包含 6个编码器层和 6 个解码器层。 ![transformer_demo2](/imgs/nlp/transformer.png) ### 编码器（Encoder） ![transformer_encoder](/imgs/nlp/transformer_encoder.png) - Transformer 的编码器用于理解输入序列的语义信息，并生成每个token的上下文表示，为解码器生成目标序列提供基础。 - 编码器由多个结构相同的编码器层（Encoder Layer）堆叠而成。 - 每个 Encoder Layer的主要任务都是对其输入序列进行上下文建模，使每个位置的表示都能融合来自整个序列的全局信息。 - 每个 Encoder Layer都包含两个子层（sublayer），分别是自注意力子层（Self-Attention Sublayer）和前馈神经网络子层（Feed-Forward Sublayer）。 - Self-Attention：用于捕捉序列中各位置之间的依赖关系。 - Feed-Forward：用于对每个位置的表示进行非线性变换，从而提升模型的表达能力。 #### 自注意力层（Self-Attention）/ 多头自注意力（multi-Head Self-Attention，MHA） 之所以被称为“自”注意力，是因为模型在计算每个位置的表示时，所参考的信息全部来自同一个输入序列本身，而不是来自另一个序列。 ##### 自注意力计算过程 自注意力的完整计算过程如下： $$ Attention(Q,K,V)=Softmax(\\frac{Q \\cdot K^{T}}{\\sqrt{d_k}})V $$ ![transformer_attention_1](/imgs/nlp/transformer_attention_1.png) ![transformer_attention_2](/imgs/nlp/transformer_attention_2.png) **（1）生成Query、Key、Value向量** 自注意力机制的第一步，是将输入序列中的每个位置表示映射为三个不同的向量，分别是 查询（Query）、键（Key） 和 值（Value）。其中$W_q、W_k、W_v$均为可学习的参数矩阵。 - Query：表示当前词的用于发起注意力匹配的向量； $Q=X W_x$ - Key：表示序列中每个位置的内容标识，用于与 Query 进行匹配； $Q=X W_k$ - Value：表示该位置携带的信息，用于加权汇总得到新的表示。 $Q=X W_v$ **（2）计算位置间相关性** 完成 Query、Key、Value 向量的生成后，模型会使用每个位置的 Query 向量与所有位置的 Key 向量进行相关性评分。 评分函数采用向量点积形式。由于在高维空间中，点积的数值可能过大，会影响 softmax 的稳定性，因此在实际计算中对结果进行了缩放。最终的评分函数为： $$ Score(i,j) = \\frac{q_i \\cdot k_j}{\\sqrt{d_k}} $$ 其中$d_k$是key向量的维度，用于缩放点积的幅度（假设q、k方差为1，则$q_i \\cdot k_j$的方差为$d_k$）。 **（3）计算注意力权重** 在得到每个位置与所有位置之间的相关性评分后，模型会使用 softmax 函数进行归一化，确保每个位置对所有位置的关注程度之和为 1，从而形成一个有效的加权分布。 **（4）加权汇总生成输出** 最后，模型会根据注意力权重对所有位置的 Value 向量进行加权求和，得到每个位置融合全局信息后的新表示。 ##### 多头自注意力计算过程 自注意力机制通过 Query、Key 和 Value 向量计算每个位置与其他位置之间的依赖关系，使模型能够有效捕捉序列中的全局信息。 然而，自然语言本身具有高度的语义复杂性，一个句子往往同时包含多种类型的语义关系。要准确理解这类句子，模型需要同时识别并建模多种层次和类型的依赖关系。但这些信息很难通过单一视角或一套注意力机制完整捕捉。 为此，Transformer 引入了多头注意力机制（Multi-Head Attention）。其核心思想是通过多组独立的 Query、Key、Value 投影，让不同注意力头分别专注于不同的语义关系，最后将各头的输出拼接融合。 多头注意力的计算过程如下： $$ MHA(Q,K,V)=Concat(head_1,head_2,..,head_n)W^o $$ **（1）分别计算各头注意力** 每个 Self-Attention Head 独立计算一套注意力输出。 ![Multi_Head_Attention_1](/imgs/nlp/Multi_Head_Attention_1.png) **（2）合并多头注意力** 多个输出矩阵按维度拼接，再乘以得到最终多头注意力的输出。 ![Multi_Head_Attention_2](/imgs/nlp/Multi_Head_Attention_2.png) #### 前馈神经网络层（Feed-Forward Network，FFN） 前馈神经网络（Feed-Forward Network，简称 FFN）是 Transformer 编码器中每个子层的重要组成部分，紧接在多头注意力子层之后。它通过对每个位置的表示进行**逐位置、非线性**的特征变换，进一步提升模型对复杂语义的建模能力。 一个标准的 FFN 子层包含两个线性变换和一个非线性激活函数，中间通常使用 ReLU激活。其计算公式如下： $$ FFN(x)=Linear_2(ReLU(Linear_1(x)))=W_2\\cdot ReLU(W_1\\cdot x+b_1)+b_2 $$ ![transformer_ffn](/imgs/nlp/transformer_ffn.png) #### 残差连接与层归一化（Residual Connection \u0026 LayerNorm） 在 Transformer 的每个编码器层中，每个子层，包括自注意力子层和前馈神经网络子层，其输出都要经过残差连接（Residual Connection）和层归一化（Layer Normalization）处理。这两者是深层神经网络中常用的结构，用于缓解模型训练中的梯度消失、收敛困难等问题，对于Transformer能够堆叠多层至关重要。 ![transformer_res_ln](/imgs/nlp/transformer_res_ln.png) ##### 残差连接 残差连接（Residual Connection，也称“跳跃连接”或“捷径连接”）最初在计算机视觉领域被提出，用于缓解深层神经网络中的梯度消失问题。其核心思想是： 将子层的输入直接与其输出相加，形成一条跨越子层的“捷径”，其数学形式为： $$ y=x+SubLayer(x) $$ 残差连接确保反向传播时，梯度至少有一条稳定通路可回传，是深层网络可稳定训练的关键结构。 ##### 层归一化 每个子层在残差连接之后都会进行层归一化（Layer Normalization，简称 LayerNorm）。它的主要作用是规范输入序列中每个token的特征分布（某个token的表示可能在不同维度上有较大数值差异），提升模型训练的稳定性。 该操作会将每个token的向量调整为均值为 0、方差为 1 的规范分布（LayerNorm 并不会把任意分布变成真正的标准正态分布，它做的是：仿射标准化（affine normalization），即均值 → 0、方差 → 1，而不保证高阶矩（偏度、峰度）符合正态分布），具体的计算公式如下： $$ 总体公式：LayerNorm(x)=\\gamma \\odot \\frac{x-\\mu}{\\sigma+\\epsilon}+\\beta $$ 假如某个token的特征向量为$x=[x^1, x^2,...,x^d]$， 1. 均值计算： 计算该向量在所有特征维度上的平均值 $$ \\mu = \\frac{1}{d} \\sum_{i=1}^{d} x^i $$ 其中$d$为特征维度（向量长度）。 2. 标准差计算 计算向量各维度的标准差 $$ \\sigma = \\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} (x^i-\\mu)^2} $$ 3. 标准化变换 将每个特征值转换为均值为 0、方差为 1 的标准正态分布； $$ \\hat{x^i}=\\frac{x^i-\\mu}{\\sigma+\\epsilon} $$ $\\epsilon$为一个小的常数，防止出现除以0的情况。 4. 缩放和平移（可学习的仿射变换） 让模型可以学习在归一化后的基础上进行适当的调整，保证归一化不会限制模型的表示能力。 $$ LayerNorm(x^i)=\\gamma^i\\cdot\\hat{x^i}+\\beta^{i} $$ $\\gamma^i$和$\\beta^i$为可学习参数。 思考： 1. **为什么 Transformer 用 LayerNorm 而不是 BatchNorm？** | 对比项 | BatchNorm | LayerNorm | | --------------------- | --------- | --------- | | 归一化维度 | batch 维 | feature 维 | | 是否依赖 batch size | 是 | 否 | | NLP / Transformer 适配性 | 差 | 极好 | | 推理阶段一致性 | 有偏差 | 完全一致 | | 变长序列 | 不适合 | 天然支持 | ##### Pre_Norm 在更深层次网络使用效果可能更好，在pytorch的nn.Transformer中通过norm_first参数设置。 Transformer的被称之为`Post_Norm`： $$ LayerNorm(x+SubLayer(x)) $$ Pre_Norm方法： $$ x+SubLayer(LayerNorm(x)) $$ #### 位置编码（Positional Encoding） Transformer 模型完全摒弃了 RNN 结构，可以并行处理所有位置的信息，但也无法天然地捕捉词语之间的顺序关系。 为了解决这一问题，Transformer 引入了一个关键机制——位置编码（Positional Encoding）。该机制为每个词引入一个表示其位置信息的向量，并将其与对应的词向量相加，作为模型输入的一部分。 **编码策略** 1. 最直接的方式：使用绝对位置编号来表示每个词的位置。 问题：越靠后的 token 位置编码就越大，若直接与词向量相加，会造成数值倾斜，让模型更关注位置，而忽视词义。 2. 位置编号归一化为[0, 1]区间，例如用$\\frac{Pos}{T}$表示位置，其中 T是句子长度。 问题：使用$\\frac{Pos}{T}$作为位置编码在形式上是合法的，但： - 它将相对位置信息与序列长度强耦合，导致相同相对距离在不同长度下映射到不同编码值，使 Attention 无法学习稳定的相对位置关系； - 同时该编码在内积空间中退化为低秩结构，表达能力不足，且在长序列外推时表现不稳定。 - 相比之下，正弦位置编码具备可线性解析的位移结构，能够在 Attention 中自然建模相对位置并支持长度外推。 3. Transformer：使用了一种基于正弦（sin）和余弦（cos）函数的位置编码方式，具体定义如下： $$ PE_{（pos,2i）}=\\sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})\\\\\\\\ PE_{（pos,2i+1）}=\\cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}) $$ 其中： - pos是当前词在序列中的位置； - i用于表示位置编码向量的维度索引，2i表示偶数维，2i+1表示奇数维； - $d_{model}$是词向量的维度大小。 Transformer提出的这种编码方式不依赖任何可学习参数，数值稳定，并具备以下优势： - 所有值都在[−1,1]范围内，数值稳定 - 编码方式固定、可预计算，无需训练； - 相同位置的编码在不同句子中保持一致； - 编码之间具有数学规律，便于模型在注意力机制中感知词语之间的相对位置关系。 ##### 问题本质：位置编码要解决什么？ Transformer 的自注意力机制： $$ \\mathrm{Attn}(Q,K,V)=\\mathrm{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V $$ 本身 **对序列是置换不变的（permutation invariant）**。 位置编码的目标是： \u003e 向 token 表示中注入一个 **与绝对位置和相对位置有关、可泛化、可比较的信号** 因此一个“合格”的位置编码至少需要满足： 1. **同一位置的编码在不同样本中一致** 2. **不同位置可区分** 3. **相对位置信息可被线性或低阶运算提取** 4. **对未见过的序列长度具备外推能力** \u003e 如果方案二的$\\frac{pos}{T}$改为$\\frac{pos}{C}, c=10000$会有问题吗？ 1. 定义 $$ \\mathrm{PE}(pos)=\\frac{pos}{C},\\quad C=10000 $$ 2. 绝对位置一致性：无论句长多少，数值一致。 3. 但问题依然存在（关键）： 1. 位置编码是一维标量：所有位置落在一条直线上，表达能力极弱：Attention 只能感知：$Q(K+PE)^\\top$，线性偏移，难以刻画复杂相对关系 2. 尺度问题（推导）：最大位置$pos_{max}$：$\\frac{pos_{max}}{10000} \\ll 1$，与 embedding 数值（通常 $\\sim \\mathcal{N}(0,1)$）相比：$\\mathrm{Var}(\\mathrm{PE}) \\ll \\mathrm{Var}(\\mathrm{Token})$，位置几乎被“淹没” 3. 无法表达相对位移（核心）：设两个位置：$\\Delta = \\frac{pos_2 - pos_1}{10000}$，Attention 中的内积项：$(x + p_1)^\\top(x + p_2)$，只能学到**线性距离差**，无法区分：“近但在前”、“近但在后”、“周期性结构” \u003e 为什么 Transformer 使用 sin / cos（核心推导） 1. 定义（原始公式） $$ \\begin{aligned} \\mathrm{PE}_{(pos,2i)} \u0026= \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right) \\\\\\\\ \\mathrm{PE}_{(pos,2i+1)} \u0026= \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right) \\end{aligned} $$ 2. 核心性质一：相对位置可线性表示（关键推导） 利用三角恒等式： $$ \\sin(a+b) = \\sin a \\cos b + \\cos a \\sin b \\\\\\\\ \\cos(a+b) = \\cos a \\cos b - \\sin a \\sin b $$ 设： $$ PE(pos+k) = A(k)\\cdot PE(pos) $$ 其中 (A(k)) 是仅依赖于 (k) 的线性变换。 Attention 可通过线性层感知相对位移 3. 核心性质二：多尺度位置分解 $$ \\omega_i = 10000^{-2i/d} $$ * 小 (i)：低频 → 长距离 * 大 (i)：高频 → 局部顺序 即： $$ pos \\mapsto (\\sin(\\omega_1 pos), \\dots, \\sin(\\omega_d pos)) $$ 等价于： \u003e 将位置映射到一个 **多尺度 Fourier 特征空间** 4. 核心性质三：对任意长度具备外推性 sin / cos： * 定义在整个实数域 * 不依赖最大长度 * 不需要重新训练 embedding \u003e `pos / seq_len` 完全不具备该性质 --- ### 解码器（Decoder） 编码器也由多个结构相同的解码器层堆叠组成。每个Decoder Layer都包含三个子层，分别是Masked自注意力子层、编码器-解码器注意力子层（Encoder-Decoder Attention）和前馈神经网络子层（Feed-Forward Network）。 - Masked自注意力子层（Masked Self Attention）： - 用于建模当前位置与前文词之间的依赖关系。 - 为了在训练时模拟逐词生成的过程，引入遮盖机制（Mask），限制每个位置只能关注它前面的词。 - 编码器-解码器注意力子层（Encoder-Decoder Attention）： - 用于建模当前解码位置与源序列各位置之间的依赖关系。 - 通过注意力机制，模型能够根据当前状态从编码器的输出中提取相关上下文信息（相当于 Seq2Seq 模型中的 Attention 机制）。 - 前馈神经网络子层（Feed-Forward Network）：与编码器中结构完全一致，对每个位置的表示进行非线性变换，增强模型的表达能力。 每个子层后也都配有残差连接与层归一化（Layer Normalization），结构设计与编码器保持一致，确保训练的稳定性和效率。 此外，解码器在输入端同样需要加入位置编码（Positional Encoding），用于提供序列中的位置信息，其计算方式与编码器中相同。 在输出端，解码器的隐藏向量会送入一个线性变换层（Linear），映射为词表大小的向量，并通过 Softmax 生成一个概率分布，用于预测当前应输出的词。 #### Masked自注意力子层（Masked Self Attention） - 并行计算：一次性输入完整目标序列，同时预测每个位置的词。 - 遮盖机制（Mask）：限制每个位置只能关注它前面的词。 ![encoder_mask](/imgs/nlp/encoder_mask.png) \u003e 注意：在推理阶段，我们只使用解码器最后一个位置的输出作为当前步的预测结果。 Mask 机制的实现非常简单：只需将注意力得分矩阵中当前位置对其后续位置的评分设置为`−∞`，如下图所示： ![mask_1](/imgs/nlp/mask_1.png) 这样，在经过 softmax 运算后，这些位置的权重会趋近于 0。最终在加权求和时，来自未来位置的信息几乎不会参与计算，从而实现了“**当前词只能看到它前面的词**”的约束。如下图所示： ![mask_2](/imgs/nlp/mask_2.png) #### 编码器-解码器注意力子层（Encoder-Decoder Attention） 该子层的主要作用是：建模当前解码位置与源语言序列中各位置之间的依赖关系，帮助模型在生成目标词时有效地参考输入内容，相当于Seq2Seq模型中的注意力机制。 编码器-解码器注意力的核心机制与前面讲过的自注意力机制完全一致，区别仅在于： - Query 来自解码器当前的输入表示，即当前生成状态； - Key和Value 来自编码器的输出表示，即整个源序列的上下文。 ### 训练与推理机制 Transformer 的训练与推理都基于自回归生成机制（Autoregressive Generation）：模型逐步生成目标序列中的每一个词。然而，在实现方式上，训练与推理存在明显区别。 #### 模型训练 并行计算+mask机制 #### 模型推理 推理阶段，模型每一步都要重新输入当前已生成的全部词，通过自注意力机制建模上下文关系，预测下一个词。 模型会基于**完整前文重新计算注意力分布**，生成当前步的输出。由于每一步的输入依赖前一步结果，整个过程必须**顺序执行，无法并行**。 每步输出的是一个词的概率分布，最终生成结果也可使用不同的**解码策略** - 贪心搜索（Greedy） - 束搜索（Beam Search） ### Pytorch API使用 ``` python from torch import nn # 经典 Transformer 结构 transformer = nn.Transformer( d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, batch_first=True ) ``` PyTorch 中的 Transformer 模块由以下几个核心类构成： #### nn.Transformer \u003e [nn.Transformer](https://docs.pytorch.org/docs/stable/generated/torch.nn.Transformer.html) 封装了完整的 Transformer架构，由编码器和解码器组成。作为顶层接口，适用于需要同时使用编码器和解码器的任务，如机器翻译。支持用户通过参数自定义层数、注意力头数、隐藏维度等模型结构。 #### nn.TransformerEncoder \u003e [nn.TransformerEncoder](https://docs.pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html) 实现了Transformer编码器结构，由多个编码器层的堆叠而成，用于将输入序列编码为上下文相关的表示。 #### nn.TransformerDecoder \u003e [nn.TransformerDecoder](https://docs.pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html) 实现了Transformer解码器结构，由多个解码器层堆叠而成，用于基于编码结果逐步生成目标序列。 #### nn.TransformerEncoderLayer \u003e [nn.TransformerEncoderLayer](https://docs.pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html) 实现了单个编码器层结构，包含一个多头自注意力子层和一个前馈神经网络子层，两者均带有残差连接和 LayerNorm。 #### nn.TransformerDecoderLayer \u003e [nn.TransformerDecoderLayer](https://docs.pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html) 实现了单个解码器层结构，包含自注意力、编码器-解码器注意力、前馈子层，同样配有残差连接和 LayerNorm。 ### The Annotated Transformer 解读 \u003e [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/) \u003e \u003e [Transformer从入门到精通](https://www.cnblogs.com/Icys/p/18119309/annotated-transformer-chinese) 《The Annotated Transformer》是对Transformer模型的详细注释和实现，基于论文《Attention Is All You Need》。该文档逐行解释了Transformer的架构、编码器-解码器堆栈、自注意力机制、位置编码等关键组件，并提供了PyTorch实现。 --- ## 预训练模型 早期的自然语言处理方法通常针对每个具体任务单独训练模型，且严重依赖大量人工标注数据。虽然在部分场景下效果可观，但也暴露出显著局限： - **语言知识难以复用**：每个模型都需从零开始训练，导致训练成本高、效率低； - **强依赖高质量标注**：在医疗、法律等专业领域，标注数据获取困难且代价高昂。 为解决这些问题，研究者提出了新的建模范式——**“预训练 + 微调”**： - **预训练阶段**：在大规模未标注语料上训练语言模型，学习词汇、句法和上下文等通用语言规律； - **微调阶段**：将预训练模型迁移至具体任务，仅需少量标注数据即可完成任务适配。 这一方法显著提升了模型的通用性和开发效率，已成为当前 NLP 的主流技术路线，并广泛应用于文本分类、问答系统、翻译、对话等任务中。 --- ### 预训练模型分类 预训练语言模型几乎都构建在 Transformer 架构之上。相较于传统的循环神经网络，Transformer具有以下优势： - 并行计算效率高，适合大规模训练； - 上下文建模能力强，可捕捉长距离依赖； - 结构通用灵活，可适配多种任务类型； - 易于扩展与迁移，支持参数堆叠与多任务学习。 因此，Transformer 成为预训练模型的主流基础架构。根据 Transformer 的使用方式不同，预训练模型大致可分为以下三类： #### 解码器（Decoder-only）模型 仅使用Transformer解码器，代表模型为GPT（Generative Pre-trained Transformer），其由 OpenAI于2018年6月提出，论文题为[《Improving Language Understanding by Generative Pre-Training》](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)。 #### 编码器（Encoder-only）模型 仅使用Transformer 编码器，代表模型为BERT（Bidirectional Encoder Representations from Transformers），由Google于2018年10月提出，论文题为[《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》](https://arxiv.org/pdf/1810.04805)。 #### 编码器-解码器（Encoder-Decoder）模型 同时使用Transformer编码器和解码器，代表模型为T5（Text-to-Text Transfer Transformer），由Google于2019年10月提出，论文题为[《Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer》](https://arxiv.org/pdf/1910.10683)。 自 GPT、BERT 和 T5 等模型发布以来，基于 Transformer 的预训练模型不断涌现，模型架构和能力持续演进。下图总结了 2018 年至 2023 年间具有代表性的模型及其发展脉络。 ![pretrained_model](/imgs/nlp/pretrained_model.png) --- ### 主流模型 #### GPT GPT（Generative Pre-trained Transformer）是第一个系统性提出“预训练 + 微调”范式的语言模型。 其核心思想是通过大规模无监督语料进行生成式语言建模预训练，即训练模型根据左侧上下文预测下一个词，从而让模型学习自然语言的通用语法、语义和上下文依赖能力。完成预训练后，再通过微调适应具体的下游任务。 GPT首次展示了生成式语言模型在自然语言理解任务中的广泛迁移能力，为后续 GPT 系列及整个预训练语言模型的发展奠定了基础。 ##### 模型结构 GPT基于Transformer的解码器结构，但与标准的Transformer解码器并不完全相同，GPT具体结构如下图所示： ![gpt](/imgs/nlp/gpt.png) 1. 输入嵌入层（Text \u0026 Position Embedding） GPT不同于原始Transformer的一点在于：位置编码采用的是可学习的位置嵌入（learnable positional embedding）。这意味着每个位置对应一个可训练的向量，模型可以在训练过程中自动优化这些向量，而非使用不可训练的三角函数编码（如正弦/余弦函数）。 2. 解码器 GPT不同于原始Transformer：掩码多头自注意力（12头） 3. 输出层 根据任务不同，GPT模型的输出可以接入不同的任务头： - Text Prediction（文本预测）：用于下一个词的生成，输出是词表大小的概率分布，经过Softmax获得，预训练阶段使用的便是该任务头。 - Task Classifier（任务分类器）：该任务头多用于模型微调阶段，以适配具体的下游任务。通过提取特定位置的表示（如最后一个token）对整个输入文本进行分类（如情感分析、话题识别等）。 ##### 预训练 GPT 的预训练阶段采用生成式语言建模（Generative Language Modeling）作为训练目标，在大规模无监督文本上进行自监督学习。 具体而言，模型的任务是基于已观察到的前文上下文，预测当前词的位置应出现的词，从而学习自然语言的统计规律与上下文依赖关系。 这种自回归语言建模方式不依赖人工标注，训练样本可以直接从原始文本中自动构建，极大地降低了构建数据的成本。 在实践中，GPT-1 使用了一个名为 BooksCorpus 的英文语料库，包含来自 7000 多本小说的完整书籍文本，总规模约 8 亿词。该语料语言自然、上下文完整，非常适合训练具备长距离依赖建模能力的语言模型。 ##### 微调 GPT的微调阶段是在完成无监督语言建模预训练之后，使用有监督的任务数据对模型进行进一步训练，使其适应具体的下游任务。 **微调的核心思路**是：在保留预训练语言建模能力的基础上，利用标注数据对整个模型进行端到端优化，从而实现知识迁移。 具体实践中，GPT采用了如下两个关键措施： - 添加任务输出层：在预训练模型顶部引入一个线性输出层（Linear Head），用于将 GPT 的隐藏状态映射为下游任务所需的标签或输出。 - 统一输入格式设计：GPT 作为自回归语言模型，其输入需为连续的文本序列。因此，在微调过程中需将各种下游任务转化为统一的文本输入格式。 下图展示了不同任务的微调逻辑： ![gpt_finetune](/imgs/nlp/gpt_finetune.png) 通过这种方式，GPT 在保留预训练模型结构和参数的基础上，仅添加极少量新参数（如线性层），便可高效完成从语言建模到多种下游任务的迁移。 此外，统一的输入格式设计进一步简化了多任务处理流程，使 GPT 能以一致的方式应对多种 NLP 任务，从而展现出强大的通用性与扩展性。 #### BERT BERT（Bidirectional Encoder Representations from Transformers）是由 Google 于 2018 年提出的一种语言预训练模型。其核心创新在于采用 Transformer 的编码器（Encoder）结构，通过双向自注意力机制，在建模每个 token 表示时同时整合左右两个方向的上下文信息，从而获得更准确、更丰富的语义表示。 BERT 的设计更侧重于自然语言理解类任务，广泛应用于文本分类、序列标注、句子匹配等场景。模型发布后，在多个语言理解基准测试中取得了前所未有的领先成绩，推动 NLP 研究全面转向“预训练 + 微调”的通用建模范式。 ##### 模型结构 BERT 基于标准的 Transformer 编码器构建，其提供了两种模型规模，分别是BERT-base和BERT-large。 具体参数规格如下： |模型版本\t|层数（Layers）\t|模型维度（d_model）\t|注意力头数（Heads）|参数量| | --------------|------|------ | --------- | ---- | |BERT-base|\t12|\t768|\t12|\t1.1 亿| |BERT-large|\t24\t|1024\t|16\t|3.4 亿| 1. 输入表示层 BERT 的每个输入 token 表示由三部分嵌入相加组成： - Token Embedding：词本身的语义表示； - Position Embedding：表示 token 在序列中的位置，为可学习向量； - Segment Embedding：用于区分句子对任务中的两个句子，分别用一个可学习的向量表示。 此外，BERT 输入中通常包含两个特殊符号： - [CLS]：句首标志，其输出向量常用于下游的文本分类任务； - [SEP]：句间分隔符，出现在每个句子末尾。 2. 编码器 编码器结构同原始Transformer相同 3. 输出层 根据下游任务的类型，BERT 可以接入不同的任务输出头： Token-Level 任务（如命名实体识别）：使用每个位置的输出表示； Sequence-Level 任务（如文本分类、句子对分类）：使用特殊 token [CLS] 的输出表示，输入时被加在序列开头，专门用于汇总整个序列的语义信息。 ##### 预训练 BERT 的预训练阶段包含两个核心任务：掩码语言模型（Masked Language Modeling, MLM） 和 下一句预测（Next Sentence Prediction, NSP），分别用于学习词级语义和句间逻辑关系。 1. 掩码语言模型（MLM） 为实现双向语言建模，BERT 不采用传统的从左到右或从右到左预测方式，而是引入了掩码语言模型。在训练中，BERT 会随机遮盖输入序列中约 15% 的 token，并训练模型根据上下文预测被遮盖的词。 遮盖策略如下： - 80% 的被遮盖 token 替换为 [MASK]； - 10% 替换为随机词； - 10% 保持原词不变。 这种机制让模型在预训练时既能看到左侧上下文，也能看到右侧上下文，真正实现深度双向建模。 2. 下一句预测（NSP） 为了提升模型理解句间关系的能力，BERT 引入了“下一句预测”任务。训练时模型接收两个句子，判断第二句是否是第一句的真实后续句，其中： - 50% 的训练样本是上下文中真实相邻的句子（正例）； - 50% 是从语料中随机采样的非相邻句子（反例）。 在预训练时，BERT 同时优化 MLM 和 NSP 两个目标，具体操作如下图所示： ![bert](/imgs/nlp/bert.png) ##### 微调 在预训练完成后，BERT 可通过少量微调适配多种下游任务，如文本分类、句子匹配、问答系统、序列标注等。微调时，模型主体结构保持不变，仅在顶部添加一个任务特定的输出层，并使用下游任务数据对整个模型进行训练。 BERT 的输入格式在微调阶段基本保持与预训练一致，仍以 token 序列为输入，使用 [CLS] 和 [SEP] 等特殊符号。不同任务的差异主要体现在输出层设计，以及从模型输出中提取哪些表示进行预测。 #### T5 T5（Text-to-Text Transfer Transformer）是 Google Research 于 2020 年提出的一种统一预训练框架，它首次在完整的 Transformer 编码器-解码器结构（Encoder-Decoder）上实现了预训练语言模型。 T5的核心思想是将所有自然语言处理任务统一表示为“文本到文本”的转换问题（Text-to-Text Framework），即无论输入是文本分类、问答还是翻译，模型的输入输出均是自然语言形式的字符串，如下图所示： ![T5](/imgs/nlp/T5.png) 这一设计使得 T5 可以通过同一个模型架构、同一套预训练机制完成多种任务，具备极强的统一性与迁移能力。 ##### 模型结构 T5模型大体遵循原始的Transformer架构。 ##### 预训练 T5模型的预训练目标被称为（Corrupted span prediction，CSP），具体过程如下： 1. 随机遮盖输入文本中的若干连续片段（span）； 2. 将每个被遮盖的连续片段替换为一个个特殊token； 3. 令模型学习生成这些遮盖片段的内容，作为输出序列。 ![T5_example](/imgs/nlp/T5_example.png) 这种方式既保留了模型的双向建模能力，又为训练提供了明确的“生成式”学习信号，使模型可以更自然的适配下游任务。 ##### 微调 T5微调阶段需要将所有任务转换为文本到文本的形式，例如： |任务类型|\t输入形式|\t目标输出| |---|---|---| |翻译|\ttranslate English to German: That is good.|\tDas ist gut.| |情感分类|\tsentiment: This movie was great.\t|positive | |问答|\tquestion: What is the capital of France? context: France is a country...|\tParis| ### 主流技术 #### Tokenizer | 技术 | 关键词 | 代表模型 | 关键机制 | 论文出处 | | ----------------------------- | ------- | ------------ | ----------- | ------------------------------------------- | | Byte Pair Encoding (BPE) | 基于频率合并 | GPT 系列初期 | 贪心合并最频对 | *Gage 1994*；*Sennrich et al. 2016* | | WordPiece | 最大化似然 | BERT 系列 | 类似 BPE + LM | *Schuster \u0026 Nakajima 2012*；*Wu et al. 2016* | | SentencePiece (Unigram LM) | LM 模型 | T5, ALBERT | 子词 LM + EM | *Kudo 2018* | | Byte-Level BPE | 基于字节 | GPT-2/3/Neox | 无需预清洗 | *Radford et al. 2019* | | Unigram LM | 概率模型 | XLM-R | 子词概率 + 剪枝 | *Kudo \u0026 Richardson 2018* | | Morphological / Feature-aware | 形态/语言规则 | 少数语言专用 | 融语法/形态学 | 多篇任务论文 | #### 位置嵌入（Positional Encoding） 当前预训练模型中的位置建模方法，已经从**“绝对位置编码”**演化为**“相对位置与几何约束”**，主流可分为五大类： 1. **绝对位置编码（Absolute Position Embedding, APE）** * 可学习（Learned APE） * 固定函数（Sinusoidal PE） 2. **相对位置编码（Relative Position Encoding, RPE）** * 相对位置偏置（Relative Bias） * 相对位置向量 3. **旋转位置编码（RoPE）** 4. **线性偏置位置编码（ALiBi）** 5. **混合 / 改进型方法（如 YaRN、xPos、NTK-aware RoPE）** **各技术概览：** | 方法 | 是否主流 | 代表模型 | | --------------- | ---------- | ------------ | | Sin/Cos APE | 否 | 早期 Transformer，教学 / baseline，**现代大模型基本不用** | | Learned APE | 否 | BERT，**在长上下文预训练模型中基本被淘汰** | | Relative Bias | 是（Encoder） | T5 / PaLM，**Encoder-heavy 模型中的事实标准** | | RoPE | 是（Decoder） | LLaMA / Qwen/DeepSeek，**Decoder-only LLM 的事实标准** | | ALiBi | 次主流 | BLOOM，部分长上下文变体模型 | | YaRN / NTK-RoPE | 是（长上下文） | LLaMA Long，**主流 LLM 长上下文扩展的工程标准手段** | --- ##### 绝对位置编码（已逐渐退出主流） 1. 固定正弦位置编码（Sinusoidal PE） $$ \\begin{aligned} PE_{(pos,2i)} \u0026= \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right) \\ PE_{(pos,2i+1)} \u0026= \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right) \\end{aligned} $$ **优点** * 不依赖最大长度 * 理论上具备外推性 * 相对位移可线性表示（Fourier 特性） **致命问题** * **与注意力是“加性耦合”**，不是结构性约束 * 在超长上下文中性能显著下降 * 实际大模型中已很少单独使用 **论文** * *Attention Is All You Need*, Vaswani et al., 2017 2. 可学习绝对位置嵌入（Learned APE） $$ x_i = e_i + p_i,\\quad p_i \\in \\mathbb{R}^d $$ **优点** * 表达能力强 * 短序列任务效果好 **核心问题（工程上不可接受）** 1. **无法外推到未见长度** 2. embedding 表大小与 max length 线性增长 3. 在长上下文任务中不稳定 **论文 / 模型** * BERT（Devlin et al., 2018） * GPT-1 / GPT-2（Radford et al.） ##### 相对位置编码（第一代主流替代方案） 1. Transformer-XL 相对位置编码（向量形式） Attention 分数中显式引入相对距离： $$ \\mathrm{Attn}*{ij} = q_i^\\top k_j + q_i^\\top r*{i-j} $$ **特点** * 相对位置显式建模 * 对序列长度泛化更好 **问题** * 实现复杂 * 计算与存储成本高 * 不利于大规模并行 **论文** * *Transformer-XL*, Dai et al., 2019 2. 相对位置偏置（Relative Position Bias）【工程主流】 **机制（以 T5 为代表）** $$ \\mathrm{Attn}*{ij} = q_i^\\top k_j + b*{i-j} $$ 其中： * (b_{i-j}) 是可学习标量 * 通常采用 **bucket 化距离** **优点** * 参数量极小 * 与 Attention 结构天然融合 * 稳定、易实现、可扩展 **论文与模型** * *T5*, Raffel et al., 2020 * PaLM * FLAN * ViT v2（部分变体） ##### 旋转位置编码（RoPE） 1. RoPE（Rotary Position Embedding） 不是“加位置”，而是**旋转 Query / Key 空间**： $$ \\mathrm{Attn}*{ij} = (R*{pos_i} q_i)^\\top (R_{pos_j} k_j) $$ 等价于： $$ \\mathrm{Attn}_{ij} \\propto \\cos(\\theta(pos_i - pos_j)) $$ **数学本质** * 在复平面进行相位旋转 * Attention 内积只依赖相对位移 **优点** * 无额外参数 * 与注意力内积深度耦合 * 对长序列泛化显著优于 APE **论文** * *RoFormer*, Su et al., 2021 ##### 线性偏置位置编码（ALiBi） 1. ALiBi（Attention with Linear Biases） $$ \\mathrm{Attn}_{ij} = q_i^\\top k_j - \\alpha_h |i - j| $$ * 每个 attention head 一个 slope * 无 embedding、无旋转 **优点** * 极强长度外推能力 * 实现极简 * 推理成本最低 **缺点** * 表达能力弱于 RoPE * 在复杂语言建模中略逊 **论文** * *Train Short, Test Long*, Press et al., 2021 ##### RoPE 改进与现代长上下文方案（2023–） NTK-aware RoPE / xPos / YaRN（工程增强） 原始 RoPE 在超长上下文中： * 角频率过密 * 远距离 token 混叠 **代表工作** 1. NTK-aware RoPE * 调整频率缩放 * LLaMA 2 长上下文版本 2. YaRN * 分段频率插值 * 保留低频稳定性 * 论文：*YaRN*, Peng et al., 2023 #### Attention（MHA/GQA/MQA） 标准多头注意力（MHA）在单层对长度为 (n) 的序列、batch size (B)、头数 (H)、每头维度 (d_k) 的情况下，关键成本（单层）为： * 计算量（近似）：$(O(B \\cdot H \\cdot n \\cdot d_k + B \\cdot H \\cdot n^2))$（QK 内积产生 ($n\\times n$) 矩阵） * 内存（KV 保存供自回归解码使用时）：若保存 keys/values 到缓存，KV tensor 大小为 $(B \\times n \\times H \\times d_k)$。 这说明： - （a）自回归**推理**时 KV 缓存的读写成为瓶颈 - （b）若 H 很大，KV 的存储/带宽按比例放大。 原始 MHA 的设计在训练/解码规模放大时会碰到显著的 **内存带宽与显存** 问题。 ##### Multi-Query Attention (MQA) **核心思想**：所有 `query heads` 共用同一组` Key/Value` 投影（即只为 `Keys/Values` **保留 1 个头而不是 H 个**），而每个 head 仍有自己独立的 Query。 **数学 / 复杂度变化** * 原 KV 大小：$(B \\times n \\times H \\times d_k)$ * MQA KV 大小：$(B \\times n \\times 1 \\times d_k = B \\times n \\times d_k)$ **KV 大小减少因子 ≈ (H)**（近似），因此 KV **读写带宽**与**缓存显存**在自回归解码时降低近 H 倍（最重要的工程收益）。 计算上：注意力分数计算仍需要为每个 `query head` 计算 $(Q_h K_{shared}^\\top)$，但对 KV 的加载/传输开销大幅减少。 **工程/经验影响** * **极大提升解码吞吐/降低延迟**（尤其在长上下文与大 batch 自回归生成时）。 * 可能引入**质量/表达能力下降**，因为不同 head 不能再拥有各自的 K/V 表征（有些语义/模式分工被弱化）。 * 在工程上常用于 decoder-only LLM 的推理优化；通常需要在训练/微调阶段适配或 uptraining 才能最小化性能下降。 **代表/出处** * Shazeer, *Fast Transformer Decoding: One Write-Head is All You Need* (2019). ##### Grouped-Query Attention (GQA) **核心思想**：在 MHA 与 MQA 之间做折衷。把 (H) 个 `query head` 按组分成 G 组$(1 \\le G \\le H)$，每组共享一组 `K/V`（组内的多个 `query heads` 共享该组的 `K/V`）。当 `G=H` 时退化为 MHA；当 `G=1` 时退化为 MQA。 **数学 / 复杂度** * KV 大小：$(B \\times n \\times G \\times d_k)$。相对于 MHA 的压缩因子为 (H/G)。 * 当 $(G \\ll H)$ 时，可显著减少 KV 缓存大小与带宽，但保留比 MQA 更好的多样性。 **工程与训练要点** * GQA 在实务中常采用 **先把已有 MHA checkpoint 转换为 GQA，再做少量 uptraining（低成本微调）**，即可恢复大部分质量（Ainslie et al. 的方法论）。这使 GQA 成为实用迁移/加速策略，而不需要从头训练新的架构。 * 选择 (G) 是工程折中：较小的 (G) 带来更大推理加速但更可能损失建模能力；文献证明用少量 uptraining（如 5% 原训练 compute）可以把损失压缩到可接受范围。 **代表/出处** * Ainslie et al., *GQA: Training Generalized Multi-Query Transformer Models* (EMNLP 2023). ##### 为什么 MQA/GQA 在「推理工程」中有效（量化说明） 考虑自回归生成逐步解码（生成第 (t) 步）时必须读取并使用前 (t-1) 个 token 的 KV。假设每个 KV 元素为 2 字节（BF16），batch B=1： * MHA KV bytes ≈ $((t-1)\\cdot H \\cdot d_k \\cdot 2)$ * MQA KV bytes ≈ $((t-1)\\cdot 1 \\cdot d_k \\cdot 2)$ 因此在长上下文（t 很大）或多卡 KV 传输场景下，带宽/显存压力下降近 (H) 倍，能显著降低推理延迟与跨卡通信开销（实测在多篇工程文章/博客里呈现为数倍到十倍的吞吐改进）。GQA 在 (H/G) 维持同类效果但更平衡质量。 ##### FlashAttention：微架构/算法级的加速（与 MQA/GQA 不冲突） **FlashAttention** 不是新的注意力「结构」而是 **IO-aware 的 exact attention 实现**：它通过 tile 分块、在片上 SRAM 中完成 softmax 计算，避免构造完整 (n\\times n) 中间矩阵，减少 HBM↔SRAM 读写，从而实现**内存与时间双降**（并能配合 MHA/MQA/GQA 使用）。 * 对比：标准实现需要存储 (QK^\\top) 或若干中间缓冲；FlashAttention 用流式（tiling）做数值稳定的 softmax，memory footprint 大幅降低，wall-clock 加速显著。 * 工程价值：训练与推理中都能直接减少显存、加速训练步长、延长可支持的上下文长度。[\"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\"](https://arxiv.org/abs/2205.14135?utm_source=chatgpt.com ) ##### 稀疏/图式注意力（窗口 + 全局 / 随机） * **Longformer**：局部滑动窗口 + 少数 global tokens，复杂度 (O(n))。适合许多长文档任务。 * 参考：\"Longformer: The Long-Document Transformer\" * **BigBird**：窗口 + 随机 + 全局三分法，理论上能近似完整 attention（Turing complete），并实现线性复杂度。 * \"Big Bird: Transformers for Longer Sequences\" ##### 低秩 / 投影近似 * **Linformer**：对 K/V 在序列维投影到低维空间，假设 attention 矩阵近似低秩，从 (O(n^2)) 降到 (O(n))。 * \"Linformer: Self-Attention with Linear Complexity\" ##### 随机特征 / 核线性化（线性注意） * **Performer (FAVOR(+))**：用随机特征逼近 softmax kernel，使 attention 复杂度变为线性（理论上有可控误差界）。适合超长序列但在某些任务上与 exact attention 有差距。 * \"[2009.14794] Rethinking Attention with Performers\" ##### LSH / 分组注意力 * **Reformer**：用 Locality-Sensitive Hashing 将相似 queries 聚类，仅与同 bucket keys 交互，复杂度 (O(n \\log n))，并配合可逆网络节省激活内存。 * \"Reformer: The Efficient Transformer\" #### LayerNorm ##### PostNorm（原始transformer使用） 原始设计（Vaswani et al., 2017）Transformer 使用的是： * **LayerNorm** * **Post-LN 结构** 即： $$ y = \\mathrm{LN}(x + \\mathrm{Sublayer}(x)) \\\\\\\\ \\mathrm{LN}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta $$ 在小模型（6–12 层）中是可行的，但在大模型中暴露出严重问题。 - （1）深层梯度不稳定（致命） 对输入梯度近似为： $$ \\frac{\\partial y}{\\partial x} \\approx \\frac{\\partial \\mathrm{LN}}{\\partial x} \\cdot (I + \\partial \\mathrm{Sublayer}) $$ LayerNorm 的梯度是**缩放 + 去均值的非对角矩阵**，导致： * 梯度被反复缩放 * 残差通道不再是“近似恒等映射” * 层数 ↑ → 梯度消失 / 爆炸 **结论**： \u003e 原始 LN + Post-LN 结构无法稳定扩展到 40+、80+ 层 ##### Pre-LayerNorm（结构级革命） **结构变化** 从：$y = \\mathrm{LN}(x + \\mathrm{Sublayer}(x))$ 变为：$y = x + \\mathrm{Sublayer}(\\mathrm{LN}(x))$ **核心数学差异** 残差分支的梯度： $$ \\frac{\\partial y}{\\partial x} \\approx I + \\frac{\\partial \\mathrm{Sublayer}}{\\partial x} $$ 即： * 梯度可以**绕过 LayerNorm** * 残差路径近似恒等映射 **工程结果** * 训练深度可从 12 层 → 80+ 层 * 几乎所有现代 LLM 的**必要条件** **论文支撑** * *On Layer Normalization in the Transformer Architecture* Xiong et al., 2020 * *Understanding the Difficulty of Training Transformers* Liu et al., 2020 **实际采用** * GPT-2 之后所有 GPT 系列 * LLaMA / Qwen / DeepSeek / Mistral ##### RMSNorm（LayerNorm 的结构简化） \u003e RMSNorm：去均值是否真的必要？ **RMSNorm 定义** $$ \\mathrm{RMSNorm}(x) = \\gamma \\cdot \\frac{x}{\\sqrt{\\frac{1}{d}\\sum_i x_i^2 + \\epsilon}} $$ **与 LayerNorm 的本质差异** | 项目 | LayerNorm | RMSNorm | | ------- | --------- | ------- | | 去均值 | 是 | 否 | | 方差归一 | 是 | 是 | | 可学习偏置 β | 有 | 无 | | 参数量 | 2d | d | **理论与经验动机** 在 **Pre-LN 架构**下： * 均值偏移不会随深度累积（残差稳定） * 方差才是主要不稳定来源 * 去均值的收益极小，但计算与通信成本真实存在 **工程收益** * 计算更快 * 数值更稳定（尤其在 BF16） * Kernel 更容易融合 **论文** * *Root Mean Square Layer Normalization* Zhang \u0026 Sennrich, 2019 **实际采用** * LLaMA 系列 * Qwen 系列 * DeepSeek * Mistral \u003e **RMSNorm 是当前 Decoder-only LLM 的事实标准** ##### Norm 位置与残差缩放的协同设计 **问题** 即使 Pre-LN + RMSNorm，在 **100+ 层**时仍会出现： * 残差累积导致激活范数缓慢上升 **解决策略（多种等价形式）** - （1）Residual Scaling $$ y = x + \\alpha \\cdot \\mathrm{Sublayer}(\\mathrm{Norm}(x)) $$ 其中：$\\alpha \\sim \\frac{1}{\\sqrt{L}}$ - （2）小初始化（等价效果） * 输出投影权重使用更小 std * FFN / Attention 输出更“保守” **理论直觉** 让残差路径在深度上保持： $$ \\mathbb{E}|x^{(l)}|^2 \\approx \\text{常数} $$ **论文 / 技术来源** * *DeepNet: Scaling Transformers to 1,000 Layers* Wang et al., 2022 * *Stabilizing Transformer Training by Preventing Internal Covariate Shift*（相关分析） **实际情况** * 多数大厂模型使用该思想 * 但**不会在论文或代码中显式标注**（工程细节） ##### Norm 相关的其他重要变体（较少但真实存在） 1. ScaleNorm（轻量替代） $$ \\mathrm{ScaleNorm}(x) = \\frac{g}{|x|} x $$ * 仅控制向量模长 * 参数极少 * 表达能力弱于 RMSNorm **论文：** * *Transformers without Tears* Nguyen \u0026 Salazar, 2019 \u003e 实际 LLM 中不如 RMSNorm 常见 2. NoNorm / μParam（研究方向） **核心思想** * 利用初始化与参数化控制尺度 * 显式去除归一化层 **问题** * 对初始化极度敏感 * 工程鲁棒性差 **代表论文** * *µParam: A Unified Framework for Scaling Transformers* Yang et al., 2022 \u003e **尚未成为主流工程方案** ##### 主流模型真实采用情况对照表 | 模型 | Norm 类型 | LN 位置 | 备注 | | -------------- | --------- | ------- | ----- | | 原始 Transformer | LayerNorm | Post-LN | 仅适合浅层 | | BERT | LayerNorm | Post-LN | 深度有限 | | GPT-2 | LayerNorm | Pre-LN | 过渡阶段 | | LLaMA | RMSNorm | Pre-LN | 主流范式 | | Qwen | RMSNorm | Pre-LN | 主流范式 | | DeepSeek | RMSNorm | Pre-LN | 主流范式 | | Mistral | RMSNorm | Pre-LN | 主流范式 | #### FFN 现代预训练模型对 Transformer 中 FFN 的优化，本质是: - 用门控非线性（SwiGLU）和更合理的参数分配，显著提升单位参数的表达能力， - 并在高端模型中通过 MoE 引入条件计算以突破容量瓶颈； - 在工程上，FFN 已成为算力与优化的主战场，而不再只是 Attention 的“配角”。 **主流 FFN 技术路线对照表** | 维度 | 原始 Transformer | 现代主流 LLM | | ------ | -------------- | -------------- | | 激活 | ReLU | SiLU | | 结构 | 单支路 | SwiGLU | | 宽度 | 4d | ≥4d（更优） | | 参数占比 | ~50% | 60–70% | | 稀疏性 | 无 | MoE（可选） | | Kernel | 标准 GEMM | Fused / Triton | ##### 原始 Transformer 中 FFN 1. 原始 FFN 定义（Vaswani et al., 2017） $$ \\mathrm{FFN}(x)=Linear_2(ReLU(Linear_1(x)))\\\\\\\\ \\iff\\mathrm{FFN}(x)=\\max(0, xW_1 + b_1) W_2 + b_2 $$ 其中： * $W_1 \\in \\mathbb{R}^{d \\times d_{ff}}$ * $W_2 \\in \\mathbb{R}^{d_{ff} \\times d}$ * 通常 $d_{ff} = 4d$ **原始定位** * Attention：**token 间交互** * FFN：**token 内的非线性特征变换** 2. 原始 FFN 在大模型中的问题 （1）非线性表达能力有限 * ReLU 在负半轴完全为 0 * 激活分布稀疏、梯度不连续 * 在深层堆叠中表现不稳定 （2）参数与算力利用效率不高 * 同样的参数预算下，ReLU-FFN 的表达能力不如门控结构 * 在 Scaling Law 视角下不是最优结构 （3）数值稳定性与混合精度不友好 * ReLU 的 hard cutoff * 对 BF16 / FP16 训练不友好 ##### 激活函数升级（ReLU → GELU / SiLU） 1. GELU（过渡方案） $$ \\mathrm{GELU}(x)=x \\cdot \\Phi(x) $$ * 平滑近似 ReLU * 在 BERT、早期 GPT 中使用 论文： * *Gaussian Error Linear Units (GELUs)* Hendrycks \u0026 Gimpel, 2016 **局限** * 非门控结构 * 在大规模生成模型中逐渐被淘汰 2. SiLU / Swish（现代门控的基础） $$ \\mathrm{SiLU}(x) = x \\cdot \\sigma(x) $$ * 平滑 * 梯度连续 * 数值稳定性好 **论文：** * *Swish: A Self-Gated Activation Function* Ramachandran et al., 2017 \u003e **SiLU 成为后续 GLU 系列的核心激活函数** ##### 门控 FFN（GLU 系列）【事实标准】 1. GLU 家族的统一视角 一般形式： $$ \\mathrm{FFN}_{GLU}(x)=(xW_1) \\odot g(xW_2) $$ 其中： * $g(\\cdot)$ 为激活函数 * $\\odot$ 为Hadamard积，即逐元素相乘（门控） 2. SwiGLU：当前 Decoder-only LLM 的事实标准 **定义** $$ \\mathrm{SwiGLU}(x)=(xW_1) \\odot \\mathrm{SiLU}(xW_2)\\\\\\\\ \\mathrm{FFN}(x)=\\mathrm{SwiGLU}(x)W_3 $$ **与原始 FFN 的对比** | 维度 | ReLU-FFN | SwiGLU-FFN | | ---- | -------- | ---------- | | 非线性 | 单支路 | 门控双支路 | | 激活 | ReLU | SiLU | | 梯度 | 不连续 | 连续 | | 表达能力 | 较弱 | 显著更强 | **理论与经验动机** - （1）条件计算（Conditional Computation） * 门控决定哪些通道被激活 * 等价于软稀疏专家选择 - （2）更高的函数逼近效率 - 在相同参数预算下：GLU \u003e ReLU（经验与理论一致） - （3）更好的梯度传播 * 乘性门控避免 ReLU 的“死亡神经元” **论文支撑** * *GLU Variants Improve Transformer* Shazeer, 2020 * *PaLM*（Chowdhery et al., 2022，附录实验） **实际采用** * LLaMA / LLaMA 2 / LLaMA 3 * Qwen * DeepSeek * Mistral \u003e **这是目前 FFN 结构层面的最大共识** ##### 参数重分配：为什么 FFN 仍是 4d 甚至更大？ **观察事实** * FFN 通常占 **60%–70% 的参数量** * Attention 占比反而下降 **理论依据：Scaling Laws** **论文：** * *Chinchilla: Training Compute-Optimal Large Language Models* Hoffmann et al., 2022 **结论：** * 在给定计算预算下，**更大的 FFN 宽度比更多 Attention heads 更划算** * FFN 是主要的“记忆与模式存储单元” ##### MoE（Mixture-of-Experts）FFN 1. MoE-FFN：用稀疏性换取容量 **基本结构** $$ \\mathrm{FFN}_{MoE}(x)=\\sum_{e \\in \\mathcal{E}} p_e(x) \\cdot \\mathrm{FFN}_e(x) $$ * 每个 token 只激活 Top-k 个 expert * 总参数量 ↑↑，计算量近似不变 **理论动机** * FFN 是主要容量瓶颈 * MoE 实现 **条件计算 + 参数规模解耦** **关键工程挑战** * 路由不均衡 * 通信开销 * 推理复杂度 **代表论文** * *Switch Transformers* Fedus et al., 2021 * *GShard* Lepikhin et al., 2020 **实际使用** * PaLM-2（部分） * Mixtral（Mistral MoE） * DeepSeek-MoE \u003e **MoE 是“高端路线”，但不是所有模型的默认选择** ##### 数值与工程层面的 FFN 优化 1. 初始化与输出缩放 **问题** * FFN 输出方差随深度累积 **对策** * 输出投影使用更小 std * 或显式 residual scaling： $$ x_{l+1} = x_l + \\alpha \\cdot \\mathrm{FFN}(\\cdot),\\quad \\alpha \\sim \\frac{1}{\\sqrt{L}} $$ **论文：** * *DeepNet* Wang et al., 2022 2. Kernel 融合与算子优化 **工程实践** * fused Linear + Activation + Linear * FlashFFN / Triton kernel * 减少内存访问与 kernel launch **影响** * FFN 在推理中常占 \u003e50% 的 wall-time * kernel 优化比结构微调更重要 3. 低精度与量化适配 * BF16 / FP16 训练 * FFN 权重对量化更敏感 → 需分组量化 / SmoothQuant * 但 FFN 仍是量化收益最大的模块之一 #### 训练与数值稳定性优化 一个工业可行的预训练 pipeline（高层）通常包含： - 架构：Pre-LN + RMSNorm + residual scaling（若需要极深则 DeepNorm）； - 初始化：Xavier/He 的基础 + 若不使用 Norm 可用 T-Fixup/DeepInit； - Optimizer：AdamW（大多数）或 Adafactor（内存受限），可在需要时尝试 Lion / LAMB（大 batch）； - 精度：BF16（或混合 FP16 + master FP32）+ 动态 loss scaling； - LR：短 warm-up（若 Pre-LN 可弱化）+ linear/cosine decay； - 数值保护：global gradient clipping、overflow 检测、健壮 softmax（FlashAttention）； - 系统：ZeRO/FSDP + FlashAttention + fused kernels + activation checkpointing； - 生产化监控与逐步放大测试。 #### 工程优化 现代大模型预训练的工程优化，本质是围绕显存、通信和算子效率构建一套多层次并行与内存管理体系： - 以 ZeRO/FSDP 为核心解决参数规模问题 - 以 FlashAttention 和 kernel 融合解决算子与带宽瓶颈 - 并通过混合精度与重计算在可控 FLOPs 增长下换取可扩展性 **主流工程优化技术全景表** | 类别 | 技术 | 是否主流 | | ------ | --------------------- | ---- | | 并行 | ZeRO / FSDP | 是 | | 并行 | Tensor Parallel | 是 | | 并行 | Pipeline Parallel | 条件 | | 显存 | Activation Checkpoint | 是 | | 精度 | BF16 | 是 | | Kernel | FlashAttention | 是 | | Kernel | Fused FFN | 是 | | IO | 高效数据管道 | 是 | | 推理 | GQA/MQA | 是 | ##### 工程优化的总体目标与约束 在大模型预训练阶段，工程优化的核心目标并非“提高理论表达能力”，而是： 1. **把硬件算力转化为有效 token throughput** 2. **在数值稳定前提下最大化并行度** 3. **降低显存、通信、IO、kernel launch 等非算力瓶颈** 4. **保证系统可扩展到数百/上千 GPU 的稳定运行** 这决定了工程优化几乎全部围绕以下四类瓶颈展开： * 显存（memory） * 通信（communication） * IO / 带宽（bandwidth） * kernel / 算子效率（compute utilization） ##### 并行化体系（最核心的工程优化） 1. 数据并行（Data Parallel, DP）及其局限 **基本思想** * 每张卡保存一份完整模型 * 不同 batch 分配到不同卡 * 反向传播后 All-Reduce 梯度 **问题** * 参数 + optimizer state 全复制 → **显存线性爆炸** * 在百亿/千亿参数下不可行 2. ZeRO：工程上的分水岭（事实标准） **核心思想（Rajbhandari et al.）** 把冗余状态拆分（partition）到不同设备： | Stage | 拆分内容 | | ------ | ---------------- | | ZeRO-1 | Optimizer states | | ZeRO-2 | + Gradients | | ZeRO-3 | + Parameters | **数学/工程本质** * 把原本 (O(N)) 的显存需求降为 $O(N / {GPUs})$ * 用通信换显存 **工程影响** * **使百亿到千亿模型成为“常规工程问题”** * 是 DeepSpeed、FSDP 的理论基础 **论文：** * *ZeRO: Memory Optimizations Toward Training Trillion Parameter Models* Rajbhandari et al., SC 2020 3. FSDP（Fully Sharded Data Parallel） **本质** * PyTorch 官方对 ZeRO-3 的实现与工程化增强 **优化点** * 参数、梯度、optimizer state 全分片 * fine-grained shard（按 module） * 与 PyTorch autograd 深度集成 **工程实践** * 当前 PyTorch 生态的**事实标准** * Qwen / LLaMA 生态训练广泛使用 **论文：** * *FSDP: Fully Sharded Data Parallel* PyTorch team, 2022（技术报告） 4. 张量并行（Tensor Parallel, TP） **思想** * 把**单个矩阵乘法**拆分到多张卡 * 常用于 Attention / FFN 的线性层 **数学形式（示例）** $$ W \\in \\mathbb{R}^{d \\times d} \\Rightarrow [W_1, W_2, \\dots, W_k] $$ **工程代价** * 每层都需要通信（All-Reduce / All-Gather） * 通信频率高 **适用场景** * 单卡放不下模型 * 与 ZeRO/FSDP 结合使用 **论文：** * *Megatron-LM* Shoeybi et al., 2019 5. Pipeline Parallel（PP） **思想** * 按层切分模型，形成流水线 * micro-batch 填充 pipeline **问题** * bubble（空泡）导致利用率下降 * 调度复杂 **现状** * 在极大模型中仍有价值 * 但工程复杂度高，通常是“最后一招” **论文：** * *GPipe* Huang et al., 2019 ##### 显存优化（Memory is King） 1. Activation Checkpointing（重计算） **思想** * 前向不保存全部激活 * 反向时重算 **数学权衡** * 显存：↓ * FLOPs：↑（通常 \u003c2×） **实践** * Transformer block 级 checkpoint * 工业界默认开启 **论文：** * *Training Deep Nets with Sublinear Memory Cost* Chen et al., 2016 2. 混合精度（BF16 / FP16） **工程事实** * 不使用混合精度几乎不可能训练大模型 **BF16 优势** * 更大指数范围 * 数值更稳定 * 减少 overflow/underflow **工程要点** * master weights FP32 * loss scaling（FP16） **论文：** * *Mixed Precision Training* Micikevicius et al., 2018 3. KV Cache 工程优化（训练 \u0026 推理） **核心** * KV Cache 是显存大户（O(seq_len × layers × d)） **技术** * 分页 KV Cache * chunked attention * offload（CPU/NVMe） **论文：** * *Efficient Memory Management for Large Language Model Serving* Kwon et al., 2023（vLLM） ##### 算子与 Kernel 级优化（吞吐提升的关键） 1. FlashAttention（革命性优化） **原理** * IO-aware attention * 不显式 materialize (QK^T) **数学本质** * softmax 的分块数值稳定计算 **工程收益** * 显存：O(n²) → O(n) * 速度：显著提升 * 数值稳定性更好 **论文：** * *FlashAttention* Dao et al., NeurIPS 2022 2. Fused Kernels（线性 + 激活 + 线性） **思想** * 减少 kernel launch * 减少中间内存读写 **典型融合** * QKV fused * Linear + SiLU + Linear（FFN） **实际影响** * 在 FFN 占主导的模型中，**收益非常大** **工业实现：** * Triton * NVIDIA Transformer Engine 3. 通信-计算重叠（Overlap） **技术** * backward 中 overlap All-Reduce 与 GEMM * pipeline overlap **目标** * 隐藏通信延迟 * 提高 GPU 利用率 **论文：** * *Efficient Large-Scale Language Model Training on GPU Clusters* Narayanan et al., 2021 ##### IO 与数据工程优化（常被低估） 1. 高效数据管道 **问题** * GPU 常因 data starvation 空转 **技术** * 多进程预取 * mmap / streaming * shard + shuffle **论文：** * *Efficient Training of Language Models to Fill in the Gaps* Fedus et al., 2022 2. Checkpoint 工程 **挑战** * 千亿参数 checkpoint 可达 TB 级 **技术** * sharded checkpoint * async save * 增量 checkpoint **实践：** * DeepSpeed / FSDP checkpoint ##### 推理友好工程（训练阶段即考虑） 1. 结构选择的工程动机 | 设计 | 工程收益 | | ------------ | ----------- | | Decoder-only | KV Cache 友好 | | GQA / MQA | KV Cache 减少 | | RMSNorm | kernel 融合 | **论文：** * *GQA: Training Generalized Multi-Query Transformer Models* Ainslie et al., 2023 2. 量化友好训练（QAT-aware） * SmoothQuant * 分组量化 * FFN-aware scaling **论文：** * *SmoothQuant* Xiao et al., 2022 --- ## 常用工具 ### 深度学习框架 #### Pytorch 常用的深度学习框架 #### TensorFlow ### 机器学习库 #### sklearn 它提供了丰富的工具和算法，帮助用户轻松构建、训练和评估机器学习模型。 数据处理： - 数据集切分：train_test_split ### 数据处理 - numpy - pandas - scipy ### 图形化交互 - TensorBoard：可以监听log数据，图形化展示训练曲线 - tqdm：进度条 - netron：模型结构可视化 ### LLM网站及工具 #### HuggingFace Hugging Face 是一家提供开源 AI 工具和平台的公司，致力于简化预训练模型的使用，加速机器学习项目的开发与落地。 最初以 Transformers 库闻名，该库极大地降低了使用 BERT、GPT、T5 等模型的门槛。如今，Hugging Face 已发展成为一个完整的 AI 开发生态系统，支持自然语言处理、计算机视觉、语音处理、多模态任务等多个领域。 Hugging Face 的生态系统主要由两个核心部分组成： ##### 1）Hugging Face Hub Hugging Face提供了一个集中式的开源平台，用于托管和分享模型、数据集和应用。 - 官网地址为：https://huggingface.co/ - 国内镜像地址为：https://hf-mirror.com/ ##### 2）工具链（Libraries） Hugging Face 提供了一套围绕预训练模型构建的工具库。这些组件彼此独立，又可以协同工作，覆盖了从数据处理到模型训练与推理的完整流程。 各组件具体功能如下： - Datasets：Datasets 是用于加载和处理数据集的工具库。支持从在线仓库或本地文件（如 CSV、JSON）加载文本数据，并支持清洗、编码、切分等预处理操作。处理后的数据可直接用于模型训练，是连接原始数据与模型输入的重要桥梁。 - Tokenizers：Tokenizers 是用于将文本转换为模型输入的工具。它支持文本分词、编码为 token ID，同时自动处理特殊符号、填充（padding）、attention mask 和句子对标记（token type ID）。分词器通常与模型配套使用，可通过统一接口加载。 - Transformers：Transformers 是 Hugging Face 最核心的库，用于加载、使用和微调各种预训练模型。该库统一了模型接口，支持数百种模型结构，如 BERT、GPT 等，用户可以通过一行代码 from_pretrained()直接加载公开模型，快速用于推理或训练。 #### ModelScope 类似HuggingFace，由Alibaba维护。 # NLP进阶 # Materials ## lessons/presentation/blogs - [cs224n](https://web.stanford.edu/class/cs224n/) - [illustrated-transformer](https://jalammar.github.io/illustrated-transformer/) - [huggingface LLM course](https://huggingface.co/learn/llm-course/en/chapter1/6) ## Papers/ - [ACL Anthology](https://aclanthology.org/)（ACL、EMNLP、NAACL 等会议论文集合）——查 NLP 最新论文的权威库。 - arXiv / Papers with Code / Semantic Scholar —— 快速跟踪最新 preprints 与复现代码。 - 关注主要会议：ACL, EMNLP, NeurIPS, ICML, ICLR。 ## Books - [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3) - [Practical Deep Learning](https://course.fast.ai/) - [Natural Language Processing with Transformers](https://github.com/nlp-with-transformers/notebooks) # References - [尚硅谷大模型技术之NLP1.0.3.pdf](/pdf/nlp/尚硅谷大模型技术之NLP1.0.3.pdf) - [尚硅谷大模型技术之NLP1.0.3.docx](/pdf/nlp/尚硅谷大模型技术之NLP1.0.3.docx) - [自然语言处理 基于预训练模型的方法.pdf](/pdf/nlp/自然语言处理_基于预训练模型的方法.pdf) ","date":"2024-10-09T00:00:00Z","permalink":"https://loveleaves.github.io/p/nlp_intro/","title":"【NLP】 NLP 手册"},{"content":"References 深度神经网络剪枝综述，A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommendations 目前针对大模型剪枝的方法有哪些？ MIT 6.5940 TinyML and Efficient Deep Learning Computing 模型压缩的小白入门教程 介绍 **模型剪枝（Model Pruning）**是一种用于减少神经网络模型参数数量和计算量的技术。它通过识别和去除在训练过程中对模型性能影响较小的参数或连接，从而实现模型的精简和加速。\n通常，模型剪枝可以分为两种类型：结构化剪枝（Structured Pruning）和非结构化剪枝（Unstructured Pruning）。\n结构化剪枝和非结构化剪枝的主要区别在于剪枝目标和由此产生的网络结构。结构化剪枝根据特定规则删除连接或层结构，同时保留整体网络结构。而非结构化剪枝会剪枝各个参数，从而产生不规则的稀疏结构。\n模型剪枝的一般步骤包括：\n训练初始模型：首先，需要训练一个初始的大模型，通常是为了达到足够的性能水平。 评估参数重要性：使用某种评估方法（如：权重的绝对值、梯度信息等）来确定模型中各个参数的重要性。 剪枝：根据评估结果，剪枝掉不重要的参数或连接，可以是结构化的或非结构化的。 修正和微调：进行剪枝后，需要进行一定的修正和微调，以确保模型的性能不会显著下降。 模型剪枝可以带来多方面的好处，包括减少模型的存储需求、加速推理速度、减少模型在边缘设备上的资源消耗等。然而，剪枝可能会带来一定的性能损失，因此需要在剪枝前后进行适当的评估和调整。\n剪枝方法 Network Slimming 基本思想 论文核心点\n以 BN 中的 γ 为切入点，即 γ 越小，其对应的特征图越不重要 为了使得 γ 能有特征选择的作用，引入 L1 正则来控制 γ $$ L = \\sum_{(x,y)} l(f(x, W), y) + \\lambda \\sum_{\\gamma \\in \\Gamma} g(\\gamma) $$ BatchNorm 如何得到每个特征图的重要性呢？\u0026ndash;BN要解决的问题\nNetwork slimming，就是利用BN层中的缩放因子Y 先来回顾下BN是做什么的：$\\hat{x}^{(k)} = \\frac{x^{(k)} - E[x^{(k)}]}{\\sqrt{Var[x^{(k)}]}}$ 整体感觉就是一个归一化操作，但是BN中还额外引入了两个可训练的参数：γ和β。 BN本质作用\nBN要做的就是把越来越偏离的分布给他拉回来！ 再重新规范化到均值为0方差为1的标准正态分布 这样能够使得激活函数在数值层面更敏感，训练更快 有一种感觉：经过BN后，把数值分布强制在了非线性函数的线性区域中 BN额外参数\n如果都是线性的了，神经网络还有意义吗？ BN另一方面还需要保证一些非线性，对规范化后的结果再进行变换 这两个参数是训练得到的：$y^{(k)}=\\gamma^{(k)}\\hat{x}^{(k)}+\\beta^{(k)}$ 感觉就是从正太分布进行一些改变，拉动一下，变一下形状！ 稀疏化原理与效果\n论文中提出：训练时使用 L1 正则化能对参数进行稀疏作用 L1：稀疏与特征选择；L2：平滑特征 L1 正则化：$J(\\vec{\\theta}) = \\frac{1}{2} \\sum_{i=1}^{m} \\left( h_{\\vec{\\theta}}(x^{(i)}) - y^{(i)} \\right)^2 + \\lambda \\sum_{j=1}^{n} |\\theta_j|$，$\\theta_j$是要正则化的参数 L2 正则化：$J(\\vec{\\theta}) = \\frac{1}{2} \\sum_{i=1}^{m} \\left( h_{\\vec{\\theta}}(x^{(i)}) - y^{(i)} \\right)^2 + \\lambda \\sum_{j=1}^{n} \\theta_j^2$，同上 剪枝流程 代码 github\npaper with code DepGraph github paper ","date":"2024-06-21T00:00:00Z","permalink":"https://loveleaves.github.io/p/prune/","title":"模型剪枝介绍"},{"content":"嵌入式编程介绍 嵌入式编程（Embedded Programming）是指在嵌入式系统中编写软件的过程，嵌入式系统通常是专门为某一特定任务设计的计算机系统，不像传统计算机那样可以运行多种应用程序。嵌入式系统的应用非常广泛，从智能家居设备、汽车控制系统、工业自动化，到医疗设备等，几乎无处不在。\n嵌入式编程是什么？ 嵌入式编程是为了控制嵌入式系统硬件而编写的软件。嵌入式系统通常具有以下特点：\n资源有限：嵌入式系统一般硬件资源有限，如内存、处理能力、电池寿命等。 任务专一：嵌入式系统通常只执行单一任务或有限的几个任务。 高实时性：很多嵌入式系统需要满足严格的实时性要求，即程序必须在特定时间内完成特定操作。 稳定性高：由于嵌入式设备通常需要长时间运行，因此软件的稳定性和可靠性至关重要。 嵌入式编程不仅仅是开发简单的软件，它还需要开发者对硬件有一定了解，能够在有限的资源下优化代码，确保系统的高效和稳定运行。\n嵌入式编程的基础 硬件平台 彻底搞清单片机、ARM、MCU、DSP、FPGA之间的关系！ 嵌入式编程首先需要选择合适的硬件平台。常见的嵌入式硬件平台包括：\n单片机（MCU）：例如STMicroelectronics的STM32、Atmel的AVR系列、Microchip的PIC系列等。单片机广泛应用于各种小型设备。 数字信号处理器（DSP）：用于复杂的计算，像离散余弦变换、快速傅里叶变换，常用于图像处理，在数码相机等设备中使用。 开发板：如树莓派、Arduino、ESP32等，它们适合快速原型开发。 FPGA：如Xilinx、Intel（Altera）等的FPGA芯片，适用于对硬件有高要求的应用。 ASIC：定制/半定制芯片，设计用于解决特殊需求。 ARM: 一个英国的芯片设计公司，但是不生产芯片。只卖知识产权。 嵌入式操作系统 对于一些复杂的嵌入式应用，开发者需要选择合适的操作系统来管理硬件资源。常见的嵌入式操作系统有：\nRTOS（实时操作系统）：如FreeRTOS、ChibiOS等，适用于需要高实时性的嵌入式应用。 Linux：例如在树莓派等开发板上运行嵌入式Linux，适用于需要丰富功能和较强处理能力的系统。 裸机编程：没有操作系统支持，直接对硬件进行编程，适用于资源较为有限的设备。 编程语言 嵌入式开发常用的编程语言主要有：\nC语言：由于其高效、底层控制能力和较小的代码体积，C语言是嵌入式编程中最常用的语言。 C++：对于一些更复杂的系统，C++提供了面向对象的特性，帮助开发者更好地管理代码。 汇编语言：在一些资源非常有限或者对性能要求极高的场景下，可能需要使用汇编语言来直接控制硬件。 嵌入式编程工具 嵌入式开发离不开合适的开发工具，这些工具通常包括：\nIDE（集成开发环境）：如Keil、IAR Embedded Workbench、Eclipse等，用于编写、编译和调试嵌入式代码。 编译器：GCC（GNU Compiler Collection）是最常用的开源编译器，它支持多种架构的嵌入式开发。 调试工具：JTAG调试器、SWD（Serial Wire Debug）调试器等，用于硬件级调试，帮助开发者实时查看代码执行状态。 仿真器：一些开发环境如Proteus提供硬件仿真，帮助开发者在没有实际硬件的情况下测试代码。 通信协议 一、 核心基石：UART 与 USART 这是理解所有串行通信的起点。\n特性 UART (通用异步收发器) USART (通用同步/异步收发器) 核心特点 异步通信 同步 + 异步通信 时钟信号 无单独的时钟线。依靠预定义的波特率同步。 有单独的时钟线（CLK）。同步模式依赖此时钟。 连接线 最少2根：TXD (发送) 和 RXD (接收) 异步模式：2根 (TXD, RXD) 同步模式：3+根 (TXD, RXD, CLK) 关系 USART 是 UART 的超集。一个USART模块可以配置为UART模式工作。 关键区别 简单、常见、成本低。速率和可靠性受双方时钟精度影响。 更灵活。同步模式速度更高、可靠性更强，但多占一根引脚。 通俗比喻 两个人聊天：仅靠语速（波特率）来理解对方。 一个人唱歌，一个人打拍子：唱的人按拍子唱，听的人按拍子听，节奏精准。 现状：在现代单片机中，标为“UART”的接口实质上绝大多数都是“USART”功能，只是大家习惯统称为UART。\n二、 理解层次：物理层 vs. 数据链路层 这是理清所有协议关系的最关键概念。请参考OSI模型（简化版）：\n物理层 (Physical Layer)：规定“如何传输0和1”，解决电气特性、电平标准、接口形状的问题。好比公路的材料和结构（柏油路、水泥路、铁轨）。 数据链路层 (Data Link Layer)：规定“数据包如何组织、如何校验、谁先说话”，解决帧格式、差错控制、寻址、仲裁的问题。好比交通规则（红绿灯、停让标志、车道线）。 常见的物理层标准/协议 协议 描述 特点 与UART的关系 TTL电平 单片机引脚直出电平 (0V=0, 3.3V/5V=1) 距离极短(\u0026lt;0.5m)，易受干扰 UART 产生 TTL电平信号 RS-232 通过芯片(如MAX232)将TTL电平转换为(±3~±15V) 抗干扰增强，距离可达15米，点对点 UART -\u0026gt; MAX232芯片 -\u0026gt; RS-232接口 RS-485 通过芯片(如MAX485)将TTL转换为差分信号 抗干扰极强，距离远(1200m)，支持多点 UART -\u0026gt; MAX485芯片 -\u0026gt; RS-485网络 CAN收发器 将CAN控制器的TTL信号转换为CAN差分电平(CAN_H, CAN_L) 高速、高可靠，用于汽车和工业领域 单片机CAN控制器 -\u0026gt; CAN收发器 -\u0026gt; CAN总线 常见的数据链路层协议 协议 描述 特点 与UART的关系 UART帧 UART硬件本身定义的简单帧格式(起始位+数据位+校验位+停止位) 非常简单，无地址、无复杂校验 UART 自身的规则 SPI 同步、全双工、高速通信协议。有CLK, MOSI, MISO, CS线 速度快，通常有独立的硬件控制器 独立，不依赖UART I2C 同步、半双工、两线制(SCL, SDA)。支持多主多从，通过地址寻址 引脚节省，有独立的硬件控制器 独立，不依赖UART CAN 高速、可靠、多主仲裁的工业级总线协议。有复杂的帧格式和错误检测 可靠性极高，有独立的硬件控制器和收发器 独立，不依赖UART LIN 低速、低成本、单主多从的汽车网络协议 其物理层和帧格式基于UART，是UART的上层应用协议 基于 UART构建 I2C I2C总线协议\nI2C协议是一种多主机的串行通信协议，它只需要两条线：SDA（数据线）和SCL（时钟线）。它支持多个从设备连接到同一总线上，每个设备都有一个唯一的地址。\nI2C协议通过地址来选择不同的从设备进行数据交换，因此相比SPI节省了引脚资源。其通信速率低于SPI，但足以应对多数应用。\nESP32S3初始化I2C示例\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 esp_err_t bsp_i2c_init(void) { i2c_config_t i2c_conf = { .mode = I2C_MODE_MASTER, // 主模式 .sda_io_num = BSP_I2C_SDA, // sda引脚 .sda_pullup_en = GPIO_PULLUP_ENABLE, .scl_io_num = BSP_I2C_SCL, // scl引脚 .scl_pullup_en = GPIO_PULLUP_ENABLE, .master.clk_speed = BSP_I2C_FREQ_HZ // 时钟频率 }; i2c_param_config(BSP_I2C_NUM, \u0026amp;i2c_conf); return i2c_driver_install(BSP_I2C_NUM, i2c_conf.mode, 0, 0, 0); // 不设置buffer } SPI SPI总线协议\nSPI协议是一种高速的、全双工、同步的通信总线。它通常由四条线组成：SCK（时钟线）、MOSI（主设备数据输出，从设备数据输入线）、MISO（主设备数据输入，从设备数据输出线）和CS（片选线）。\n当多个从设备被连接到同一个主设备时，每个从设备都有一个单独的片选信号。数据在时钟信号的边沿被采样，通常是在上升沿或下降沿。SPI的速率比I2C高，但需要更多的引脚资源。\nI2S I2S协议：特点、工作原理、差异及其应用 ESP32 I2S使用 数字音频接口，常见数字音频信号传输标准，如I2S、PCM（TDM）、PDM等\nI2S（Inter-IC Sound）协议 是一种用于将数字音频数据从一个设备传输到另一个设备的协议。该协议在电子设备内的集成电路（IC）之间传输脉冲编码调制（PCM）音频数据。I2S在将预录的音频文件从微控制器（MCU）传输到DAC或放大器的过程中起关键作用。此协议还可通过麦克风实现音频数字化。I2S协议不涉及压缩，因此无法播放OGG、MP3等压缩音频格式，但支持WAV文件。\nESP32 包含 2 个 I2S 外设。通过配置这些外设，可以借助 I2S 驱动来输入和输出采样数据。\n标准 模式下的 I2S 总线包含以下几条线路：\nMCLK：主时钟线。该信号线可选，具体取决于从机，主要用于向 I2S 从机提供参考时钟。 BCLK：位时钟线。用于数据线的位时钟。 WS：字（声道）选择线。通常用于识别声道（除 PDM 模式外）。 DIN/DOUT：串行数据输入/输出线。如果 DIN 和 DOUT 被配置到相同的 GPIO，数据将在内部回环。 PDM 通信模式下的 I2S 总线包含以下几条线路：\nCLK：PDM 时钟线。 DIN/DOUT：串行数据输入/输出线。 CAN CAN总线\nCAN（Controller Area Network）总线协议是由 BOSCH 发明的一种基于消息广播模式的串行通信总线，它起初用于实现汽车内ECU之间可靠的通信，后因其简单实用可靠等特点，而广泛应用于工业自动化、船舶、医疗等其它领域。相比于其它网络类型，如局域网（LAN, Local Area Network）、广域网（WAN, Wide Area Network）和个人网（PAN, Personal Area Network）等，CAN 更加适合应用于现场控制领域，因此得名。\nCAN总线是一种多主控（Multi-Master）的总线系统，它不同于USB或以太网等传统总线系统是在总线控制器的协调下，实现A节点到B节点大量数据的传输，CAN网络的消息是广播式的，亦即在同一时刻网络上所有节点侦测的数据是一致的，因此比较适合传输诸如控制、温度、转速等短消息。\nLIN Bus LIN Bus\n由于不同车厂定义的协议兼容性的问题，在 1998 年由欧洲五大车厂（BMW, Volkswagen Group, Audi Group, Volvo Cars, Mercedes-Benz）成立联合工作组，由 Volcano Automotive Group 和 Motorola 提供技术支持，开发一种定位于车身电子领域传感器（Sensors）和执行器（Actuators）组网的串行通信总线，要求该总线系统的协议和时序控制尽可能简单，即使低端MCU没有专用通信单元也可以实现基于该总线的通信。这种总线即为 LIN 总线。\n三、 无线协议分析：蓝牙、WiFi、2.4G 无线协议是完整的协议栈，包含了从物理层到应用层的所有定义。单片机通常通过串口与实现这些协议的“模组”交互。\n技术 物理层 (公路) 数据链路层及更高层 (交通规则) 单片机如何与其交互 蓝牙 (Bluetooth) 2.4GHz无线电波 (GFSK调制) 基带/L2CAP层：管理连接、分包、加密。 应用层Profile：如SPP(串口透传)、GATT(低功耗蓝牙) 单片机通过 UART 连接蓝牙模块(如HC-05)，发送AT指令或数据。模块内部完成所有复杂协议处理。UART是模块的输入口。 WiFi 2.4GHz/5GHz无线电波 (DSSS/OFDM调制) MAC层：帧结构、CSMA/CA访问控制、加密(WPA2)。 TCP/IP协议栈：运行在更上层。 单片机通过 UART 或 SPI 连接WiFi模块(如ESP8266)，发送AT指令控制其联网和收发TCP/IP数据包。 2.4G私有协议 (如NRF24L01) 2.4GHz无线电波 (GFSK调制) 厂商自定义的简单链路层：定义自己的数据包结构(地址、载荷、CRC)。 单片机通过 SPI 接口直接控制射频芯片的寄存器，配置参数并收发原始数据包。这里用了SPI，因为速度要求比UART高。 四、 总结与全局关系图 所有协议与单片机核心的关系，可以通过下图一目了然：\n图例与说明：\n关键路径：\nUART路径：展示了UART数据如何通过不同物理层芯片（RS232/RS485）转换为不同的网络标准，以及如何作为高层协议（LIN/PPP）的基础和无线模组（BLE/Wi-Fi）的输入通道。 独立协议路径：展示了SPI、I2C、CAN作为独立且平行的通信方式，直接由专用控制器管理。 无线路径：展示了单片机通过UART或SPI与无线模组交互，由模组处理所有无线通信的复杂性。 常用模块 LCD LVGL \u0026amp; gui guider 参考资料：\nlvgl官网 lvgl控件 常用控件：\n1 2 3 lv_XXX_create() // 如btn按键等 lv_label_create() // label lv_timer_create() // timer 事件Event\n1 2 3 4 5 交互类：点击、长按、释放、拖动 状态变化：失去焦点、获得焦点 对象变化：创建、删除、页面（屏）加载 添加事件接口：lv_obj_add_event_cb() 定时器timer\n常见问题 1、长文件名无法正常显示或其中中文无法显示\n1 settting里面设置fat的Long filename support，API character encoding使用utf-8，记得保存 蓝牙模块 简介 蓝牙模块可以将有线串口数据流转换为无线蓝牙数据流，实现无线串口功能。\n蓝牙类别 标志 特性 经典蓝牙 SPP，蓝牙3.0及以下 功耗高，速率较快，先配对后连接 低功耗蓝牙 BLE，蓝牙4.0及以上 功耗低，速率较慢，快速连接 增加组网、广播、定位等新功能 双模蓝牙 SPP + BLE 同时集成经典蓝牙和低功耗蓝牙，兼容性更强 主从类别 特性 单从机 仅能被动等待主设备连接，无法主动发起连接请求 主从一体 可切换角色为主设备或从设备，作为主设备时，可主动发起连接请求 常见模组\nHC-04 HC-05 HC-08 JDY-31 JDY-33 BT16 硬件连接 蓝牙模块配置（AT模式） 蓝牙未连接或者连接后按住KEY不放，可以使模块进入AT模式，此时通过串口发送AT指令，可对蓝牙模块进行配置。\nAT指令 功能 AT 测试通讯 AT+BAUD 查询和设置串口波特率 AT+DEFAULT 参数恢复默认值 AT+ROLE 查询和设置角色（主设备/从设备） AT+NAME 查询和设置经典蓝牙名称 AT+PIN 查询和设置经典蓝牙配对密码 AT+BNAME 查询和设置BLE蓝牙名称 注：以上仅以HC-04为例介绍AT模式，不同蓝牙模块的AT指令和进入AT模式的方法都可能不同，详细介绍请参考模块对应的手册\nIMU IMU芯片通常通过SPI（Serial Peripheral Interface）或I2C（Inter-Integrated Circuit）两种通信协议与微控制器进行数据交换。\n在选择SPI或I2C时，需要考虑到系统的需求。例如，如果对速度和数据吞吐量有较高要求，则SPI可能是更好的选择；如果引脚资源有限或者需要支持多设备通信，I2C则更为合适。\nmpu6050 Previous Next \u0026nbsp; \u0026nbsp; / [pdf] View the PDF file here. QMI8658 国产芯片\nESP32示例代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // 初始化qmi8658 void qmi8658_init(void) { uint8_t id = 0; // 芯片的ID号 qmi8658_register_read(QMI8658_WHO_AM_I, \u0026amp;id ,1); // 读芯片的ID号 while (id != 0x05) // 判断读到的ID号是否是0x05 { vTaskDelay(1000 / portTICK_PERIOD_MS); // 延时1秒 qmi8658_register_read(QMI8658_WHO_AM_I, \u0026amp;id ,1); // 读取ID号 } ESP_LOGI(TAG, \u0026#34;QMI8658 OK!\u0026#34;); // 打印信息 qmi8658_register_write_byte(QMI8658_RESET, 0xb0); // 复位 vTaskDelay(10 / portTICK_PERIOD_MS); // 延时10ms qmi8658_register_write_byte(QMI8658_CTRL1, 0x40); // CTRL1 设置地址自动增加 qmi8658_register_write_byte(QMI8658_CTRL7, 0x03); // CTRL7 允许加速度和陀螺仪 qmi8658_register_write_byte(QMI8658_CTRL2, 0x95); // CTRL2 设置ACC 4g 250Hz qmi8658_register_write_byte(QMI8658_CTRL3, 0xd5); // CTRL3 设置GRY 512dps 250Hz } 滤波 TF卡 esp-idf官方例程：https://github.com/espressif/esp-idf/tree/bbe8aabca0/examples/storage/sd_card\nESP32与TF卡通信主要有两种方式：\nSSPI：串行外设接口 SDIO：安全数字输入输出接口。其中SDIO又分为两种模式： 四线模式：有四个数据引脚 一线模式：只有一个数据引脚 References soc介绍 怎么干好嵌入式MCU、ARM、DSP这一行 深入理解CPU和异构计算芯片GPU/FPGA/ASIC （上篇） ","date":"2023-03-21T00:00:00Z","permalink":"https://loveleaves.github.io/p/embedded_programing_intro/","title":"【嵌入式编程】 嵌入式编程介绍"},{"content":"References https://www.bilibili.com/video/BV1fA4y1o715 https://www.bilibili.com/video/BV1QK411d76w 算法通关手册 LeetCode 刷题攻略 二分查找 概念 二分查找的做法：定义查找的范围 [left,right]，初始查找范围是整个数组。每次取查找范围的中点 mid，比较 nums[mid] 和 target 的大小，如果相等则 mid 即为要寻找的下标，如果不相等则根据 nums[mid] 和 target 的大小关系将查找范围缩小一半。\n关键点：\n搜索区间：注意区间的开闭，是否满足定义 循环不变量：最终搜索子区间，及之后退出循环的left、right指向位置 简单场景：单target查找 以leetcode-704为例：\n搜索区间：左闭右闭 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 标准写法 def search(self, nums: List[int], target: int) -\u0026gt; int: left = 0 # 搜索区间：[left,right] right = len(nums) - 1 while left \u0026lt;= right: mid = (left+right)//2 if nums[mid] \u0026lt; target: left = mid + 1 elif nums[mid] \u0026gt; target: right = mid - 1 else: return mid return -1 这里因为定义搜索区间为左闭右闭，所以：\nleft、right初始值毫无疑问 最终搜索子区间索引为：[i,i] while循环作为子区间搜索，索引子区间[i,i]肯定属于搜索区间，所以判断条件为\u0026lt;= 当nums[mid] \u0026lt; target时，已知条件是target肯定在右边，但为什么不能更新为left = mid，是因为考虑到缩小到的子区间里可能有mid==left情况，此时如果不+1移动更新区间，就是死循环；right值处理跟left类似 所以只要在mid==left时更新区间，其实也可能设置为mid，因为[mid, right]也是符合条件的搜索区间，代码如下： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # 其他写法 def search(self, nums: List[int], target: int) -\u0026gt; int: left = 0 right = len(nums) - 1 while left \u0026lt;= right: mid = (left+right)//2 if nums[mid] \u0026lt; target: if mid == left: left = mid + 1 else: # 不为mid+1也没问题，不过部分子区间重复搜索 left = mid elif nums[mid] \u0026gt; target: if mid == left: right = mid - 1 else: right = mid else: return mid return -1 搜索区间：左闭右开 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 标准写法 def search(self, nums: List[int], target: int) -\u0026gt; int: left = 0 # 搜索区间：[left,right) right = len(nums) while left \u0026lt; right: mid = (left+right)//2 if nums[mid] \u0026lt; target: left = mid + 1 elif nums[mid] \u0026gt; target: right = mid else: return mid return -1 这里因为定义搜索区间为左闭右开，所以：\nleft、right初始值毫无疑问 最终搜索子区间索引为：[i,i+1] while循环作为子区间搜索，索引子区间[i,i]不属于搜索区间，所以判断条件为\u0026lt; 当nums[mid] \u0026lt; target时，但为什么不能更新为left = mid，和上面类似；当nums[mid] \u0026gt; target时，则已知最小可搜索空间为[left,mid)，当然也可以为[left,mid+i)（mid\u0026lt;=mid+i\u0026lt;=right），但这就多了许多重复搜索 所以只要在mid==left时更新区间，其实也可能设置为mid，因为[mid, right]也是符合条件的搜索区间，代码如下： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 其他写法 def search(self, nums: List[int], target: int) -\u0026gt; int: left = 0 # 搜索区间：[left,right) right = len(nums) while left \u0026lt; right: mid = (left+right)//2 if nums[mid] \u0026lt; target: if mid == left: left = mid + 1 else: left = mid elif nums[mid] \u0026gt; target: right = mid else: return mid return -1 搜索区间：左开右开 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 标准写法 def search(self, nums: List[int], target: int) -\u0026gt; int: left = -1 # 搜索区间：(left,right) right = len(nums) while left + 1 \u0026lt; right: mid = (left+right)//2 if nums[mid] \u0026lt; target: left = mid elif nums[mid] \u0026gt; target: right = mid else: return mid return -1 原因和前两个类似，这里：\n最终搜索子区间索引为：[i,i+1,i+2] 利用二分性质：第一个或最后一个target位置 以leetcode-34为例，定义搜索空间为左闭右闭：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # 搜索第一个target位置 def lowerBound(nums, target): left = 0 right = len(nums) - 1 while left \u0026lt;= right: mid = (left + right)//2 if nums[mid] \u0026lt; target: left = mid + 1 # 等于情况移动right：不断将子搜索区间移动到小于target的第一个位置 else: right = mid - 1 return left def searchRange(self, nums: List[int], target: int) -\u0026gt; List[int]: first_pos = lowerBound(nums, target) # 找不到target情况 if first_pos == len(nums) or nums[first_pos] != target: return [-1, -1] end_pos = lowerBound(nums, target+1) - 1 return [first_pos, end_pos] 上面的nums[mid] == target情况修改为移动left：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def lowerBound(nums, target): left = 0 right = len(nums) - 1 while left \u0026lt;= right: mid = left + (right - left)//2 # 等于情况移动left：不断将子搜索区间移动大于target的第一个位置 if nums[mid] \u0026lt;= target: left = mid + 1 else: right = mid - 1 return left def searchRange(self, nums: List[int], target: int) -\u0026gt; List[int]: first_pos = lowerBound(nums, target) # 找不到target情况 if first_pos == 0 or nums[first_pos-1] != target: return [-1, -1] end_pos = lowerBound(nums, target-1) return [end_pos, first_pos-1] 总结 由上可以总结如下：\n二分查找在mid和target值相等时，继续移动区间，则在有多个target值的情况时： 1、相等时移动right，最终结果为等于target的第一个位置 2、相等时移动left，最终结果为大于target的第一个位置 注意 但当相等情况复杂时，如果相等位置切分不当，left最终位置可能与预期位置不一致，可以通过以leetcode-33为例思考。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def search(self, nums: List[int], target: int) -\u0026gt; int: def moveLeft(i): end = nums[-1] if nums[i] \u0026gt; end: return target \u0026gt;= nums[i] or target \u0026lt;= end else: return target \u0026gt;= nums[i] and target \u0026lt;= end left = 0 right = len(nums) - 1 while left \u0026lt;= right: mid = (left+right)//2 # 如果相等情况不return mid，则moveLeft判断就需要进行仔细考虑该场景下相等情况子区间的移动 if nums[mid] == target: return mid elif moveLeft(mid): left = mid+1 else: right=mid-1 if left == len(nums) or nums[left] != target: return -1 return left 局部可二分 以leetcode-162为例，定义搜索空间为左闭右闭：\n为什么可以使用二分 简单证明：\n我们可以以下标为x轴，元素值为y轴，在平面直角坐标系里绘制出每个元素的位置，再以直线连接，这样构成了一副折线图。由于题目保证两边界之外的元素为负无穷，相邻值不相等且至少存在一个非负无穷元素，所以必然有峰。那么我们任取相邻两点为m、m+1，则如果nums[m]\u0026lt;nums[m+1]，那么m+i, i=1,2..范围肯定存在一个该范围的最大值且为峰顶，同理，如果小于，那么反方向也肯定存在一个该范围的最大值且为峰顶。因而可以用二分解决。\n1 2 3 4 5 6 7 8 9 10 11 12 13 def findPeakElement(self, nums: List[int]) -\u0026gt; int: left = 0 right = len(nums) - 2 # 忽略末尾元素，防止越界 # 或者这样处理 # right = len(nums) - 1 # nums.append(-inf) while left \u0026lt;= right: mid = (left+right)//2 if nums[mid] \u0026gt; nums[mid+1]: right = mid - 1 else: left = mid + 1 return left 回溯 概念 回溯法就是暴力解法。其基本思想是通过遍历每个阶段的所有可能选择，从而获得想要的所有可能，通常使用递归实现。并且通常从第一步开始选择，到最后一步结束，所以呈现类似对树进行DFS的操作，每层代表着每一步，从上往下的每个节点代表着累积的选择。\n递阶段：每次递的过程代表了每步进行了一种选择，递操作结束时代表所有步都做出了一种选择。 归阶段：每次归的过程往往伴随着撤销操作，代表对上一个选择的放弃，归操作结束时代表撤销所有步的所有选择。 关键点：\n递阶段：考虑当前步的所有选择，可通过条件进行剪枝。每步的选择可分为关联选择和无关选择。 关联选择：步与步之间的选择是有关联关系的，某步选了，其他步就会有约束（如不能选）； 无关选择：步与步之间的选择互不干扰，不受其他步的约束。 递结束：当前所有步的选择结果是否符合 归阶段：注意撤销之前选择 其他： 选择约束：某步的选择有约束，如可以不选、必须选或不能重复选等 递结束条件：需要多少步等 无关选择类型 以leetcode-17为例：\n无关选择：每步的选择与其他步不相关 递结束条件：步数为digits长度 选择约束：每步的所有选择必须选一个 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def letterCombinations(self, digits: str) -\u0026gt; List[str]: if not digits: return [] ans = [] # 存储每步的选择 paths = [] num2char = { \u0026#39;1\u0026#39;: \u0026#34;\u0026#34;, \u0026#39;2\u0026#39;: \u0026#34;abc\u0026#34;, \u0026#39;3\u0026#39;: \u0026#34;def\u0026#34;, \u0026#39;4\u0026#39;: \u0026#34;ghi\u0026#34;, \u0026#39;5\u0026#39;: \u0026#34;jkl\u0026#34;, \u0026#39;6\u0026#39;: \u0026#34;mno\u0026#34;, \u0026#39;7\u0026#39;: \u0026#34;pqrs\u0026#34;, \u0026#39;8\u0026#39;: \u0026#34;tuv\u0026#34;, \u0026#39;9\u0026#39;: \u0026#34;wxyz\u0026#34; } def dfs(i): # 已经走完最后一步 if i==len(digits): ans.append(\u0026#34;\u0026#34;.join(paths)) return # 遍历所有选择，选择之间无关联 for c in num2char[digits[i]]: # 作出当前步的选择，往下递 paths.append(c) dfs(i+1) # 归时撤销之前选择 paths.pop() # 从第一步开始 dfs(0) return ans 关联选择类型 以leetcode-78为例：\n关联选择：每步的选择与其他步相关 递结束条件：步数为k 满足结果条件：选择为组合不重复，非排列 选择约束：每步的选择为前面选择后剩下的，且必须选一个 简单粗暴，但超时，因为去重和修改每步选择的实现耗时严重\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def combine(self, n: int, k: int) -\u0026gt; List[List[int]]: ans = set() paths = [] def dfs(i, choice): if i==k: # 满足结果条件，去除重复 ans.add(tuple(sorted(paths))) return for index in range(len(choice)): new_choice=choice.copy() # 缩小下一步可用选择 del new_choice[index] paths.append(choice[index]) dfs(i+1, new_choice) paths.pop() dfs(0, [i for i in range(1,n+1)]) return [list(item) for item in ans] 优化：修改每步选择的实现，加入结果条件的约束\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def combine(self, n: int, k: int) -\u0026gt; List[List[int]]: ans = [] paths = [] choices = [i for i in range(1,n+1)] # 所有选择 mask = [True for _ in range(n)] # dfs实现，所以一个数组即可记录一条路径上的选择结果 def dfs(i): if i==k: ans.append(paths.copy()) return for j in range(len(choices)): # 选择是否可用 if mask[j]: # 添加满足结果条件约束 if len(paths) \u0026gt;0: # 严格递增的选择，上一步的选择比当前大，证明已经遍历过该选择 if choices[j] \u0026lt; paths[-1]: continue paths.append(choices[j]) # 额外对选择进行标记 mask[j]=False dfs(i+1) paths.pop() # 选择其他时进行回退 mask[j]=True dfs(0) return ans 根据满足结果条件进行优化：注意到结果为其实为选择的排列\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 def combine(self, n: int, k: int) -\u0026gt; List[List[int]]: ans = [] paths = [] choices = [i for i in range(1,n+1)] def dfs(i): # 这里第几步不是用i表示，而是paths长度，i表示上一步选择的位置 if len(paths)==k: ans.append(paths.copy()) return # 从之前选择位置往后选 for j in range(i, len(choices)): paths.append(choices[j]) dfs(j+1) paths.pop() dfs(0) return ans 同理，leetcode-51\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def solveNQueens(self, n: int) -\u0026gt; List[List[str]]: ans = [] if n == 0: return ans # 注意这里不能写成（行引用，非n个独立list）：[[\u0026#34;.\u0026#34;] *n] * n paths = [[\u0026#34;.\u0026#34;] *n for _ in range(n)] column = [False] * n ldiag = [False] * (2 * n - 1) # 左对角线 rdiag = [False] * (2 * n - 1) # 右对角线 def dfs(row): if row == n: ans.append([\u0026#34;\u0026#34;.join(item) for item in paths]) return for i in range(n): # 难点：注意到结果要满足的条件 if column[i] or ldiag[n - row + i - 1] or rdiag[row + i]: continue paths[row][i] = \u0026#39;Q\u0026#39; column[i] = ldiag[n - row + i - 1] = rdiag[row + i] = True dfs(row + 1) paths[row][i] = \u0026#39;.\u0026#39; column[i] = ldiag[n - row + i - 1] = rdiag[row + i] = False dfs(0) return ans 动态规划 动态规划的定义 动态规划（Dynamic Programming）：简称 DP，是一种求解多阶段决策过程最优化问题的方法。在动态规划中，通过把原问题分解为相对简单的子问题，先求解子问题，再由子问题的解而得到原问题的解。\n动态规划最早由理查德 · 贝尔曼于 1957 年在其著作「动态规划（Dynamic Programming）」一书中提出。这里的 Programming 并不是编程的意思，而是指一种「表格处理方法」，即将每一步计算的结果存储在表格中，供随后的计算查询使用。\n动态规划的核心思想 动态规划的核心思想：\n把「原问题」分解为「若干个重叠的子问题」，每个子问题的求解过程都构成一个 「阶段」。在完成一个阶段的计算之后，动态规划方法才会执行下一个阶段的计算。 在求解子问题的过程中，按照「自顶向下的记忆化搜索方法」或者「自底向上的递推方法」求解出「子问题的解」，把结果存储在表格中，当需要再次求解此子问题时，直接从表格中查询该子问题的解，从而避免了大量的重复计算。 这看起来很像是分治算法，但动态规划与分治算法的不同点在于：\n适用于动态规划求解的问题，在分解之后得到的子问题往往是相互联系的，会出现若干个重叠子问题。 使用动态规划方法会将这些重叠子问题的解保存到表格里，供随后的计算查询使用，从而避免大量的重复计算。 动态规划的特征 究竟什么样的问题才可以使用动态规划算法解决呢？\n首先，能够使用动态规划方法解决的问题必须满足以下三个特征：\n最优子结构性质 重叠子问题性质 无后效性 最优子结构性质 最优子结构：指的是一个问题的最优解包含其子问题的最优解。\n举个例子，如下图所示，原问题 $S = \\lbrace a_1, a_2, a_3, a_4 \\rbrace$，在 $a_1$ 步我们选出一个当前最优解之后，问题就转换为求解子问题 $S_{\\text{子问题}} = \\lbrace a_2, a_3, a_4 \\rbrace$。如果原问题 $S$ 的最优解可以由「第 $a_1$ 步得到的局部最优解」和「 $S_{\\text{子问题}}$ 的最优解」构成，则说明该问题满足最优子结构性质。\n也就是说，如果原问题的最优解包含子问题的最优解，则说明该问题满足最优子结构性质。\n重叠子问题性质 重叠子问题性质：指的是在求解子问题的过程中，有大量的子问题是重复的，一个子问题在下一阶段的决策中可能会被多次用到。如果有大量重复的子问题，那么只需要对其求解一次，然后用表格将结果存储下来，以后使用时可以直接查询，不需要再次求解。\n之前我们提到的「斐波那契数列」例子中，$f(0)$、$f(1)$、$f(2)$、$f(3)$ 都进行了多次重复计算。动态规划算法利用了子问题重叠的性质，在第一次计算 $f(0)$、$f(1)$、$f(2)$、$f(3)$ 时就将其结果存入表格，当再次使用时可以直接查询，无需再次求解，从而提升效率。\n无后效性 无后效性：指的是子问题的解（状态值）只与之前阶段有关，而与后面阶段无关。当前阶段的若干状态值一旦确定，就不再改变，不会再受到后续阶段决策的影响。\n也就是说，一旦某一个子问题的求解结果确定以后，就不会再被修改。\n举个例子，下图是一个有向无环带权图，我们在求解从 $A$ 点到 $F$ 点的最短路径问题时，假设当前已知从 $A$ 点到 $D$ 点的最短路径（$2 + 7 = 9$）。那么无论之后的路径如何选择，都不会影响之前从 $A$ 点到 $D$ 点的最短路径长度。这就是「无后效性」。\n而如果一个问题具有「后效性」，则可能需要先将其转化或者逆向求解来消除后效性，然后才可以使用动态规划算法。\n动态规划的基本思路 如下图所示，我们在使用动态规划方法解决某些最优化问题时，可以将解决问题的过程按照一定顺序（时间顺序、空间顺序或其他顺序）分解为若干个相互联系的「阶段」。然后按照顺序对每一个阶段做出「决策」，这个决策既决定了本阶段的效益，也决定了下一阶段的初始状态。依次做完每个阶段的决策之后，就得到了一个整个问题的决策序列。\n这样就将一个原问题分解为了一系列的子问题，再通过逐步求解从而获得最终结果。\n这种前后关联、具有链状结构的多阶段进行决策的问题也叫做「多阶段决策问题」。\n通常我们使用动态规划方法来解决问题的基本思路如下：\n划分阶段：将原问题按顺序（时间顺序、空间顺序或其他顺序）分解为若干个相互联系的「阶段」。划分后的阶段⼀定是有序或可排序的，否则问题⽆法求解。 这里的「阶段」指的是⼦问题的求解过程。每个⼦问题的求解过程都构成⼀个「阶段」，在完成前⼀阶段的求解后才会进⾏后⼀阶段的求解。 定义状态：将和子问题相关的某些变量（位置、数量、体积、空间等等）作为一个「状态」表示出来。状态的选择要满⾜⽆后效性。 一个「状态」对应一个或多个子问题，所谓某个「状态」下的值，指的就是这个「状态」所对应的子问题的解。 状态转移：根据「上一阶段的状态」和「该状态下所能做出的决策」，推导出「下一阶段的状态」。或者说根据相邻两个阶段各个状态之间的关系，确定决策，然后推导出状态间的相互转移方式（即「状态转移方程」）。 初始条件和边界条件：根据问题描述、状态定义和状态转移方程，确定初始条件和边界条件。 最终结果：确定问题的求解目标，然后按照一定顺序求解每一个阶段的问题。最后根据状态转移方程的递推结果，确定最终结果。 实现及优化思路 通常思路：\n先确定每次操作及选择（子问题） 确定状态转移方程和初始值 用递归实现（逆向） 用数组（形状：n * choices）存储状态，改为循环实现（正向） 根据状态转移方程确定子问题关联了多少步的状态，从而优化空间 关键点：\n子问题 状态转移方程 初始值（满足条件和不满足条件） 0-1背包 概念 有 n 个物品，第 i 个物品的体积为 w[i] ，价值为 v[i] 。\n每个物品至多选一个，求体积和不超过 capacity 时的最大价值和。\n注意点：\n当前操作？枚举第 \\( i \\) 个物品选或不选： 不选，剩余容量不变； 选，剩余容量减少 \\( w[i] \\)。 子问题？在剩余容量为 \\( c \\) 时， 从 前 \\( i \\) 个物品 中得到的最大价值和。 下一个子问题？分类讨论： 不选：在剩余容量为 \\( c \\) 时，\n从 前 \\( i-1 \\) 个物品 中得到的最大价值和。 选：在剩余容量为 \\( c - w[i] \\) 时，\n从 前 \\( i-1 \\) 个物品 中得到的最大价值和。 回溯思考：有n步，每步有两种选择。 递归公式：\n$$ dfs(i, c) = \\max(dfs(i-1, c), dfs(i-1, c - w[i]) + v[i]) $$例子 以leetcode-494为例：\n回溯实现，python超时，C++通过\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def findTargetSumWays(self, nums: List[int], target: int) -\u0026gt; int: ans = [0] n = len(nums) def dfs(i, t): if i == n: if t == target: ans[0] += 1 return # 每步可选\u0026#39;+\u0026#39;或\u0026#39;-\u0026#39; dfs(i+1, t-nums[i]) dfs(i+1, t+nums[i]) dfs(0, 0) return ans[0] 动态规划\n注意到：所求方案中的负数和或正数和与总和及target值相关，可转换为0-1背包问题进行求解 状态转移方程：$dfs(i,c)=dfs(i-1,c)+dfs(i-1,c-nums[i])$ dfs(i,c)含义：前i个和为c的方案数 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 # 递归形式 def findTargetSumWays(self, nums: List[int], target: int) -\u0026gt; int: target += sum(nums) # 数学约束，保证target存在 if target \u0026lt; 0 or target % 2 != 0: return 0 target //=2 n = len(nums) @cache def dfs(i, t): if i\u0026lt;0: if t==0: return 1 return 0 if nums[i] \u0026gt; t: return dfs(i-1, t) return dfs(i-1, t) + dfs(i-1, t-nums[i]) return dfs(n-1, target) # 数组形式 def findTargetSumWays(self, nums: List[int], target: int) -\u0026gt; int: target += sum(nums) if target \u0026lt; 0 or target % 2 != 0: return 0 target //=2 n = len(nums) # 动态转移方程改为 dfs(i+1,c)=dfs(i,c)+dfs(i,c-nums[i]) dfs = [[0]*(target+1) for _ in range(n+1)] # dfs[0][0]表示最后一轮且c-nums[i]刚好为0 # 其实dfs[i][0]都应该初始化为1，但下面c \u0026lt; nums[i]操作肯定会执行进行拷贝，所以只用初始化第一个 dfs[0][0] = 1 for i in range(n): # 这里遍历范围为target+1，保证n步都能满足 for c in range(target+1): if c \u0026lt; nums[i]: dfs[i+1][c] = dfs[i][c] else: dfs[i+1][c] = dfs[i][c]+dfs[i][c-nums[i]] return dfs[n][target] # 数组形式可优化空间，由状态转移方程可以知道：某一状态只会用到前一个状态的结果，所以只需要两个数组即可 def findTargetSumWays(self, nums: List[int], target: int) -\u0026gt; int: target += sum(nums) if target \u0026lt; 0 or target % 2 != 0: return 0 target //=2 n = len(nums) # 两个数组存储，空间 O(target) dfs = [[0]*(target+1) for _ in range(2)] dfs[0][0] = 1 # 用cur，next标记当前状态和下一个状态，也可用取模操作，但取模操作耗时高 cur, next = 0, 1 for i in range(n): for c in range(target+1): if c \u0026lt; nums[i]: dfs[next][c] = dfs[cur][c] else: dfs[next][c] = dfs[cur][c]+dfs[cur][c-nums[i]] cur, next = next, cur return dfs[cur][target] # 注意到：下一次的状态其实只需要当前的两个位置的状态结果c和c-nums[i]，nums[i]非负，所以可用一个数组逆序转移 def findTargetSumWays(self, nums: List[int], target: int) -\u0026gt; int: target += sum(nums) if target \u0026lt; 0 or target % 2 != 0: return 0 target //=2 n = len(nums) dfs = [0]*(target+1) dfs[0] = 1 for x in nums: for c in range(target, -1, -1): if c \u0026gt;= x: dfs[c] = dfs[c]+dfs[c-x] return dfs[target] 完全背包 概念 有 \\( n \\) 种物品，第 \\( i \\) 种物品的体积为 \\( w[i] \\)，价值为 \\( v[i] \\)。\n每种物品 无限次重复 选，求体积和不超过 \\( capacity \\) 时的最大价值和。\n注意点：\n当前操作？枚举第 \\( i \\) 种物品选一个或不选： 不选，剩余容量不变； 选一个，剩余容量减少 \\( w[i] \\)。 子问题？在剩余容量为 \\( c \\) 时，从 前 \\( i \\) 种物品 中得到的最大价值和。 下一个子问题？分类讨论： 不选：在剩余容量为 \\( c \\) 时，从 前 \\( i-1 \\) 种物品 中得到的最大价值和。 选一个：在剩余容量为 \\( c - w[i] \\) 时，从 前 \\( i \\) 种物品 中得到的最大价值和。 递归公式： $$ dfs(i, c) = \\max(dfs(i-1, c), dfs(i, c - w[i]) + v[i]) $$例子 以leetcode-322为例\n不同于0-1背包某个物品只能选一次 这里要求最小数量：修改转移方程和不满足时返回值 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # 递归实现 def coinChange(self, coins: List[int], amount: int) -\u0026gt; int: @cache def dfs(i, c): if i\u0026lt;0: if c==0: return 0 return inf if coins[i] \u0026gt; c: return dfs(i-1, c) return min(dfs(i-1, c), dfs(i, c-coins[i])+1) ans = dfs(len(coins)-1, amount) return ans if ans \u0026lt; inf else -1 # 和上一题同理，改为数组+空间优化 def coinChange(self, coins: List[int], amount: int) -\u0026gt; int: n = len(coins) dfs = [inf]*(amount+1) dfs[0]=0 for x in coins: for c in range(amount+1): if c\u0026gt;=x: dfs[c] = min(dfs[c], dfs[c-x]+1) ans = dfs[amount] return ans if ans \u0026lt; inf else -1 ","date":"2022-06-17T00:00:00Z","permalink":"https://loveleaves.github.io/p/coding/","title":"【Coding】算法题编程记录"}]